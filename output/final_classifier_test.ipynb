{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mechanisms of Action (MoA) Prediction - Final Classifier\n",
    "## Test\n",
    "\n",
    "### Introduction\n",
    "In this notebook, we will create a test data prediction pipeline for our final multi-label classifier in order to produce a submission file. In the prediction pipeline (depending on how effective it is) we will add the models created for our 0-label classifiers.\n",
    "\n",
    "Because our strategy was to conduct nested cross-validation with bayesian hyperparameter search in each fold, each fold may have produced a model using different parameters, so a dataframe will be imported with these parameters (written during the training process). With this, we can ensemble all the models created during nested cross-validation (some may be the same, some may be different!) as well as the models created in using different random states.\n",
    "\n",
    "Due to the computationally intensive nature of nested cross-validation strategy, we don't end up with many models to ensemble. However, the expectation is that the fewer models we have perform better, so there will be no need for more models to ensemble. Provided this is true, the test prediction pipeline is consequently much faster (as each test record doesn't need to be passed through a ridiculous number of models before a prediction is created).\n",
    "\n",
    "This entire process has been novel to me. I have realised the **need** for a robust and effective cross-validation strategy - this can make or break a data science experiment. So with that in mind, I have not been making any submissions to the Kaggle public test set (that is used for the public leaderboard). My hope is that by having such a robust cross-validation strategy, I don't *have to* constantly evaluate the model performance based on the leaderboard, which is essentially overfitting to the leaderboard test data. \n",
    "\n",
    "One aspect that can be improved with the training pipeline is the usage of a sub-validation set through Keras's fit method, rather than a sub-validation set of my own creation. Scaling, feature selection etc are all done without using the OOF validation set (hence nested cross-validation), but we can further improve this by not using sub-validation data too! Now we're getting serious about over-fitting...\n",
    "\n",
    "Also, there is certainly room for more research into machine learning based feature selection techniques for multi-label problems. Currenty I employ a SelectFromModel technique *per label* (in a similar vein to one-vs-all classification). Storing the features selected for each label, by the end I rank the features in terms of how many times they were selected. The features to use in the final model are then defined by a parameter of num_features (that is included in our Bayesian hyperparameter search). This is rudimentary and could *definitely* be improved!\n",
    "\n",
    "## 1.00 Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import gc\n",
    "import random\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "import json # For reading in csv with string list representation values\n",
    "\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Data vis packages\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Data prep\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Modelling packages\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.python.keras import backend as k\n",
    "# Key layers\n",
    "from tensorflow.keras.models import load_model\n",
    "# Cross validation\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n",
      "REPLICAS: 1\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "\n",
    "strategy = tf.distribute.get_strategy()\n",
    "REPLICAS = strategy.num_replicas_in_sync\n",
    "print(f'REPLICAS: {REPLICAS}')\n",
    "\n",
    "# Data access\n",
    "gpu_options = tf.compat.v1.GPUOptions(allow_growth=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.00 Read in Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_features shape: \t\t(23814, 876)\n",
      "test_features shape: \t\t(3982, 876)\n",
      "train_targets_scored shape: \t(23814, 207)\n",
      "sample_submission shape: \t(3982, 207)\n"
     ]
    }
   ],
   "source": [
    "# Directory and file paths\n",
    "input_dir                 = '../input/lish-moa/'\n",
    "train_features_path       = os.path.join(input_dir, 'train_features.csv')\n",
    "test_features_path        = os.path.join(input_dir, 'test_features.csv')\n",
    "train_targets_scored_path = os.path.join(input_dir, 'train_targets_scored.csv')\n",
    "sample_submission_path    = os.path.join(input_dir, 'sample_submission.csv')\n",
    "\n",
    "# Read in data\n",
    "train_features       = pd.read_csv(train_features_path)\n",
    "test_features        = pd.read_csv(test_features_path)\n",
    "train_targets_scored = pd.read_csv(train_targets_scored_path)\n",
    "sample_submission    = pd.read_csv(sample_submission_path)\n",
    "\n",
    "del train_features_path, test_features_path, train_targets_scored_path, sample_submission_path\n",
    "\n",
    "print(f'train_features shape: \\t\\t{train_features.shape}')\n",
    "print(f'test_features shape: \\t\\t{test_features.shape}')\n",
    "print(f'train_targets_scored shape: \\t{train_targets_scored.shape}')\n",
    "print(f'sample_submission shape: \\t{sample_submission.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model name: nn_final_classifier\n"
     ]
    }
   ],
   "source": [
    "# Define key parameters\n",
    "SCALER_METHOD = RobustScaler()\n",
    "\n",
    "MODEL_TO_USE = 'nn'\n",
    "MODEL_NAME = MODEL_TO_USE + '_final_classifier'\n",
    "\n",
    "print(f'Model name: {MODEL_NAME}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.00 Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transformed_row_features(df):\n",
    "    \"\"\"\n",
    "    Input data and returns transformed features using row level statistics.\n",
    "    \"\"\"\n",
    "    \n",
    "    def get_row_stat(df, stat, feat_type):\n",
    "        \"\"\"\n",
    "        Input data and returns row level statistics.\n",
    "        stat: str ['sum','mean','med','std','min','max']\n",
    "        feat_type: str [None,'g','c']\n",
    "        \"\"\"\n",
    "\n",
    "        # Separate features into numerical and categorical (and by feature type if specified)\n",
    "        if feat_type == None:\n",
    "            df_numerical = df.select_dtypes('number').drop('cp_time', axis=1)\n",
    "            df_categorical = df.select_dtypes('object')\n",
    "        elif feat_type == 'g':\n",
    "            df_numerical = df.select_dtypes('number').drop('cp_time', axis=1)\n",
    "            df_categorical = df.select_dtypes('object')\n",
    "            # Subset to g features\n",
    "            df_numerical = df_numerical[df_numerical.columns[df_numerical.columns.str.startswith('g-')]]\n",
    "            df_categorical = df_categorical[df_categorical.columns[df_categorical.columns.str.startswith('g-')]]\n",
    "        elif feat_type == 'c':\n",
    "            df_numerical = df.select_dtypes('number').drop('cp_time', axis=1)\n",
    "            df_categorical = df.select_dtypes('object')\n",
    "            # Subset to g features\n",
    "            df_numerical = df_numerical[df_numerical.columns[df_numerical.columns.str.startswith('c-')]]\n",
    "            df_categorical = df_categorical[df_categorical.columns[df_categorical.columns.str.startswith('c-')]]\n",
    "\n",
    "        # Add statistic feature\n",
    "        if stat == 'sum':\n",
    "            stat_feat = df_numerical.sum(axis=1)\n",
    "        elif stat == 'mean':\n",
    "            stat_feat = df_numerical.mean(axis=1)\n",
    "        elif stat == 'med':\n",
    "            stat_feat = df_numerical.median(axis=1)\n",
    "        elif stat == 'std':\n",
    "            stat_feat = df_numerical.std(axis=1)\n",
    "        elif stat == 'min':\n",
    "            stat_feat = df_numerical.min(axis=1)\n",
    "        elif stat == 'max':\n",
    "            stat_feat = df_numerical.max(axis=1)\n",
    "\n",
    "        return(stat_feat)\n",
    "    \n",
    "    \n",
    "    # Get list of original column names (so we don't make transformations using new features)\n",
    "    df_cols = df.columns\n",
    "    \n",
    "    # Total row stats\n",
    "    df['row_sum']  = get_row_stat(df=df[df_cols], stat='sum' , feat_type=None)\n",
    "    df['row_mean'] = get_row_stat(df=df[df_cols], stat='mean', feat_type=None)\n",
    "    df['row_med']  = get_row_stat(df=df[df_cols], stat='med' , feat_type=None)\n",
    "    df['row_std']  = get_row_stat(df=df[df_cols], stat='std' , feat_type=None)\n",
    "    df['row_min']  = get_row_stat(df=df[df_cols], stat='min' , feat_type=None)\n",
    "    df['row_max']  = get_row_stat(df=df[df_cols], stat='max' , feat_type=None)\n",
    "    # G feature row stats\n",
    "    df['row_sum_g']  = get_row_stat(df=df[df_cols], stat='sum' , feat_type='g')\n",
    "    df['row_mean_g'] = get_row_stat(df=df[df_cols], stat='mean', feat_type='g')\n",
    "    df['row_med_g']  = get_row_stat(df=df[df_cols], stat='med' , feat_type='g')\n",
    "    df['row_std_g']  = get_row_stat(df=df[df_cols], stat='std' , feat_type='g')\n",
    "    df['row_min_g']  = get_row_stat(df=df[df_cols], stat='min' , feat_type='g')\n",
    "    df['row_max_g']  = get_row_stat(df=df[df_cols], stat='max' , feat_type='g')\n",
    "    # C feature row stats\n",
    "    df['row_sum_c']  = get_row_stat(df=df[df_cols], stat='sum' , feat_type='c')\n",
    "    df['row_mean_c'] = get_row_stat(df=df[df_cols], stat='mean', feat_type='c')\n",
    "    df['row_med_c']  = get_row_stat(df=df[df_cols], stat='med' , feat_type='c')\n",
    "    df['row_std_c']  = get_row_stat(df=df[df_cols], stat='std' , feat_type='c')\n",
    "    df['row_min_c']  = get_row_stat(df=df[df_cols], stat='min' , feat_type='c')\n",
    "    df['row_max_c']  = get_row_stat(df=df[df_cols], stat='max' , feat_type='c')\n",
    "    \n",
    "    # G features row stats / row sum\n",
    "    df['row_sum_g_by_row_sum']  = df['row_sum_g']  / df['row_sum']\n",
    "    df['row_mean_g_by_row_sum'] = df['row_mean_g'] / df['row_sum']\n",
    "    df['row_med_g_by_row_sum']  = df['row_med_g']  / df['row_sum']\n",
    "    df['row_std_g_by_row_sum']  = df['row_std_g']  / df['row_sum']\n",
    "    df['row_min_g_by_row_sum']  = df['row_min_g']  / df['row_sum']\n",
    "    df['row_max_g_by_row_sum']  = df['row_max_g']  / df['row_sum']\n",
    "    # C features row stats / row sum\n",
    "    df['row_sum_c_by_row_sum']  = df['row_sum_c']  / df['row_sum']\n",
    "    df['row_mean_c_by_row_sum'] = df['row_mean_c'] / df['row_sum']\n",
    "    df['row_med_c_by_row_sum']  = df['row_med_c']  / df['row_sum']\n",
    "    df['row_std_c_by_row_sum']  = df['row_std_c']  / df['row_sum']\n",
    "    df['row_min_c_by_row_sum']  = df['row_min_c']  / df['row_sum']\n",
    "    df['row_max_c_by_row_sum']  = df['row_max_c']  / df['row_sum']\n",
    "    \n",
    "    # G features row stats / row mean\n",
    "    df['row_sum_g_by_row_mean']  = df['row_sum_g']  / df['row_mean']\n",
    "    df['row_mean_g_by_row_mean'] = df['row_mean_g'] / df['row_mean']\n",
    "    df['row_med_g_by_row_mean']  = df['row_med_g']  / df['row_mean']\n",
    "    df['row_std_g_by_row_mean']  = df['row_std_g']  / df['row_mean']\n",
    "    df['row_min_g_by_row_mean']  = df['row_min_g']  / df['row_mean']\n",
    "    df['row_max_g_by_row_mean']  = df['row_max_g']  / df['row_mean']\n",
    "    # C features row stats / row mean\n",
    "    df['row_sum_c_by_row_mean']  = df['row_sum_c']  / df['row_mean']\n",
    "    df['row_mean_c_by_row_mean'] = df['row_mean_c'] / df['row_mean']\n",
    "    df['row_med_c_by_row_mean']  = df['row_med_c']  / df['row_mean']\n",
    "    df['row_std_c_by_row_mean']  = df['row_std_c']  / df['row_mean']\n",
    "    df['row_min_c_by_row_mean']  = df['row_min_c']  / df['row_mean']\n",
    "    df['row_max_c_by_row_mean']  = df['row_max_c']  / df['row_mean']\n",
    "    \n",
    "    # G features row stats / row med\n",
    "    df['row_sum_g_by_row_med']  = df['row_sum_g']  / df['row_med']\n",
    "    df['row_mean_g_by_row_med'] = df['row_mean_g'] / df['row_med']\n",
    "    df['row_med_g_by_row_med']  = df['row_med_g']  / df['row_med']\n",
    "    df['row_std_g_by_row_med']  = df['row_std_g']  / df['row_med']\n",
    "    df['row_min_g_by_row_med']  = df['row_min_g']  / df['row_med']\n",
    "    df['row_max_g_by_row_med']  = df['row_max_g']  / df['row_med']\n",
    "    # C features row stats / row med\n",
    "    df['row_sum_c_by_row_med']  = df['row_sum_c']  / df['row_med']\n",
    "    df['row_mean_c_by_row_med'] = df['row_mean_c'] / df['row_med']\n",
    "    df['row_med_c_by_row_med']  = df['row_med_c']  / df['row_med']\n",
    "    df['row_std_c_by_row_med']  = df['row_std_c']  / df['row_med']\n",
    "    df['row_min_c_by_row_med']  = df['row_min_c']  / df['row_med']\n",
    "    df['row_max_c_by_row_med']  = df['row_max_c']  / df['row_med']\n",
    "    \n",
    "    # G features row stats / row std\n",
    "    df['row_sum_g_by_row_std']  = df['row_sum_g']  / df['row_std']\n",
    "    df['row_mean_g_by_row_std'] = df['row_mean_g'] / df['row_std']\n",
    "    df['row_med_g_by_row_std']  = df['row_med_g']  / df['row_std']\n",
    "    df['row_std_g_by_row_std']  = df['row_std_g']  / df['row_std']\n",
    "    df['row_min_g_by_row_std']  = df['row_min_g']  / df['row_std']\n",
    "    df['row_max_g_by_row_std']  = df['row_max_g']  / df['row_std']\n",
    "    # C features row stats / row std\n",
    "    df['row_sum_c_by_row_std']  = df['row_sum_c']  / df['row_std']\n",
    "    df['row_mean_c_by_row_std'] = df['row_mean_c'] / df['row_std']\n",
    "    df['row_med_c_by_row_std']  = df['row_med_c']  / df['row_std']\n",
    "    df['row_std_c_by_row_std']  = df['row_std_c']  / df['row_std']\n",
    "    df['row_min_c_by_row_std']  = df['row_min_c']  / df['row_std']\n",
    "    df['row_max_c_by_row_std']  = df['row_max_c']  / df['row_std']\n",
    "    \n",
    "    # G features row stats / row min\n",
    "    df['row_sum_g_by_row_min']  = df['row_sum_g']  / df['row_min']\n",
    "    df['row_mean_g_by_row_min'] = df['row_mean_g'] / df['row_min']\n",
    "    df['row_med_g_by_row_min']  = df['row_med_g']  / df['row_min']\n",
    "    df['row_std_g_by_row_min']  = df['row_std_g']  / df['row_min']\n",
    "    df['row_min_g_by_row_min']  = df['row_min_g']  / df['row_min']\n",
    "    df['row_max_g_by_row_min']  = df['row_max_g']  / df['row_min']\n",
    "    # C features row stats / row min\n",
    "    df['row_sum_c_by_row_min']  = df['row_sum_c']  / df['row_min']\n",
    "    df['row_mean_c_by_row_min'] = df['row_mean_c'] / df['row_min']\n",
    "    df['row_med_c_by_row_min']  = df['row_med_c']  / df['row_min']\n",
    "    df['row_std_c_by_row_min']  = df['row_std_c']  / df['row_min']\n",
    "    df['row_min_c_by_row_min']  = df['row_min_c']  / df['row_min']\n",
    "    df['row_max_c_by_row_min']  = df['row_max_c']  / df['row_min']\n",
    "    \n",
    "    # G features row stats / row max\n",
    "    df['row_sum_g_by_row_max']  = df['row_sum_g']  / df['row_max']\n",
    "    df['row_mean_g_by_row_max'] = df['row_mean_g'] / df['row_max']\n",
    "    df['row_med_g_by_row_max']  = df['row_med_g']  / df['row_max']\n",
    "    df['row_std_g_by_row_max']  = df['row_std_g']  / df['row_max']\n",
    "    df['row_min_g_by_row_max']  = df['row_min_g']  / df['row_max']\n",
    "    df['row_max_g_by_row_max']  = df['row_max_g']  / df['row_max']\n",
    "    # C features row stats / row max\n",
    "    df['row_sum_c_by_row_max']  = df['row_sum_c']  / df['row_max']\n",
    "    df['row_mean_c_by_row_max'] = df['row_mean_c'] / df['row_max']\n",
    "    df['row_med_c_by_row_max']  = df['row_med_c']  / df['row_max']\n",
    "    df['row_std_c_by_row_max']  = df['row_std_c']  / df['row_max']\n",
    "    df['row_min_c_by_row_max']  = df['row_min_c']  / df['row_max']\n",
    "    df['row_max_c_by_row_max']  = df['row_max_c']  / df['row_max']\n",
    "    \n",
    "    # G features row stats / C features row stats\n",
    "    df['row_sum_g_by_row_sum_c']  = df['row_sum_g']  / df['row_sum_g']\n",
    "    df['row_sum_g_by_row_mean_c'] = df['row_mean_g'] / df['row_mean_g']\n",
    "    df['row_sum_g_by_row_med_c']  = df['row_med_g']  / df['row_med_g']\n",
    "    df['row_sum_g_by_row_std_c']  = df['row_std_g']  / df['row_std_g']\n",
    "    df['row_sum_g_by_row_min_c']  = df['row_min_g']  / df['row_min_g']\n",
    "    df['row_sum_g_by_row_max_c']  = df['row_max_g']  / df['row_max_g']\n",
    "    \n",
    "    # Row stats / cp_time\n",
    "    df['row_sum_by_cp_time']  = df['row_sum']  / df['cp_time']\n",
    "    df['row_mean_by_cp_time'] = df['row_mean'] / df['cp_time']\n",
    "    df['row_med_by_cp_time']  = df['row_med']  / df['cp_time']\n",
    "    df['row_std_by_cp_time']  = df['row_std']  / df['cp_time']\n",
    "    df['row_min_by_cp_time']  = df['row_min']  / df['cp_time']\n",
    "    df['row_max_by_cp_time']  = df['row_max']  / df['cp_time']\n",
    "    \n",
    "    # G features row stats / cp_time\n",
    "    df['row_sum_g_by_cp_time']  = df['row_sum_g']  / df['cp_time']\n",
    "    df['row_mean_g_by_cp_time'] = df['row_mean_g'] / df['cp_time']\n",
    "    df['row_med_g_by_cp_time']  = df['row_med_g']  / df['cp_time']\n",
    "    df['row_std_g_by_cp_time']  = df['row_std_g']  / df['cp_time']\n",
    "    df['row_min_g_by_cp_time']  = df['row_min_g']  / df['cp_time']\n",
    "    df['row_max_g_by_cp_time']  = df['row_max_g']  / df['cp_time']\n",
    "    \n",
    "    # C features row stats / cp_time\n",
    "    df['row_sum_c_by_cp_time']  = df['row_sum_c']  / df['cp_time']\n",
    "    df['row_mean_c_by_cp_time'] = df['row_mean_c'] / df['cp_time']\n",
    "    df['row_med_c_by_cp_time']  = df['row_med_c']  / df['cp_time']\n",
    "    df['row_std_c_by_cp_time']  = df['row_std_c']  / df['cp_time']\n",
    "    df['row_min_c_by_cp_time']  = df['row_min_c']  / df['cp_time']\n",
    "    df['row_max_c_by_cp_time']  = df['row_max_c']  / df['cp_time']\n",
    "    \n",
    "    return(df, df_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transformed_col_features(df, df_cols, stat, row_feat_type, col_feat_type, feature_name):\n",
    "    \"\"\"\n",
    "    Input data and returns transformed features using column level statistics.\n",
    "    stat: str ['sum','mean','med','std','min','max']\n",
    "    row_feat_type: str [None,'g','c']\n",
    "    col_feat_type: str [None,'g','c']\n",
    "    feature_name: str, name to call new outputted feature\n",
    "    df_cols: list of column names from original dataset\n",
    "    \"\"\"\n",
    "    \n",
    "    def get_column_stat(df, stat, feat_type):\n",
    "        \"\"\"\n",
    "        Input data and returns column level statistics.\n",
    "        stat: str ['sum','mean','med','std','min','max']\n",
    "        feat_type: str [None,'g','c']\n",
    "        \"\"\"\n",
    "\n",
    "        # Separate features into numerical and categorical (and by feature type if specified)\n",
    "        if feat_type == None:\n",
    "            df_numerical = df.select_dtypes('number').drop('cp_time', axis=1)\n",
    "            df_categorical = df.select_dtypes('object')\n",
    "        elif feat_type == 'g':\n",
    "            df_numerical = df.select_dtypes('number').drop('cp_time', axis=1)\n",
    "            df_categorical = df.select_dtypes('object')\n",
    "            # Subset to g features\n",
    "            df_numerical = df_numerical[df_numerical.columns[df_numerical.columns.str.startswith('g-')]]\n",
    "            df_categorical = df_categorical[\n",
    "                df_categorical.columns[df_categorical.columns.astype(str).str.startswith('g-')]\n",
    "            ]\n",
    "        elif feat_type == 'c':\n",
    "            df_numerical = df.select_dtypes('number').drop('cp_time', axis=1)\n",
    "            df_categorical = df.select_dtypes('object')\n",
    "            # Subset to g features\n",
    "            df_numerical = df_numerical[df_numerical.columns[df_numerical.columns.str.startswith('c-')]]\n",
    "            df_categorical = df_categorical[\n",
    "                df_categorical.columns[df_categorical.columns.astype(str).str.startswith('c-')]\n",
    "            ]\n",
    "\n",
    "        # Add statistic feature\n",
    "        if stat == 'sum':\n",
    "            stat_feat = np.sum(df_numerical.values)\n",
    "        elif stat == 'mean':\n",
    "            stat_feat = np.mean(df_numerical.values)\n",
    "        elif stat == 'med':\n",
    "            stat_feat = np.median(df_numerical.values)\n",
    "        elif stat == 'std':\n",
    "            stat_feat = np.std(df_numerical.values)\n",
    "        elif stat == 'min':\n",
    "            stat_feat = np.min(df_numerical.values)\n",
    "        elif stat == 'max':\n",
    "            stat_feat = np.max(df_numerical.values)\n",
    "\n",
    "        return(stat_feat)\n",
    "    \n",
    "    # Get column level statistic\n",
    "    col_stat = get_column_stat(df=df[df_cols], stat=stat, feat_type=col_feat_type)\n",
    "    \n",
    "    # Redefine the feature suffix based on row_feat_type\n",
    "    if row_feat_type == None:\n",
    "        row_feat_type = ''\n",
    "    elif row_feat_type == 'g':\n",
    "        row_feat_type = '_g'\n",
    "    elif row_feat_type == 'c':\n",
    "        row_feat_type = '_c'\n",
    "    \n",
    "    # Get transformed feature\n",
    "    if stat == 'sum':\n",
    "        df[feature_name] = df['row_sum' + row_feat_type] / col_stat\n",
    "    elif stat == 'mean':\n",
    "        df[feature_name] = df['row_mean' + row_feat_type] / col_stat\n",
    "    elif stat == 'med':\n",
    "        df[feature_name] = df['row_med' + row_feat_type] / col_stat\n",
    "    elif stat == 'std':\n",
    "        df[feature_name] = df['row_std' + row_feat_type] / col_stat\n",
    "    elif stat == 'min':\n",
    "        df[feature_name] = df['row_min' + row_feat_type] / col_stat\n",
    "    elif stat == 'max':\n",
    "        df[feature_name] = df['row_max' + row_feat_type] / col_stat\n",
    "    \n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_feature_set(X_train, X_test, y_train, \n",
    "                          selected_features,\n",
    "                          num_features,\n",
    "                          pca, \n",
    "                          num_components,\n",
    "                          seed,\n",
    "                          scaler=SCALER_METHOD,\n",
    "                          verbose=0):\n",
    "    \"\"\"\n",
    "    Takes in X_train and X_test datasets, and applies feature transformations,\n",
    "    feature selection, scaling and pca (dependent on arguments). \n",
    "    \n",
    "    Returns transformed X_train and X_test data ready for training/prediction, and returns\n",
    "    list of numerical cols and categorical cols, for the use of creating embeddings.\n",
    "    \"\"\"\n",
    "    \n",
    "    ## DATA PREPARATION ##\n",
    "    \n",
    "    # Drop unique ID feature\n",
    "    X_train = X_train.drop('sig_id', axis=1)\n",
    "    X_test  = X_test.drop('sig_id', axis=1)\n",
    "    # Get indices for train and test dfs - we'll need these later\n",
    "    train_idx = list(X_train.index)\n",
    "    test_idx  = list(X_test.index)\n",
    "    \n",
    "    \n",
    "    ## IN-FOLD FEATURE ENGINEERING ##\n",
    "\n",
    "    if verbose == 1:\n",
    "        print('ENGINGEERING FEATURES...')\n",
    "        \n",
    "    for X_dataset in [X_train, X_test]:\n",
    "        # Row transformations\n",
    "        df, df_cols = get_transformed_row_features(X_dataset)\n",
    "    for X_dataset in [X_train, X_test]:\n",
    "        # Total row stats / column stats\n",
    "        get_transformed_col_features(X_dataset, df_cols, 'sum', None, None, 'row_sum_by_col_sum')\n",
    "        get_transformed_col_features(X_dataset, df_cols, 'mean',None, None, 'row_mean_by_col_mean')\n",
    "        get_transformed_col_features(X_dataset, df_cols, 'med', None, None, 'row_med_by_col_med')\n",
    "        get_transformed_col_features(X_dataset, df_cols, 'std', None, None, 'row_std_by_col_std')\n",
    "        get_transformed_col_features(X_dataset, df_cols, 'min', None, None, 'row_min_by_col_min')\n",
    "        get_transformed_col_features(X_dataset, df_cols, 'max', None, None, 'row_max_by_col_max')\n",
    "        # G features row stats / column stats\n",
    "        get_transformed_col_features(X_dataset, df_cols, 'sum', 'g', None, 'row_sum_g_by_col_sum')\n",
    "        get_transformed_col_features(X_dataset, df_cols, 'mean','g', None, 'row_mean_g_by_col_mean')\n",
    "        get_transformed_col_features(X_dataset, df_cols, 'med', 'g', None, 'row_med_g_by_col_med')\n",
    "        get_transformed_col_features(X_dataset, df_cols, 'std', 'g', None, 'row_std_g_by_col_std')\n",
    "        get_transformed_col_features(X_dataset, df_cols, 'min', 'g', None, 'row_min_g_by_col_min')\n",
    "        get_transformed_col_features(X_dataset, df_cols, 'max', 'g', None, 'row_max_g_by_col_max')    \n",
    "        # C features row stats / column stats\n",
    "        get_transformed_col_features(X_dataset, df_cols, 'sum', 'c', None, 'row_sum_c_by_col_sum')\n",
    "        get_transformed_col_features(X_dataset, df_cols, 'mean','c', None, 'row_mean_c_by_col_mean')\n",
    "        get_transformed_col_features(X_dataset, df_cols, 'med', 'c', None, 'row_med_c_by_col_med')\n",
    "        get_transformed_col_features(X_dataset, df_cols, 'std', 'c', None, 'row_std_c_by_col_std')\n",
    "        get_transformed_col_features(X_dataset, df_cols, 'min', 'c', None, 'row_min_c_by_col_min')\n",
    "        get_transformed_col_features(X_dataset, df_cols, 'max', 'c', None, 'row_max_c_by_col_max')\n",
    "        # G features row stats / C features column stats\n",
    "        get_transformed_col_features(X_dataset, df_cols, 'sum', 'g', 'c', 'row_sum_g_by_col_sum_c')\n",
    "        get_transformed_col_features(X_dataset, df_cols, 'mean','g', 'c', 'row_mean_g_by_col_mean_c')\n",
    "        get_transformed_col_features(X_dataset, df_cols, 'med', 'g', 'c', 'row_med_g_by_col_med_c')\n",
    "        get_transformed_col_features(X_dataset, df_cols, 'std', 'g', 'c', 'row_std_g_by_col_std_c')\n",
    "        get_transformed_col_features(X_dataset, df_cols, 'min', 'g', 'c', 'row_min_g_by_col_min_c')\n",
    "        get_transformed_col_features(X_dataset, df_cols, 'max', 'g', 'c', 'row_max_g_by_col_max_c')\n",
    "        # C features row stats / G features column stats\n",
    "        get_transformed_col_features(X_dataset, df_cols, 'sum', 'c', 'g', 'row_sum_c_by_col_sum_g')\n",
    "        get_transformed_col_features(X_dataset, df_cols, 'mean','c', 'g', 'row_mean_c_by_col_mean_g')\n",
    "        get_transformed_col_features(X_dataset, df_cols, 'med', 'c', 'g', 'row_med_c_by_col_med_g')\n",
    "        get_transformed_col_features(X_dataset, df_cols, 'std', 'c', 'g', 'row_std_c_by_col_std_g')\n",
    "        get_transformed_col_features(X_dataset, df_cols, 'min', 'c', 'g', 'row_min_c_by_col_min_g')\n",
    "        get_transformed_col_features(X_dataset, df_cols, 'max', 'c', 'g', 'row_max_c_by_col_max_g')    \n",
    "\n",
    "    # Replace any infinite values generated with 0\n",
    "    X_train.replace(to_replace=[np.inf, -np.inf, np.nan], value=0, inplace=True)\n",
    "    X_test.replace(to_replace=[np.inf, -np.inf, np.nan], value=0, inplace=True)\n",
    "    \n",
    "    # Separate train data types\n",
    "    X_train_numerical   = X_train.select_dtypes('number')\n",
    "    X_train_categorical = X_train.select_dtypes('object')\n",
    "    X_train_categorical = X_train_categorical.astype('category')\n",
    "    # Separate val data types\n",
    "    X_test_numerical   = X_test.select_dtypes('number')\n",
    "    X_test_categorical = X_test.select_dtypes('object')\n",
    "    X_test_categorical = X_test_categorical.astype('category')\n",
    "    \n",
    "    # Get colnames before scaling and feature selection\n",
    "    num_cols = X_train_numerical.columns\n",
    "    cat_cols = X_train_categorical.columns\n",
    "    \n",
    "    ## SCALING ##\n",
    "    \n",
    "    if scaler != None:\n",
    "        if verbose == 1:\n",
    "            print('APPLYING SCALER...')\n",
    "            \n",
    "        # Fit and transform scaler to train and val\n",
    "        scaler.fit(X_train_numerical)\n",
    "        X_train_numerical = scaler.transform(X_train_numerical)\n",
    "        X_test_numerical  = scaler.transform(X_test_numerical)\n",
    "        # Convert to back dataframe\n",
    "        X_train_numerical = pd.DataFrame(X_train_numerical, index=train_idx, columns=num_cols)\n",
    "        X_test_numerical  = pd.DataFrame(X_test_numerical, index=test_idx, columns=num_cols)\n",
    "    \n",
    "    \n",
    "    ## FEATURE SELECTION ##\n",
    "    \n",
    "    # Subset to features selected during train process (and stored in corresponding parameters file)\n",
    "    if verbose == 1:\n",
    "        print('APPLYING FEATURE SELECTOR...')\n",
    "        num_cols = X_train_numerical.shape[1]\n",
    "                \n",
    "    # Subset datasets to selected features only\n",
    "    X_train_numerical = X_train_numerical[selected_features]\n",
    "    X_test_numerical  = X_test_numerical[selected_features]\n",
    "    if verbose == 1: \n",
    "        print(f'{num_cols - X_train_numerical.shape[1]} features removed in feature selection.')\n",
    "        del num_cols\n",
    "\n",
    "            \n",
    "    ## PCA ##\n",
    "    \n",
    "    if pca != None:\n",
    "        if verbose == 1:\n",
    "            print('APPLYING PCA...')\n",
    "            \n",
    "        # Fit and transform pca to train and val\n",
    "        pca.fit(X_train_numerical)\n",
    "        X_train_numerical = pca.transform(X_train_numerical)\n",
    "        X_test_numerical  = pca.transform(X_test_numerical)\n",
    "        if verbose == 1:\n",
    "            print(f'NUMBER OF PRINCIPAL COMPONENTS: {pca.n_components_}')\n",
    "        # Convert numerical features into pandas dataframe and clean colnames\n",
    "        X_train_numerical = pd.DataFrame(X_train_numerical, index=train_idx).add_prefix('pca_')\n",
    "        X_test_numerical  = pd.DataFrame(X_test_numerical, index=test_idx).add_prefix('pca_')\n",
    "    \n",
    "    \n",
    "    ## CATEGORICAL FEATURES ##\n",
    "    \n",
    "    # Get categorical and numerical column names\n",
    "    num_cols = X_train_numerical.columns\n",
    "    cat_cols = X_train_categorical.columns\n",
    "\n",
    "    # Encode categorical features\n",
    "    X_train_categorical = X_train_categorical.apply(lambda x: x.cat.codes)\n",
    "    X_test_categorical  = X_test_categorical.apply(lambda x: x.cat.codes)\n",
    "\n",
    "    \n",
    "    # Concatenate transformed categorical features with transformed numerical features  \n",
    "    X_train = pd.concat([X_train_categorical, X_train_numerical], axis=1)\n",
    "    X_test  = pd.concat([X_test_categorical, X_test_numerical], axis=1)\n",
    "    \n",
    "    if verbose == 1:\n",
    "        print(f'TRAIN SHAPE: \\t\\t{X_train.shape}')\n",
    "        print(f'VALIDATION SHAPE: \\t{X_test.shape}')\n",
    "    \n",
    "    return X_train, X_test, num_cols, cat_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_features\n",
    "y_train = train_targets_scored.drop('sig_id', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.00 Test Predictions\n",
    "\n",
    "Because in the model train pipeline, we performed in-fold Bayesian hyperparameter searches for each model, it is expected that the model architecture will be slighlty different for each of the 10 folds. Consequently, we'll need to read in the csv of parameters to prepare the test prediction pipeline before we start to make the predictions (as we won't be able to feed in the same dataset into each model - differing transformations will be required per model).\n",
    "\n",
    "In future, I'd like to automate this step. In order to do this, more work will need to be carried out on the train notebook, but due to time constraints and resource limits, we will have to move on for now without making those amendments. \n",
    "\n",
    "### 4.01 Prepare Prediction Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_test_predictions(X_test,\n",
    "                          selected_features,\n",
    "                          num_features,\n",
    "                          num_components, \n",
    "                          use_embedding, \n",
    "                          seed, \n",
    "                          kfold,\n",
    "                          num_folds,\n",
    "                          X_train=X_train, \n",
    "                          y_train=y_train, \n",
    "                          model_name=MODEL_NAME,\n",
    "                          submission=sample_submission):\n",
    "    \"\"\"\n",
    "    Reads in X_test feature set, loads the model specified by model_path, and \n",
    "    applies transformations as per num_components and use_embedding\n",
    "    \n",
    "    Returns dataframe with sig_id and a binary column indicating \n",
    "    \"\"\"\n",
    "    \n",
    "    # Retrieve the dataframe ids that were used in kfold during cross validation (using specified seed)\n",
    "    kf = KFold(n_splits=num_folds, random_state=seed)\n",
    "    for fold, (tdx, vdx) in enumerate(kf.split(X_train, y_train)):\n",
    "        if fold == kfold:\n",
    "            # End the loop when it gets to kfold so we can retain tdx for kfold\n",
    "            break\n",
    "            \n",
    "    # Instantiate PCA method\n",
    "    pca = PCA(n_components=num_components, random_state=seed)\n",
    "        \n",
    "    # Subset X_train and y_train as per what occurred during cross validation for kfold and seed\n",
    "    X_train, y_train = X_train.iloc[tdx, :], y_train.iloc[tdx, :]\n",
    "    \n",
    "    # Transform data - again to replicate what occurred with at kfold and seed\n",
    "    X_train, X_test, num_cols, cat_cols = transform_feature_set(X_train           = X_train, \n",
    "                                                                X_test            = X_test, \n",
    "                                                                y_train           = y_train, \n",
    "                                                                selected_features = selected_features,\n",
    "                                                                num_features      = num_features,\n",
    "                                                                pca               = pca,\n",
    "                                                                num_components    = num_components,\n",
    "                                                                seed              = seed)\n",
    "        \n",
    "    # Further transformations if an embedding was used at kfold and seed\n",
    "    if use_embedding == True:\n",
    "        # Separate data to fit into embedding and numerical input layers\n",
    "        X_train = [np.absolute(X_train[i]) for i in cat_cols] + [X_train[num_cols]]\n",
    "        X_test = [np.absolute(X_test[i]) for i in cat_cols] + [X_test[num_cols]]\n",
    "            \n",
    "    # Get the model name and file path for kfold and seed, then load that model\n",
    "    model_name = model_name + '_seed' + str(seed)\n",
    "    model_path = 'models/' + model_name + '/' + model_name + '_' + str(kfold) + '.h5'\n",
    "    model = load_model(model_path)\n",
    "    \n",
    "    # Make test predictions using the model created at kfold and seed\n",
    "    preds = model.predict(X_test)\n",
    "            \n",
    "    return(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passing file .ipynb_checkpoints\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>kfold</th>\n",
       "      <th>selected_features</th>\n",
       "      <th>num_features</th>\n",
       "      <th>num_components</th>\n",
       "      <th>use_embedding</th>\n",
       "      <th>seed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>['row_std_by_col_std', 'row_max_g_by_row_min',...</td>\n",
       "      <td>810</td>\n",
       "      <td>233</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>['row_std_by_col_std', 'row_max_g_by_row_min',...</td>\n",
       "      <td>757</td>\n",
       "      <td>194</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>['row_max_g_by_row_min', 'row_std_by_col_std',...</td>\n",
       "      <td>810</td>\n",
       "      <td>233</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>['row_std_by_col_std', 'row_std', 'row_max_g_b...</td>\n",
       "      <td>810</td>\n",
       "      <td>233</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>['row_max_g_by_row_min', 'row_std', 'row_std_b...</td>\n",
       "      <td>500</td>\n",
       "      <td>200</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>['row_max_g_by_row_min', 'row_std_g', 'row_std...</td>\n",
       "      <td>500</td>\n",
       "      <td>200</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>['row_max_g_by_row_min', 'row_std_by_cp_time',...</td>\n",
       "      <td>810</td>\n",
       "      <td>233</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>['row_max_g_by_row_min', 'row_std', 'row_std_b...</td>\n",
       "      <td>810</td>\n",
       "      <td>233</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>['row_std', 'row_std_by_cp_time', 'row_min_by_...</td>\n",
       "      <td>717</td>\n",
       "      <td>274</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>['row_std_by_col_std', 'row_max_g_by_row_min',...</td>\n",
       "      <td>810</td>\n",
       "      <td>233</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   kfold                                  selected_features  num_features  \\\n",
       "0      0  ['row_std_by_col_std', 'row_max_g_by_row_min',...           810   \n",
       "1      1  ['row_std_by_col_std', 'row_max_g_by_row_min',...           757   \n",
       "2      2  ['row_max_g_by_row_min', 'row_std_by_col_std',...           810   \n",
       "3      3  ['row_std_by_col_std', 'row_std', 'row_max_g_b...           810   \n",
       "4      4  ['row_max_g_by_row_min', 'row_std', 'row_std_b...           500   \n",
       "5      5  ['row_max_g_by_row_min', 'row_std_g', 'row_std...           500   \n",
       "6      6  ['row_max_g_by_row_min', 'row_std_by_cp_time',...           810   \n",
       "7      7  ['row_max_g_by_row_min', 'row_std', 'row_std_b...           810   \n",
       "8      8  ['row_std', 'row_std_by_cp_time', 'row_min_by_...           717   \n",
       "9      9  ['row_std_by_col_std', 'row_max_g_by_row_min',...           810   \n",
       "\n",
       "   num_components  use_embedding  seed  \n",
       "0             233              1    14  \n",
       "1             194              1    14  \n",
       "2             233              1    14  \n",
       "3             233              1    14  \n",
       "4             200              1    14  \n",
       "5             200              1    14  \n",
       "6             233              1    14  \n",
       "7             233              1    14  \n",
       "8             274              1    14  \n",
       "9             233              1    14  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compile model parameters for all models produced\n",
    "parameter_files = os.listdir('final_classifier_parameters')\n",
    "\n",
    "model_parameters = pd.DataFrame()\n",
    "# Remove any files that aren't a parameter csv\n",
    "for idx, file in enumerate(parameter_files):\n",
    "    try:\n",
    "        model_parameters = pd.read_csv(f'final_classifier_parameters/{file}')\n",
    "    except ValueError:\n",
    "        print(f'Passing file {file}')\n",
    "        pass\n",
    "\n",
    "# Print model parameters\n",
    "model_parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.02 Make Test Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making predictions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 10%|█         | 1/10 [00:55<08:22, 55.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making predictions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|██        | 2/10 [01:46<07:14, 54.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making predictions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 30%|███       | 3/10 [02:38<06:14, 53.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making predictions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|████      | 4/10 [03:28<05:16, 52.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making predictions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|█████     | 5/10 [04:19<04:20, 52.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making predictions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|██████    | 6/10 [05:11<03:27, 51.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making predictions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 70%|███████   | 7/10 [06:02<02:35, 51.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making predictions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|████████  | 8/10 [06:53<01:43, 51.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making predictions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 90%|█████████ | 9/10 [07:45<00:51, 51.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making predictions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [08:36<00:00, 51.52s/it]\n"
     ]
    }
   ],
   "source": [
    "# Make 0_label test predictions for all models created during CV for all seeds\n",
    "preds = []\n",
    "for idx in tqdm(model_parameters.index):\n",
    "    \n",
    "    # Get number of folds for each seed - add 1 because of zero indexing\n",
    "    seed      = model_parameters.iloc[idx]['seed']\n",
    "    num_folds = max(model_parameters.loc[model_parameters.seed == seed, 'kfold']) + 1\n",
    "    \n",
    "    # Convert string list representation to list of strings for selected_features\n",
    "    selected_features = model_parameters.loc[idx, 'selected_features'].replace(\"'\", '\"')\n",
    "    selected_features = json.loads(selected_features)\n",
    "    # Remove non-numerical features from selected_features list\n",
    "    if 'cp_type' in selected_features:\n",
    "        selected_features.remove('cp_type')\n",
    "    if 'cp_dose' in selected_features:\n",
    "        selected_features.remove('cp_dose')\n",
    "    \n",
    "    # Make test predictions\n",
    "    fold_preds = make_test_predictions(\n",
    "        X_test            = test_features, \n",
    "        selected_features = selected_features,\n",
    "        num_features      = model_parameters.iloc[idx]['num_features'],\n",
    "        num_components    = model_parameters.iloc[idx]['num_components'], \n",
    "        use_embedding     = model_parameters.iloc[idx]['use_embedding'], \n",
    "        seed              = seed, \n",
    "        kfold             = model_parameters.iloc[idx]['kfold'],\n",
    "        num_folds         = num_folds\n",
    "    )\n",
    "    preds.append(fold_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sig_id</th>\n",
       "      <th>5-alpha_reductase_inhibitor</th>\n",
       "      <th>11-beta-hsd1_inhibitor</th>\n",
       "      <th>acat_inhibitor</th>\n",
       "      <th>acetylcholine_receptor_agonist</th>\n",
       "      <th>acetylcholine_receptor_antagonist</th>\n",
       "      <th>acetylcholinesterase_inhibitor</th>\n",
       "      <th>adenosine_receptor_agonist</th>\n",
       "      <th>adenosine_receptor_antagonist</th>\n",
       "      <th>adenylyl_cyclase_activator</th>\n",
       "      <th>...</th>\n",
       "      <th>tropomyosin_receptor_kinase_inhibitor</th>\n",
       "      <th>trpv_agonist</th>\n",
       "      <th>trpv_antagonist</th>\n",
       "      <th>tubulin_inhibitor</th>\n",
       "      <th>tyrosine_kinase_inhibitor</th>\n",
       "      <th>ubiquitin_specific_protease_inhibitor</th>\n",
       "      <th>vegfr_inhibitor</th>\n",
       "      <th>vitamin_b</th>\n",
       "      <th>vitamin_d_receptor_agonist</th>\n",
       "      <th>wnt_inhibitor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id_0004d9e33</td>\n",
       "      <td>0.001691</td>\n",
       "      <td>0.001426</td>\n",
       "      <td>0.002286</td>\n",
       "      <td>0.017347</td>\n",
       "      <td>0.028212</td>\n",
       "      <td>0.007495</td>\n",
       "      <td>0.002672</td>\n",
       "      <td>0.004651</td>\n",
       "      <td>0.001185</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001537</td>\n",
       "      <td>0.002621</td>\n",
       "      <td>0.003100</td>\n",
       "      <td>0.003465</td>\n",
       "      <td>0.002033</td>\n",
       "      <td>0.001523</td>\n",
       "      <td>0.004193</td>\n",
       "      <td>0.002999</td>\n",
       "      <td>0.003710</td>\n",
       "      <td>0.002713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id_001897cda</td>\n",
       "      <td>0.000917</td>\n",
       "      <td>0.002057</td>\n",
       "      <td>0.002178</td>\n",
       "      <td>0.004902</td>\n",
       "      <td>0.004709</td>\n",
       "      <td>0.004658</td>\n",
       "      <td>0.004267</td>\n",
       "      <td>0.006555</td>\n",
       "      <td>0.004035</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002343</td>\n",
       "      <td>0.001881</td>\n",
       "      <td>0.003351</td>\n",
       "      <td>0.002179</td>\n",
       "      <td>0.017245</td>\n",
       "      <td>0.002102</td>\n",
       "      <td>0.032521</td>\n",
       "      <td>0.001765</td>\n",
       "      <td>0.003567</td>\n",
       "      <td>0.002584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id_002429b5b</td>\n",
       "      <td>0.001654</td>\n",
       "      <td>0.001826</td>\n",
       "      <td>0.001684</td>\n",
       "      <td>0.007391</td>\n",
       "      <td>0.008268</td>\n",
       "      <td>0.001490</td>\n",
       "      <td>0.003062</td>\n",
       "      <td>0.004547</td>\n",
       "      <td>0.001790</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002616</td>\n",
       "      <td>0.001503</td>\n",
       "      <td>0.003741</td>\n",
       "      <td>0.007312</td>\n",
       "      <td>0.005048</td>\n",
       "      <td>0.001836</td>\n",
       "      <td>0.021108</td>\n",
       "      <td>0.002582</td>\n",
       "      <td>0.004846</td>\n",
       "      <td>0.002329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id_00276f245</td>\n",
       "      <td>0.001403</td>\n",
       "      <td>0.001257</td>\n",
       "      <td>0.001412</td>\n",
       "      <td>0.013701</td>\n",
       "      <td>0.006676</td>\n",
       "      <td>0.006088</td>\n",
       "      <td>0.002560</td>\n",
       "      <td>0.005352</td>\n",
       "      <td>0.001283</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001351</td>\n",
       "      <td>0.001475</td>\n",
       "      <td>0.004000</td>\n",
       "      <td>0.049550</td>\n",
       "      <td>0.006395</td>\n",
       "      <td>0.001090</td>\n",
       "      <td>0.006839</td>\n",
       "      <td>0.002154</td>\n",
       "      <td>0.003174</td>\n",
       "      <td>0.003203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id_0027f1083</td>\n",
       "      <td>0.002768</td>\n",
       "      <td>0.002004</td>\n",
       "      <td>0.002412</td>\n",
       "      <td>0.018689</td>\n",
       "      <td>0.024226</td>\n",
       "      <td>0.003102</td>\n",
       "      <td>0.003743</td>\n",
       "      <td>0.004358</td>\n",
       "      <td>0.001314</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001216</td>\n",
       "      <td>0.001106</td>\n",
       "      <td>0.004042</td>\n",
       "      <td>0.001914</td>\n",
       "      <td>0.001901</td>\n",
       "      <td>0.001590</td>\n",
       "      <td>0.004227</td>\n",
       "      <td>0.002139</td>\n",
       "      <td>0.001950</td>\n",
       "      <td>0.002504</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 207 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         sig_id  5-alpha_reductase_inhibitor  11-beta-hsd1_inhibitor  \\\n",
       "0  id_0004d9e33                     0.001691                0.001426   \n",
       "1  id_001897cda                     0.000917                0.002057   \n",
       "2  id_002429b5b                     0.001654                0.001826   \n",
       "3  id_00276f245                     0.001403                0.001257   \n",
       "4  id_0027f1083                     0.002768                0.002004   \n",
       "\n",
       "   acat_inhibitor  acetylcholine_receptor_agonist  \\\n",
       "0        0.002286                        0.017347   \n",
       "1        0.002178                        0.004902   \n",
       "2        0.001684                        0.007391   \n",
       "3        0.001412                        0.013701   \n",
       "4        0.002412                        0.018689   \n",
       "\n",
       "   acetylcholine_receptor_antagonist  acetylcholinesterase_inhibitor  \\\n",
       "0                           0.028212                        0.007495   \n",
       "1                           0.004709                        0.004658   \n",
       "2                           0.008268                        0.001490   \n",
       "3                           0.006676                        0.006088   \n",
       "4                           0.024226                        0.003102   \n",
       "\n",
       "   adenosine_receptor_agonist  adenosine_receptor_antagonist  \\\n",
       "0                    0.002672                       0.004651   \n",
       "1                    0.004267                       0.006555   \n",
       "2                    0.003062                       0.004547   \n",
       "3                    0.002560                       0.005352   \n",
       "4                    0.003743                       0.004358   \n",
       "\n",
       "   adenylyl_cyclase_activator  ...  tropomyosin_receptor_kinase_inhibitor  \\\n",
       "0                    0.001185  ...                               0.001537   \n",
       "1                    0.004035  ...                               0.002343   \n",
       "2                    0.001790  ...                               0.002616   \n",
       "3                    0.001283  ...                               0.001351   \n",
       "4                    0.001314  ...                               0.001216   \n",
       "\n",
       "   trpv_agonist  trpv_antagonist  tubulin_inhibitor  \\\n",
       "0      0.002621         0.003100           0.003465   \n",
       "1      0.001881         0.003351           0.002179   \n",
       "2      0.001503         0.003741           0.007312   \n",
       "3      0.001475         0.004000           0.049550   \n",
       "4      0.001106         0.004042           0.001914   \n",
       "\n",
       "   tyrosine_kinase_inhibitor  ubiquitin_specific_protease_inhibitor  \\\n",
       "0                   0.002033                               0.001523   \n",
       "1                   0.017245                               0.002102   \n",
       "2                   0.005048                               0.001836   \n",
       "3                   0.006395                               0.001090   \n",
       "4                   0.001901                               0.001590   \n",
       "\n",
       "   vegfr_inhibitor  vitamin_b  vitamin_d_receptor_agonist  wnt_inhibitor  \n",
       "0         0.004193   0.002999                    0.003710       0.002713  \n",
       "1         0.032521   0.001765                    0.003567       0.002584  \n",
       "2         0.021108   0.002582                    0.004846       0.002329  \n",
       "3         0.006839   0.002154                    0.003174       0.003203  \n",
       "4         0.004227   0.002139                    0.001950       0.002504  \n",
       "\n",
       "[5 rows x 207 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ensemble predictions and generate submission\n",
    "for idx, fold_preds in enumerate(preds):\n",
    "    # Convert fold_preds to dataframe\n",
    "    fold_preds = pd.DataFrame(fold_preds, columns=sample_submission.columns[1:])\n",
    "\n",
    "    # Update the submission for the first round of preds\n",
    "    if idx == 0:\n",
    "        # Add sig_id feature to fold_preds\n",
    "        fold_preds['sig_id'] = sample_submission['sig_id']\n",
    "        sample_submission.update(fold_preds)\n",
    "    # Add to the preds following the first round of preds\n",
    "    else:\n",
    "        sample_submission.iloc[:, 1:] = sample_submission.iloc[:, 1:] + fold_preds\n",
    "\n",
    "# Divide summed preds by number of total folds  \n",
    "sample_submission.iloc[:, 1:] = sample_submission.iloc[:, 1:] / len(preds)\n",
    "\n",
    "sample_submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission.to_csv('submissions/submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
