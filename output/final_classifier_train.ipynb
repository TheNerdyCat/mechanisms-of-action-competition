{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mechanisms of Action (MoA) Prediction - Final Classifier\n",
    "## Train\n",
    "\n",
    "In this notebook, we will implement the full train pipeline, inclusive of feature engineering, model preparation, as well as any other meta-classifiers or pre-final classifier algorithms. Namely, the zero_label classifier - this has shown some positive performance on the leaderboard, but was not overwhelmingly positive, so we will experiment with the best location in the pipeline to put this in.\n",
    "\n",
    "When a promising baseline model is created, we will run Bayesian hyperparameter optimisation as part of our CV strategy. The model will be trained on multiple seeds also, so we can ensemble those too at the end. By the time the final submission is ready to be produced, we will have numerous models and model seeds, all optimised as much as possible, to ensemble for the final predictions. \n",
    "\n",
    "\n",
    "## 1.00 Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import gc\n",
    "import random\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Data vis packages\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Data prep\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Modelling packages\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.python.keras import backend as k\n",
    "# Key layers\n",
    "from tensorflow.keras.models import Model, Sequential, load_model\n",
    "from tensorflow.keras.layers import Input, Add, Dense, Flatten\n",
    "# Activation layers\n",
    "from tensorflow.keras.layers import ReLU, LeakyReLU, ELU, ThresholdedReLU\n",
    "# Dropout layers\n",
    "from tensorflow.keras.layers import Dropout, AlphaDropout, GaussianDropout\n",
    "# Normalisation layers\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "# Embedding layers\n",
    "from tensorflow.keras.layers import Embedding, Concatenate, Reshape\n",
    "# Callbacks\n",
    "from tensorflow.keras.callbacks import Callback, EarlyStopping, LearningRateScheduler, ModelCheckpoint\n",
    "# Optimisers\n",
    "from tensorflow.keras.optimizers import SGD, RMSprop, Adam, Adadelta, Adagrad, Adamax, Nadam, Ftrl\n",
    "# Model cross validation and evaluation\n",
    "from sklearn.model_selection import KFold\n",
    "from tensorflow.keras.losses import binary_crossentropy\n",
    "\n",
    "# For Bayesian hyperparameter searching\n",
    "from skopt import gbrt_minimize, gp_minimize\n",
    "from skopt.utils import use_named_args\n",
    "from skopt.space import Real, Categorical, Integer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n",
      "REPLICAS: 1\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "\n",
    "strategy = tf.distribute.get_strategy()\n",
    "REPLICAS = strategy.num_replicas_in_sync\n",
    "print(f'REPLICAS: {REPLICAS}')\n",
    "\n",
    "# Data access\n",
    "gpu_options = tf.compat.v1.GPUOptions(allow_growth=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.00 Read in Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_features shape: \t\t(23814, 876)\n",
      "test_features shape: \t\t(3982, 876)\n",
      "train_targets_scored shape: \t(23814, 207)\n",
      "sample_submission shape: \t(3982, 207)\n"
     ]
    }
   ],
   "source": [
    "# Directory and file paths\n",
    "input_dir                 = '../input/lish-moa/'\n",
    "train_features_path       = os.path.join(input_dir, 'train_features.csv')\n",
    "test_features_path        = os.path.join(input_dir, 'test_features.csv')\n",
    "train_targets_scored_path = os.path.join(input_dir, 'train_targets_scored.csv')\n",
    "sample_submission_path    = os.path.join(input_dir, 'sample_submission.csv')\n",
    "\n",
    "# Read in data\n",
    "train_features       = pd.read_csv(train_features_path)\n",
    "test_features        = pd.read_csv(test_features_path)\n",
    "train_targets_scored = pd.read_csv(train_targets_scored_path)\n",
    "sample_submission    = pd.read_csv(sample_submission_path)\n",
    "\n",
    "del train_features_path, test_features_path, train_targets_scored_path, sample_submission_path\n",
    "\n",
    "print(f'train_features shape: \\t\\t{train_features.shape}')\n",
    "print(f'test_features shape: \\t\\t{test_features.shape}')\n",
    "print(f'train_targets_scored shape: \\t{train_targets_scored.shape}')\n",
    "print(f'sample_submission shape: \\t{sample_submission.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model name: nn_final_classifier_seed19\n"
     ]
    }
   ],
   "source": [
    "# Define key parameters\n",
    "SEED = 19\n",
    "np.random.seed(SEED)\n",
    "\n",
    "SCALER_METHOD = RobustScaler()\n",
    "\n",
    "FEATURE_SELECTOR = RandomForestClassifier(random_state=SEED)\n",
    "NUM_FEATURES = 500\n",
    "\n",
    "NUM_COMPONENTS = 200\n",
    "PCA_METHOD = PCA(n_components=NUM_COMPONENTS, random_state=SEED)\n",
    "\n",
    "EPOCHS = 100\n",
    "BATCH_SIZE = 64\n",
    "KFOLDS = 2\n",
    "PATIENCE = 10\n",
    "\n",
    "USE_EMBEDDING = True\n",
    "MODEL_TO_USE = 'nn'\n",
    "model_name_save = MODEL_TO_USE + '_final_classifier_seed' + str(SEED)\n",
    "\n",
    "print(f'Model name: {model_name_save}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.00 Data Preparation\n",
    "\n",
    "### 3.01 Feature Engineering\n",
    "Due to the high dimensionality of the data, our engineered features focus on record-level aggregations, for all numerical features as well as for g-features and c-features. A pipeline for features that use columns-wise aggregations is created but not executed (as this would allow for data leakage). These transformations will be made in-fold during cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_features\n",
    "y = train_targets_scored.drop('sig_id', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_row_stat(df, stat, feat_type):\n",
    "    \"\"\"\n",
    "    Input data and returns row level statistics.\n",
    "    stat: str ['sum','mean','med','std','min','max']\n",
    "    feat_type: str [None,'g','c']\n",
    "    \"\"\"\n",
    "    \n",
    "    # Separate features into numerical and categorical (and by feature type if specified)\n",
    "    if feat_type == None:\n",
    "        df_numerical = df.select_dtypes('number').drop('cp_time', axis=1)\n",
    "        df_categorical = df.select_dtypes('object')\n",
    "    elif feat_type == 'g':\n",
    "        df_numerical = df.select_dtypes('number').drop('cp_time', axis=1)\n",
    "        df_categorical = df.select_dtypes('object')\n",
    "        # Subset to g features\n",
    "        df_numerical = df_numerical[df_numerical.columns[df_numerical.columns.str.startswith('g-')]]\n",
    "        df_categorical = df_categorical[df_categorical.columns[df_categorical.columns.str.startswith('g-')]]\n",
    "    elif feat_type == 'c':\n",
    "        df_numerical = df.select_dtypes('number').drop('cp_time', axis=1)\n",
    "        df_categorical = df.select_dtypes('object')\n",
    "        # Subset to g features\n",
    "        df_numerical = df_numerical[df_numerical.columns[df_numerical.columns.str.startswith('c-')]]\n",
    "        df_categorical = df_categorical[df_categorical.columns[df_categorical.columns.str.startswith('c-')]]\n",
    "        \n",
    "    # Add statistic feature\n",
    "    if stat == 'sum':\n",
    "        stat_feat = df_numerical.sum(axis=1)\n",
    "    elif stat == 'mean':\n",
    "        stat_feat = df_numerical.mean(axis=1)\n",
    "    elif stat == 'med':\n",
    "        stat_feat = df_numerical.median(axis=1)\n",
    "    elif stat == 'std':\n",
    "        stat_feat = df_numerical.std(axis=1)\n",
    "    elif stat == 'min':\n",
    "        stat_feat = df_numerical.min(axis=1)\n",
    "    elif stat == 'max':\n",
    "        stat_feat = df_numerical.max(axis=1)\n",
    "    \n",
    "    return(stat_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of original column names (so we don't make transformations using new features)\n",
    "X_cols = X.columns\n",
    "\n",
    "# Total row stats\n",
    "X['row_sum']  = get_row_stat(X[X_cols], stat='sum' , feat_type=None)\n",
    "X['row_mean'] = get_row_stat(X[X_cols], stat='mean', feat_type=None)\n",
    "X['row_med']  = get_row_stat(X[X_cols], stat='med' , feat_type=None)\n",
    "X['row_std']  = get_row_stat(X[X_cols], stat='std' , feat_type=None)\n",
    "X['row_min']  = get_row_stat(X[X_cols], stat='min' , feat_type=None)\n",
    "X['row_max']  = get_row_stat(X[X_cols], stat='max' , feat_type=None)\n",
    "# G feature row stats\n",
    "X['row_sum_g']  = get_row_stat(X[X_cols], stat='sum' , feat_type='g')\n",
    "X['row_mean_g'] = get_row_stat(X[X_cols], stat='mean', feat_type='g')\n",
    "X['row_med_g']  = get_row_stat(X[X_cols], stat='med' , feat_type='g')\n",
    "X['row_std_g']  = get_row_stat(X[X_cols], stat='std' , feat_type='g')\n",
    "X['row_min_g']  = get_row_stat(X[X_cols], stat='min' , feat_type='g')\n",
    "X['row_max_g']  = get_row_stat(X[X_cols], stat='max' , feat_type='g')\n",
    "# C feature row stats\n",
    "X['row_sum_c']  = get_row_stat(X[X_cols], stat='sum' , feat_type='c')\n",
    "X['row_mean_c'] = get_row_stat(X[X_cols], stat='mean', feat_type='c')\n",
    "X['row_med_c']  = get_row_stat(X[X_cols], stat='med' , feat_type='c')\n",
    "X['row_std_c']  = get_row_stat(X[X_cols], stat='std' , feat_type='c')\n",
    "X['row_min_c']  = get_row_stat(X[X_cols], stat='min' , feat_type='c')\n",
    "X['row_max_c']  = get_row_stat(X[X_cols], stat='max' , feat_type='c')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features: \t\t990\n",
      "Number of transformed features: 114\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sig_id</th>\n",
       "      <th>cp_type</th>\n",
       "      <th>cp_time</th>\n",
       "      <th>cp_dose</th>\n",
       "      <th>g-0</th>\n",
       "      <th>g-1</th>\n",
       "      <th>g-2</th>\n",
       "      <th>g-3</th>\n",
       "      <th>g-4</th>\n",
       "      <th>g-5</th>\n",
       "      <th>...</th>\n",
       "      <th>row_med_g_by_cp_time</th>\n",
       "      <th>row_std_g_by_cp_time</th>\n",
       "      <th>row_min_g_by_cp_time</th>\n",
       "      <th>row_max_g_by_cp_time</th>\n",
       "      <th>row_sum_c_by_cp_time</th>\n",
       "      <th>row_mean_c_by_cp_time</th>\n",
       "      <th>row_med_c_by_cp_time</th>\n",
       "      <th>row_std_c_by_cp_time</th>\n",
       "      <th>row_min_c_by_cp_time</th>\n",
       "      <th>row_max_c_by_cp_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id_000644bb2</td>\n",
       "      <td>trt_cp</td>\n",
       "      <td>24</td>\n",
       "      <td>D1</td>\n",
       "      <td>1.0620</td>\n",
       "      <td>0.5577</td>\n",
       "      <td>-0.2479</td>\n",
       "      <td>-0.6208</td>\n",
       "      <td>-0.1944</td>\n",
       "      <td>-1.0120</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000996</td>\n",
       "      <td>0.032260</td>\n",
       "      <td>-0.237125</td>\n",
       "      <td>0.141417</td>\n",
       "      <td>1.248729</td>\n",
       "      <td>0.012487</td>\n",
       "      <td>0.014988</td>\n",
       "      <td>0.021499</td>\n",
       "      <td>-0.042625</td>\n",
       "      <td>0.061833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id_000779bfc</td>\n",
       "      <td>trt_cp</td>\n",
       "      <td>72</td>\n",
       "      <td>D1</td>\n",
       "      <td>0.0743</td>\n",
       "      <td>0.4087</td>\n",
       "      <td>0.2991</td>\n",
       "      <td>0.0604</td>\n",
       "      <td>1.0190</td>\n",
       "      <td>0.5207</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000247</td>\n",
       "      <td>0.010696</td>\n",
       "      <td>-0.072736</td>\n",
       "      <td>0.077306</td>\n",
       "      <td>0.438551</td>\n",
       "      <td>0.004386</td>\n",
       "      <td>0.005024</td>\n",
       "      <td>0.005964</td>\n",
       "      <td>-0.017750</td>\n",
       "      <td>0.019556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id_000a6266a</td>\n",
       "      <td>trt_cp</td>\n",
       "      <td>48</td>\n",
       "      <td>D1</td>\n",
       "      <td>0.6280</td>\n",
       "      <td>0.5817</td>\n",
       "      <td>1.5540</td>\n",
       "      <td>-0.0764</td>\n",
       "      <td>-0.0323</td>\n",
       "      <td>1.2390</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000619</td>\n",
       "      <td>0.020839</td>\n",
       "      <td>-0.107583</td>\n",
       "      <td>0.113479</td>\n",
       "      <td>-0.419985</td>\n",
       "      <td>-0.004200</td>\n",
       "      <td>-0.001846</td>\n",
       "      <td>0.012435</td>\n",
       "      <td>-0.049250</td>\n",
       "      <td>0.024458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id_0015fd391</td>\n",
       "      <td>trt_cp</td>\n",
       "      <td>48</td>\n",
       "      <td>D1</td>\n",
       "      <td>-0.5138</td>\n",
       "      <td>-0.2491</td>\n",
       "      <td>-0.2656</td>\n",
       "      <td>0.5288</td>\n",
       "      <td>4.0620</td>\n",
       "      <td>-0.8095</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000033</td>\n",
       "      <td>0.024807</td>\n",
       "      <td>-0.088479</td>\n",
       "      <td>0.112312</td>\n",
       "      <td>-3.677556</td>\n",
       "      <td>-0.036776</td>\n",
       "      <td>-0.034885</td>\n",
       "      <td>0.025885</td>\n",
       "      <td>-0.117292</td>\n",
       "      <td>0.019160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id_001626bd3</td>\n",
       "      <td>trt_cp</td>\n",
       "      <td>72</td>\n",
       "      <td>D2</td>\n",
       "      <td>-0.3254</td>\n",
       "      <td>-0.4009</td>\n",
       "      <td>0.9700</td>\n",
       "      <td>0.6919</td>\n",
       "      <td>1.4180</td>\n",
       "      <td>-0.8244</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.013890</td>\n",
       "      <td>-0.138889</td>\n",
       "      <td>0.057708</td>\n",
       "      <td>0.287353</td>\n",
       "      <td>0.002874</td>\n",
       "      <td>0.002342</td>\n",
       "      <td>0.006829</td>\n",
       "      <td>-0.020764</td>\n",
       "      <td>0.015389</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 990 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         sig_id cp_type  cp_time cp_dose     g-0     g-1     g-2     g-3  \\\n",
       "0  id_000644bb2  trt_cp       24      D1  1.0620  0.5577 -0.2479 -0.6208   \n",
       "1  id_000779bfc  trt_cp       72      D1  0.0743  0.4087  0.2991  0.0604   \n",
       "2  id_000a6266a  trt_cp       48      D1  0.6280  0.5817  1.5540 -0.0764   \n",
       "3  id_0015fd391  trt_cp       48      D1 -0.5138 -0.2491 -0.2656  0.5288   \n",
       "4  id_001626bd3  trt_cp       72      D2 -0.3254 -0.4009  0.9700  0.6919   \n",
       "\n",
       "      g-4     g-5  ...  row_med_g_by_cp_time  row_std_g_by_cp_time  \\\n",
       "0 -0.1944 -1.0120  ...             -0.000996              0.032260   \n",
       "1  1.0190  0.5207  ...              0.000247              0.010696   \n",
       "2 -0.0323  1.2390  ...             -0.000619              0.020839   \n",
       "3  4.0620 -0.8095  ...             -0.000033              0.024807   \n",
       "4  1.4180 -0.8244  ...              0.000000              0.013890   \n",
       "\n",
       "   row_min_g_by_cp_time  row_max_g_by_cp_time  row_sum_c_by_cp_time  \\\n",
       "0             -0.237125              0.141417              1.248729   \n",
       "1             -0.072736              0.077306              0.438551   \n",
       "2             -0.107583              0.113479             -0.419985   \n",
       "3             -0.088479              0.112312             -3.677556   \n",
       "4             -0.138889              0.057708              0.287353   \n",
       "\n",
       "   row_mean_c_by_cp_time  row_med_c_by_cp_time  row_std_c_by_cp_time  \\\n",
       "0               0.012487              0.014988              0.021499   \n",
       "1               0.004386              0.005024              0.005964   \n",
       "2              -0.004200             -0.001846              0.012435   \n",
       "3              -0.036776             -0.034885              0.025885   \n",
       "4               0.002874              0.002342              0.006829   \n",
       "\n",
       "   row_min_c_by_cp_time  row_max_c_by_cp_time  \n",
       "0             -0.042625              0.061833  \n",
       "1             -0.017750              0.019556  \n",
       "2             -0.049250              0.024458  \n",
       "3             -0.117292              0.019160  \n",
       "4             -0.020764              0.015389  \n",
       "\n",
       "[5 rows x 990 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# G features row stats / row sum\n",
    "X['row_sum_g_by_row_sum']  = X['row_sum_g']  / X['row_sum']\n",
    "X['row_mean_g_by_row_sum'] = X['row_mean_g'] / X['row_sum']\n",
    "X['row_med_g_by_row_sum']  = X['row_med_g']  / X['row_sum']\n",
    "X['row_std_g_by_row_sum']  = X['row_std_g']  / X['row_sum']\n",
    "X['row_min_g_by_row_sum']  = X['row_min_g']  / X['row_sum']\n",
    "X['row_max_g_by_row_sum']  = X['row_max_g']  / X['row_sum']\n",
    "# C features row stats / row sum\n",
    "X['row_sum_c_by_row_sum']  = X['row_sum_c']  / X['row_sum']\n",
    "X['row_mean_c_by_row_sum'] = X['row_mean_c'] / X['row_sum']\n",
    "X['row_med_c_by_row_sum']  = X['row_med_c']  / X['row_sum']\n",
    "X['row_std_c_by_row_sum']  = X['row_std_c']  / X['row_sum']\n",
    "X['row_min_c_by_row_sum']  = X['row_min_c']  / X['row_sum']\n",
    "X['row_max_c_by_row_sum']  = X['row_max_c']  / X['row_sum']\n",
    "\n",
    "# G features row stats / row mean\n",
    "X['row_sum_g_by_row_mean']  = X['row_sum_g']  / X['row_mean']\n",
    "X['row_mean_g_by_row_mean'] = X['row_mean_g'] / X['row_mean']\n",
    "X['row_med_g_by_row_mean']  = X['row_med_g']  / X['row_mean']\n",
    "X['row_std_g_by_row_mean']  = X['row_std_g']  / X['row_mean']\n",
    "X['row_min_g_by_row_mean']  = X['row_min_g']  / X['row_mean']\n",
    "X['row_max_g_by_row_mean']  = X['row_max_g']  / X['row_mean']\n",
    "# C features row stats / row mean\n",
    "X['row_sum_c_by_row_mean']  = X['row_sum_c']  / X['row_mean']\n",
    "X['row_mean_c_by_row_mean'] = X['row_mean_c'] / X['row_mean']\n",
    "X['row_med_c_by_row_mean']  = X['row_med_c']  / X['row_mean']\n",
    "X['row_std_c_by_row_mean']  = X['row_std_c']  / X['row_mean']\n",
    "X['row_min_c_by_row_mean']  = X['row_min_c']  / X['row_mean']\n",
    "X['row_max_c_by_row_mean']  = X['row_max_c']  / X['row_mean']\n",
    "\n",
    "# G features row stats / row med\n",
    "X['row_sum_g_by_row_med']  = X['row_sum_g']  / X['row_med']\n",
    "X['row_mean_g_by_row_med'] = X['row_mean_g'] / X['row_med']\n",
    "X['row_med_g_by_row_med']  = X['row_med_g']  / X['row_med']\n",
    "X['row_std_g_by_row_med']  = X['row_std_g']  / X['row_med']\n",
    "X['row_min_g_by_row_med']  = X['row_min_g']  / X['row_med']\n",
    "X['row_max_g_by_row_med']  = X['row_max_g']  / X['row_med']\n",
    "# C features row stats / row med\n",
    "X['row_sum_c_by_row_med']  = X['row_sum_c']  / X['row_med']\n",
    "X['row_mean_c_by_row_med'] = X['row_mean_c'] / X['row_med']\n",
    "X['row_med_c_by_row_med']  = X['row_med_c']  / X['row_med']\n",
    "X['row_std_c_by_row_med']  = X['row_std_c']  / X['row_med']\n",
    "X['row_min_c_by_row_med']  = X['row_min_c']  / X['row_med']\n",
    "X['row_max_c_by_row_med']  = X['row_max_c']  / X['row_med']\n",
    "\n",
    "# G features row stats / row std\n",
    "X['row_sum_g_by_row_std']  = X['row_sum_g']  / X['row_std']\n",
    "X['row_mean_g_by_row_std'] = X['row_mean_g'] / X['row_std']\n",
    "X['row_med_g_by_row_std']  = X['row_med_g']  / X['row_std']\n",
    "X['row_std_g_by_row_std']  = X['row_std_g']  / X['row_std']\n",
    "X['row_min_g_by_row_std']  = X['row_min_g']  / X['row_std']\n",
    "X['row_max_g_by_row_std']  = X['row_max_g']  / X['row_std']\n",
    "# C features row stats / row std\n",
    "X['row_sum_c_by_row_std']  = X['row_sum_c']  / X['row_std']\n",
    "X['row_mean_c_by_row_std'] = X['row_mean_c'] / X['row_std']\n",
    "X['row_med_c_by_row_std']  = X['row_med_c']  / X['row_std']\n",
    "X['row_std_c_by_row_std']  = X['row_std_c']  / X['row_std']\n",
    "X['row_min_c_by_row_std']  = X['row_min_c']  / X['row_std']\n",
    "X['row_max_c_by_row_std']  = X['row_max_c']  / X['row_std']\n",
    "\n",
    "# G features row stats / row min\n",
    "X['row_sum_g_by_row_min']  = X['row_sum_g']  / X['row_min']\n",
    "X['row_mean_g_by_row_min'] = X['row_mean_g'] / X['row_min']\n",
    "X['row_med_g_by_row_min']  = X['row_med_g']  / X['row_min']\n",
    "X['row_std_g_by_row_min']  = X['row_std_g']  / X['row_min']\n",
    "X['row_min_g_by_row_min']  = X['row_min_g']  / X['row_min']\n",
    "X['row_max_g_by_row_min']  = X['row_max_g']  / X['row_min']\n",
    "# C features row stats / row min\n",
    "X['row_sum_c_by_row_min']  = X['row_sum_c']  / X['row_min']\n",
    "X['row_mean_c_by_row_min'] = X['row_mean_c'] / X['row_min']\n",
    "X['row_med_c_by_row_min']  = X['row_med_c']  / X['row_min']\n",
    "X['row_std_c_by_row_min']  = X['row_std_c']  / X['row_min']\n",
    "X['row_min_c_by_row_min']  = X['row_min_c']  / X['row_min']\n",
    "X['row_max_c_by_row_min']  = X['row_max_c']  / X['row_min']\n",
    "\n",
    "# G features row stats / row max\n",
    "X['row_sum_g_by_row_max']  = X['row_sum_g']  / X['row_max']\n",
    "X['row_mean_g_by_row_max'] = X['row_mean_g'] / X['row_max']\n",
    "X['row_med_g_by_row_max']  = X['row_med_g']  / X['row_max']\n",
    "X['row_std_g_by_row_max']  = X['row_std_g']  / X['row_max']\n",
    "X['row_min_g_by_row_max']  = X['row_min_g']  / X['row_max']\n",
    "X['row_max_g_by_row_max']  = X['row_max_g']  / X['row_max']\n",
    "# C features row stats / row max\n",
    "X['row_sum_c_by_row_max']  = X['row_sum_c']  / X['row_max']\n",
    "X['row_mean_c_by_row_max'] = X['row_mean_c'] / X['row_max']\n",
    "X['row_med_c_by_row_max']  = X['row_med_c']  / X['row_max']\n",
    "X['row_std_c_by_row_max']  = X['row_std_c']  / X['row_max']\n",
    "X['row_min_c_by_row_max']  = X['row_min_c']  / X['row_max']\n",
    "X['row_max_c_by_row_max']  = X['row_max_c']  / X['row_max']\n",
    "\n",
    "# G features row stats / C features row stats\n",
    "X['row_sum_g_by_row_sum_c']  = X['row_sum_g']  / X['row_sum_g']\n",
    "X['row_sum_g_by_row_mean_c'] = X['row_mean_g'] / X['row_mean_g']\n",
    "X['row_sum_g_by_row_med_c']  = X['row_med_g']  / X['row_med_g']\n",
    "X['row_sum_g_by_row_std_c']  = X['row_std_g']  / X['row_std_g']\n",
    "X['row_sum_g_by_row_min_c']  = X['row_min_g']  / X['row_min_g']\n",
    "X['row_sum_g_by_row_max_c']  = X['row_max_g']  / X['row_max_g']\n",
    "\n",
    "# Row stats / cp_time\n",
    "X['row_sum_by_cp_time']  = X['row_sum']  / X['cp_time']\n",
    "X['row_mean_by_cp_time'] = X['row_mean'] / X['cp_time']\n",
    "X['row_med_by_cp_time']  = X['row_med']  / X['cp_time']\n",
    "X['row_std_by_cp_time']  = X['row_std']  / X['cp_time']\n",
    "X['row_min_by_cp_time']  = X['row_min']  / X['cp_time']\n",
    "X['row_max_by_cp_time']  = X['row_max']  / X['cp_time']\n",
    "\n",
    "# G features row stats / cp_time\n",
    "X['row_sum_g_by_cp_time']  = X['row_sum_g']  / X['cp_time']\n",
    "X['row_mean_g_by_cp_time'] = X['row_mean_g'] / X['cp_time']\n",
    "X['row_med_g_by_cp_time']  = X['row_med_g']  / X['cp_time']\n",
    "X['row_std_g_by_cp_time']  = X['row_std_g']  / X['cp_time']\n",
    "X['row_min_g_by_cp_time']  = X['row_min_g']  / X['cp_time']\n",
    "X['row_max_g_by_cp_time']  = X['row_max_g']  / X['cp_time']\n",
    "\n",
    "# C features row stats / cp_time\n",
    "X['row_sum_c_by_cp_time']  = X['row_sum_c']  / X['cp_time']\n",
    "X['row_mean_c_by_cp_time'] = X['row_mean_c'] / X['cp_time']\n",
    "X['row_med_c_by_cp_time']  = X['row_med_c']  / X['cp_time']\n",
    "X['row_std_c_by_cp_time']  = X['row_std_c']  / X['cp_time']\n",
    "X['row_min_c_by_cp_time']  = X['row_min_c']  / X['cp_time']\n",
    "X['row_max_c_by_cp_time']  = X['row_max_c']  / X['cp_time']\n",
    "\n",
    "print(f'Number of features: \\t\\t{len(X.columns)}')\n",
    "print(f'Number of transformed features: {len(X.columns) - len(X_cols)}')\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_column_stat(df, stat, feat_type):\n",
    "    \"\"\"\n",
    "    Input data and returns column level statistics.\n",
    "    stat: str ['sum','mean','med','std','min','max']\n",
    "    feat_type: str [None,'g','c']\n",
    "    \"\"\"\n",
    "    \n",
    "    # Separate features into numerical and categorical (and by feature type if specified)\n",
    "    if feat_type == None:\n",
    "        df_numerical = df.select_dtypes('number').drop('cp_time', axis=1)\n",
    "        df_categorical = df.select_dtypes('object')\n",
    "    elif feat_type == 'g':\n",
    "        df_numerical = df.select_dtypes('number').drop('cp_time', axis=1)\n",
    "        df_categorical = df.select_dtypes('object')\n",
    "        # Subset to g features\n",
    "        df_numerical = df_numerical[df_numerical.columns[df_numerical.columns.str.startswith('g-')]]\n",
    "        df_categorical = df_categorical[df_categorical.columns[df_categorical.columns.str.startswith('g-')]]\n",
    "    elif feat_type == 'c':\n",
    "        df_numerical = df.select_dtypes('number').drop('cp_time', axis=1)\n",
    "        df_categorical = df.select_dtypes('object')\n",
    "        # Subset to g features\n",
    "        df_numerical = df_numerical[df_numerical.columns[df_numerical.columns.str.startswith('c-')]]\n",
    "        df_categorical = df_categorical[df_categorical.columns[df_categorical.columns.str.startswith('c-')]]\n",
    "        \n",
    "    # Add statistic feature\n",
    "    if stat == 'sum':\n",
    "        stat_feat = np.sum(df_numerical.values)\n",
    "    elif stat == 'mean':\n",
    "        stat_feat = np.mean(df_numerical.values)\n",
    "    elif stat == 'med':\n",
    "        stat_feat = np.median(df_numerical.values)\n",
    "    elif stat == 'std':\n",
    "        stat_feat = np.std(df_numerical.values)\n",
    "    elif stat == 'min':\n",
    "        stat_feat = np.min(df_numerical.values)\n",
    "    elif stat == 'max':\n",
    "        stat_feat = np.max(df_numerical.values)\n",
    "    \n",
    "    return(stat_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transformed_col_features(df, stat, row_feat_type, col_feat_type, feature_name):\n",
    "    \"\"\"\n",
    "    Input data and returns transformed features using column level statistics.\n",
    "    stat: str ['sum','mean','med','std','min','max']\n",
    "    row_feat_type: str [None,'g','c']\n",
    "    col_feat_type: str [None,'g','c']\n",
    "    feature_name: str, name to call new outputted feature\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get column level statistic\n",
    "    col_stat = get_column_stat(X[X_cols], stat=stat, feat_type=col_feat_type)\n",
    "    \n",
    "    # Redefine the feature suffix based on row_feat_type\n",
    "    if row_feat_type == None:\n",
    "        row_feat_type = ''\n",
    "    elif row_feat_type == 'g':\n",
    "        row_feat_type = '_g'\n",
    "    elif row_feat_type == 'c':\n",
    "        row_feat_type = '_c'\n",
    "    \n",
    "    # Get transformed feature\n",
    "    if stat == 'sum':\n",
    "        df[feature_name] = df['row_sum' + row_feat_type] / col_stat\n",
    "    elif stat == 'mean':\n",
    "        df[feature_name] = df['row_mean' + row_feat_type] / col_stat\n",
    "    elif stat == 'med':\n",
    "        df[feature_name] = df['row_med' + row_feat_type] / col_stat\n",
    "    elif stat == 'std':\n",
    "        df[feature_name] = df['row_std' + row_feat_type] / col_stat\n",
    "    elif stat == 'min':\n",
    "        df[feature_name] = df['row_min' + row_feat_type] / col_stat\n",
    "    elif stat == 'max':\n",
    "        df[feature_name] = df['row_max' + row_feat_type] / col_stat\n",
    "    \n",
    "    \n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.02 Full Data Manipulation Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_feature_set(X_train, X_test, y_train, y_test, \n",
    "                          verbose=0, \n",
    "                          scaler=SCALER_METHOD, \n",
    "                          feature_selector=FEATURE_SELECTOR,\n",
    "                          num_features=NUM_FEATURES,\n",
    "                          pca=PCA_METHOD, \n",
    "                          seed=SEED):\n",
    "    \"\"\"\n",
    "    Takes in X_train and X_test datasets, and applies feature transformations,\n",
    "    feature selection, scaling and pca (dependent on arguments). \n",
    "    \n",
    "    Returns transformed X_train and X_test data ready for training/prediction, and returns\n",
    "    list of numerical cols and categorical cols, for the use of creating embeddings.\n",
    "    \"\"\"\n",
    "    \n",
    "    ## DATA PREPARATION ##\n",
    "    \n",
    "    # Drop unique ID feature\n",
    "    X_train = X_train.drop('sig_id', axis=1)\n",
    "    X_test  = X_test.drop('sig_id', axis=1)\n",
    "    # Get indices for train and test dfs - we'll need these later\n",
    "    train_idx = list(X_train.index)\n",
    "    test_idx  = list(X_test.index)\n",
    "    \n",
    "    \n",
    "    ## IN-FOLD FEATURE ENGINEERING ##\n",
    "\n",
    "    if verbose == 1:\n",
    "        print('ENGINGEERING FEATURES...')\n",
    "        \n",
    "    for X_dataset in [X_train, X_test]:\n",
    "        # Total row stats / column stats\n",
    "        get_transformed_col_features(X_dataset, 'sum', None, None, 'row_sum_by_col_sum')\n",
    "        get_transformed_col_features(X_dataset, 'mean',None, None, 'row_mean_by_col_mean')\n",
    "        get_transformed_col_features(X_dataset, 'med', None, None, 'row_med_by_col_med')\n",
    "        get_transformed_col_features(X_dataset, 'std', None, None, 'row_std_by_col_std')\n",
    "        get_transformed_col_features(X_dataset, 'min', None, None, 'row_min_by_col_min')\n",
    "        get_transformed_col_features(X_dataset, 'max', None, None, 'row_max_by_col_max')\n",
    "        # G features row stats / column stats\n",
    "        get_transformed_col_features(X_dataset, 'sum', 'g', None, 'row_sum_g_by_col_sum')\n",
    "        get_transformed_col_features(X_dataset, 'mean','g', None, 'row_mean_g_by_col_mean')\n",
    "        get_transformed_col_features(X_dataset, 'med', 'g', None, 'row_med_g_by_col_med')\n",
    "        get_transformed_col_features(X_dataset, 'std', 'g', None, 'row_std_g_by_col_std')\n",
    "        get_transformed_col_features(X_dataset, 'min', 'g', None, 'row_min_g_by_col_min')\n",
    "        get_transformed_col_features(X_dataset, 'max', 'g', None, 'row_max_g_by_col_max')    \n",
    "        # C features row stats / column stats\n",
    "        get_transformed_col_features(X_dataset, 'sum', 'c', None, 'row_sum_c_by_col_sum')\n",
    "        get_transformed_col_features(X_dataset, 'mean','c', None, 'row_mean_c_by_col_mean')\n",
    "        get_transformed_col_features(X_dataset, 'med', 'c', None, 'row_med_c_by_col_med')\n",
    "        get_transformed_col_features(X_dataset, 'std', 'c', None, 'row_std_c_by_col_std')\n",
    "        get_transformed_col_features(X_dataset, 'min', 'c', None, 'row_min_c_by_col_min')\n",
    "        get_transformed_col_features(X_dataset, 'max', 'c', None, 'row_max_c_by_col_max')\n",
    "        # G features row stats / C features column stats\n",
    "        get_transformed_col_features(X_dataset, 'sum', 'g', 'c', 'row_sum_g_by_col_sum_c')\n",
    "        get_transformed_col_features(X_dataset, 'mean','g', 'c', 'row_mean_g_by_col_mean_c')\n",
    "        get_transformed_col_features(X_dataset, 'med', 'g', 'c', 'row_med_g_by_col_med_c')\n",
    "        get_transformed_col_features(X_dataset, 'std', 'g', 'c', 'row_std_g_by_col_std_c')\n",
    "        get_transformed_col_features(X_dataset, 'min', 'g', 'c', 'row_min_g_by_col_min_c')\n",
    "        get_transformed_col_features(X_dataset, 'max', 'g', 'c', 'row_max_g_by_col_max_c')\n",
    "        # C features row stats / G features column stats\n",
    "        get_transformed_col_features(X_dataset, 'sum', 'c', 'g', 'row_sum_c_by_col_sum_g')\n",
    "        get_transformed_col_features(X_dataset, 'mean','c', 'g', 'row_mean_c_by_col_mean_g')\n",
    "        get_transformed_col_features(X_dataset, 'med', 'c', 'g', 'row_med_c_by_col_med_g')\n",
    "        get_transformed_col_features(X_dataset, 'std', 'c', 'g', 'row_std_c_by_col_std_g')\n",
    "        get_transformed_col_features(X_dataset, 'min', 'c', 'g', 'row_min_c_by_col_min_g')\n",
    "        get_transformed_col_features(X_dataset, 'max', 'c', 'g', 'row_max_c_by_col_max_g')    \n",
    "\n",
    "    # Replace any infinite values generated with 0\n",
    "    X_train.replace(to_replace=[np.inf, -np.inf, np.nan], value=0, inplace=True)\n",
    "    X_test.replace(to_replace=[np.inf, -np.inf, np.nan], value=0, inplace=True)\n",
    "    \n",
    "    # Separate train data types\n",
    "    X_train_numerical   = X_train.select_dtypes('number')\n",
    "    X_train_categorical = X_train.select_dtypes('object')\n",
    "    X_train_categorical = X_train_categorical.astype('category')\n",
    "    # Separate val data types\n",
    "    X_test_numerical   = X_test.select_dtypes('number')\n",
    "    X_test_categorical = X_test.select_dtypes('object')\n",
    "    X_test_categorical = X_test_categorical.astype('category')\n",
    "    \n",
    "    # Get colnames before scaling and feature selection\n",
    "    num_cols = X_train_numerical.columns\n",
    "    cat_cols = X_train_categorical.columns\n",
    "    \n",
    "    ## SCALING ##\n",
    "    \n",
    "    if scaler != None:\n",
    "        if verbose == 1:\n",
    "            print('APPLYING SCALER...')\n",
    "            \n",
    "        # Fit and transform scaler to train and val\n",
    "        scaler.fit(X_train_numerical)\n",
    "        X_train_numerical = scaler.transform(X_train_numerical)\n",
    "        X_test_numerical  = scaler.transform(X_test_numerical)\n",
    "        # Convert to back dataframe\n",
    "        X_train_numerical = pd.DataFrame(X_train_numerical, index=train_idx, columns=num_cols)\n",
    "        X_test_numerical  = pd.DataFrame(X_test_numerical, index=test_idx, columns=num_cols)\n",
    "    \n",
    "    \n",
    "    ## FEATURE SELECTION ##\n",
    "    \n",
    "    # Feature selection is only ran on numerical data\n",
    "    if feature_selector != None:\n",
    "        if verbose == 1:\n",
    "            print('APPLYING FEATURE SELECTOR...')\n",
    "        num_cols = X_train_numerical.shape[1]\n",
    "            \n",
    "        # Fit tree based classifier to select features\n",
    "        selected_features = [] \n",
    "        labels = list(y_train.columns)\n",
    "        \n",
    "        # Run feature selection for each label and record the selected feature names\n",
    "        for label in labels:\n",
    "            # Fit feature selection model\n",
    "            feature_selector_fit = SelectFromModel(estimator=feature_selector)\n",
    "            feature_selector_fit = feature_selector_fit.fit(X_train_numerical, y_train[label])\n",
    "            \n",
    "            # Retrieve the names of the features selected for each label\n",
    "            feature_idx = feature_selector_fit.get_support()\n",
    "            feature_select = list(X_train_numerical.columns[feature_idx])\n",
    "            selected_features.append(feature_select)\n",
    "        \n",
    "        # Count numbers of times features were selected\n",
    "        selected_features = [feature for sublist in selected_features for feature in sublist]\n",
    "        selected_features = pd.Series(selected_features).value_counts()\n",
    "        selected_features = selected_features.sort_values(ascending=False).reset_index()\n",
    "        # Select top n features, based on num_features\n",
    "        selected_features = list(selected_features[:num_features].rename(columns={'index':'feature'})['feature'])\n",
    "        # Subset datasets to selected features only\n",
    "        X_train_numerical = X_train_numerical[selected_features]\n",
    "        X_test_numerical  = X_test_numerical[selected_features]\n",
    "        # Store column names for selected features\n",
    "        selected_features = [selected_features, cat_cols]\n",
    "        selected_features = [item for sublist in selected_features for item in sublist]\n",
    "        \n",
    "        if verbose == 1: \n",
    "            print(f'{num_cols - X_train_numerical.shape[1]} features removed in feature selection.')\n",
    "            del num_cols\n",
    "        \n",
    "            \n",
    "    ## PCA ##\n",
    "    \n",
    "    if pca != None:\n",
    "        if verbose == 1:\n",
    "            print('APPLYING PCA...')\n",
    "        # Fit and transform pca to train and val\n",
    "        pca.fit(X_train_numerical)\n",
    "        X_train_numerical = pca.transform(X_train_numerical)\n",
    "        X_test_numerical  = pca.transform(X_test_numerical)\n",
    "        if verbose == 1:\n",
    "            print(f'NUMBER OF PRINCIPAL COMPONENTS: {pca.n_components_}')\n",
    "        # Convert numerical features into pandas dataframe and clean colnames\n",
    "        X_train_numerical = pd.DataFrame(X_train_numerical, index=train_idx).add_prefix('pca_')\n",
    "        X_test_numerical  = pd.DataFrame(X_test_numerical, index=test_idx).add_prefix('pca_')\n",
    "    \n",
    "    \n",
    "    ## CATEGORICAL FEATURES ##\n",
    "    \n",
    "    # Get categorical and numerical column names\n",
    "    num_cols = X_train_numerical.columns\n",
    "    cat_cols = X_train_categorical.columns\n",
    "\n",
    "    # Encode categorical features\n",
    "    X_train_categorical = X_train_categorical.apply(lambda x: x.cat.codes)\n",
    "    X_test_categorical  = X_test_categorical.apply(lambda x: x.cat.codes)\n",
    "\n",
    "    # Concatenate transformed categorical features with transformed numerical features  \n",
    "    X_train = pd.concat([X_train_categorical, X_train_numerical], axis=1)\n",
    "    X_test  = pd.concat([X_test_categorical, X_test_numerical], axis=1)\n",
    "    \n",
    "    if verbose == 1:\n",
    "        print(f'TRAIN SHAPE: \\t\\t{X_train.shape}')\n",
    "        print(f'VALIDATION SHAPE: \\t{X_test.shape}')\n",
    "    \n",
    "    return X_train, X_test, num_cols, cat_cols, selected_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.00 Modelling\n",
    "### 4.01 Learning Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def build_lrfn(lr_start          = 0.00001, \n",
    "               lr_max            = 0.0008, \n",
    "               lr_min            = 0.00001, \n",
    "               lr_rampup_epochs  = 20, \n",
    "               lr_sustain_epochs = 0, \n",
    "               lr_exp_decay      = 0.8):\n",
    "    \n",
    "    lr_max = lr_max * strategy.num_replicas_in_sync\n",
    "\n",
    "    def lrfn(epoch):\n",
    "        if epoch < lr_rampup_epochs:\n",
    "            lr = (lr_max - lr_start) / lr_rampup_epochs * epoch + lr_start\n",
    "        elif epoch < lr_rampup_epochs + lr_sustain_epochs:\n",
    "            lr = lr_max\n",
    "        else:\n",
    "            lr = (lr_max - lr_min) * lr_exp_decay**(epoch - lr_rampup_epochs - lr_sustain_epochs) + lr_min\n",
    "        return lr\n",
    "\n",
    "    return lrfn\n",
    "\n",
    "lrfn = build_lrfn()\n",
    "lr = LearningRateScheduler(lrfn, verbose=0)\n",
    "\n",
    "plt.plot([lrfn(epoch) for epoch in range(EPOCHS)])\n",
    "plt.title('Learning Rate Schedule')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Learning Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.02 Define Baseline Model\n",
    "\n",
    "The below model was the original architecture, however when we conduct our Bayesian Hyperparameter search, we'll be playing around with the architecture of this baseline model a little. Parameter tuning will affect the model depth as well as the numbers of nodes at each layer, the dropout layers, activation functions and optimisers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(X_data, numerical_cols, categorical_cols, use_embedding=USE_EMBEDDING):\n",
    "    \n",
    "    if use_embedding == True:\n",
    "        inputs = []\n",
    "        embeddings = []\n",
    "\n",
    "        for col in categorical_cols:\n",
    "            # Create categorical embedding\n",
    "            input_ = Input(shape=(1,))\n",
    "            input_dim = int(X_data[col].max() + 1)\n",
    "            embedding = Embedding(input_dim=input_dim, output_dim=10, input_length=1)(input_)\n",
    "            embedding = Reshape(target_shape=(10,))(embedding)\n",
    "            inputs.append(input_)\n",
    "            embeddings.append(embedding)\n",
    "\n",
    "        input_numeric = Input(shape=(len(numerical_cols),))\n",
    "        embedding_numeric = Dense(8192, activation='relu')(input_numeric) \n",
    "        inputs.append(input_numeric)\n",
    "        embeddings.append(embedding_numeric)\n",
    "\n",
    "        x = Concatenate()(embeddings)\n",
    "        \n",
    "    if use_embedding == False:\n",
    "        input_ = Input(shape=(X_data.shape[1], ))\n",
    "        x = Dense(8192, activation='relu')(input_)\n",
    "        \n",
    "    x = Dense(4096, activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.5)(x) \n",
    "    \n",
    "    x = Dense(2048, activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "\n",
    "    x = Dense(1024, activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    \n",
    "    x = Dense(512, activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    \n",
    "    x = Dense(256, activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    \n",
    "    output = Dense(y.shape[1], activation='sigmoid')(x)\n",
    "\n",
    "    if use_embedding == True:\n",
    "        model = Model(inputs, output)\n",
    "    elif use_embedding == False:\n",
    "        model = Model(input_, output)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metric(y_true, y_pred):\n",
    "     return binary_crossentropy(y_true, y_pred).numpy().mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.03 Bayesian Hyperparameter Search\n",
    "\n",
    "Here we will conduct Bayesian hyperparameter search for a number of different parameters in our model - and save the best performing model for each fold. Some of what was coded above will be overwritten, but I wanted to leave the above model in the notebook so we have a record of what our baseline model was before we conducted hyperparameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Redefine hyperparamater constants\n",
    "EPOCHS           = 100\n",
    "SEED             = 19\n",
    "np.random.seed(SEED)\n",
    "SCALER_METHOD    = RobustScaler()\n",
    "FEATURE_SELECTOR = RandomForestClassifier(random_state=SEED)\n",
    "KFOLDS           = 2\n",
    "MODEL_TO_USE     = 'nn'\n",
    "model_name_save  = MODEL_TO_USE + '_final_classifier_seed' + str(SEED)\n",
    "\n",
    "# Create model directory path if does not exist already\n",
    "if not os.path.exists(f'models/{model_name_save}'):\n",
    "    os.mkdir(f'models/{model_name_save}')\n",
    "\n",
    "# Define hyperparameter search dimensions\n",
    "dim_num_features     = Integer(low=300, high=900, name='num_features')\n",
    "dim_num_components   = Integer(low=20,  high=300,  name='num_components')\n",
    "dim_learning_rate    = Real(low=1e-4,   high=1e-2, prior='log-uniform', name='learning_rate')\n",
    "dim_num_dense_layers = Integer(low=1,   high=6,    name='num_dense_layers')\n",
    "dim_num_input_nodes  = Integer(low=1,   high=4096, name='num_input_nodes')\n",
    "dim_num_dense_nodes  = Integer(low=1,   high=4096, name='num_dense_nodes')\n",
    "dim_activation = Categorical(categories=['relu','leaky_relu','elu','threshold_relu'], name='activation')\n",
    "dim_batch_size       = Integer(low=1,   high=64,   name='batch_size')\n",
    "dim_patience         = Integer(low=3,   high=15,   name='patience')\n",
    "dim_optimiser = Categorical(categories=['sgd','adam','rms_prop','ada_delta','ada_grad',\n",
    "                                        'ada_max','n_adam','ftrl'], name='optimiser')\n",
    "dim_optimiser_decay  = Real(low=1e-6,   high=1e-2, name='optimiser_decay')\n",
    "dim_dropout_layer = Categorical(categories=['dropout','gaussian_dropout','alpha_dropout'],name='dropout_layer')\n",
    "dim_dropout_val      = Real(low=0.1,    high=0.8,  name='dropout_val')\n",
    "dim_use_embedding    = Integer(low=0,   high=1,    name='use_embedding')\n",
    "\n",
    "\n",
    "dimensions = [dim_num_features,\n",
    "              dim_num_components,\n",
    "              dim_learning_rate,\n",
    "              dim_num_dense_layers,\n",
    "              dim_num_input_nodes,\n",
    "              dim_num_dense_nodes,\n",
    "              dim_activation,\n",
    "              dim_batch_size,\n",
    "              dim_patience,\n",
    "              dim_optimiser,\n",
    "              dim_optimiser_decay,\n",
    "              dim_dropout_layer,\n",
    "              dim_dropout_val,\n",
    "              dim_use_embedding\n",
    "             ]\n",
    "\n",
    "# Set default hyperparameters\n",
    "default_parameters = [\n",
    "    500,       # num_features\n",
    "    200,       # num_components\n",
    "    1e-3,      # learning_rate\n",
    "    1,         # num_dense_layers\n",
    "    512,       # num_input_nodes\n",
    "    16,        # num_dense_nodes\n",
    "    'relu',    # activation\n",
    "    64,        # batch_size\n",
    "    3,         # patience\n",
    "    'adam',    # optimiser\n",
    "    1e-3,      # optimiser_decay\n",
    "    'dropout', # dropout_layer\n",
    "    0.1,       # dropout_val\n",
    "    1          # use_embedding\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.04 Train Model with Bayesian Hyperparameter Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD 0\n",
      "--------------------------------------------------------------------------------------------------\n",
      "RUNNING PARAMETER SEARCH...\n",
      "\n",
      "ENGINGEERING FEATURES...\n",
      "APPLYING SCALER...\n",
      "APPLYING FEATURE SELECTOR...\n",
      "517 features removed in feature selection.\n",
      "APPLYING PCA...\n",
      "NUMBER OF PRINCIPAL COMPONENTS: 200\n",
      "TRAIN SHAPE: \t\t(11907, 202)\n",
      "VALIDATION SHAPE: \t(11907, 202)\n",
      "TRAINING...\n",
      "CURRENT LOSS: \t\t0.018638930950773056\n",
      "BEST LOSS: \t\t0.018638930950773056\n",
      "\n",
      "ENGINGEERING FEATURES...\n",
      "APPLYING SCALER...\n",
      "APPLYING FEATURE SELECTOR...\n",
      "459 features removed in feature selection.\n",
      "APPLYING PCA...\n",
      "NUMBER OF PRINCIPAL COMPONENTS: 136\n",
      "TRAIN SHAPE: \t\t(11907, 138)\n",
      "VALIDATION SHAPE: \t(11907, 138)\n",
      "TRAINING...\n",
      "CURRENT LOSS: \t\t0.02505427516222896\n",
      "BEST LOSS: \t\t0.018638930950773056\n",
      "\n",
      "ENGINGEERING FEATURES...\n",
      "APPLYING SCALER...\n",
      "APPLYING FEATURE SELECTOR...\n",
      "559 features removed in feature selection.\n",
      "APPLYING PCA...\n",
      "NUMBER OF PRINCIPAL COMPONENTS: 187\n",
      "TRAIN SHAPE: \t\t(11907, 189)\n",
      "VALIDATION SHAPE: \t(11907, 189)\n",
      "TRAINING...\n",
      "CURRENT LOSS: \t\t0.02140231153667774\n",
      "BEST LOSS: \t\t0.018638930950773056\n",
      "\n",
      "ENGINGEERING FEATURES...\n",
      "APPLYING SCALER...\n",
      "APPLYING FEATURE SELECTOR...\n",
      "350 features removed in feature selection.\n",
      "APPLYING PCA...\n",
      "NUMBER OF PRINCIPAL COMPONENTS: 238\n",
      "TRAIN SHAPE: \t\t(11907, 240)\n",
      "VALIDATION SHAPE: \t(11907, 240)\n",
      "TRAINING...\n",
      "CURRENT LOSS: \t\t0.019721339932565606\n",
      "BEST LOSS: \t\t0.018638930950773056\n",
      "\n",
      "ENGINGEERING FEATURES...\n",
      "APPLYING SCALER...\n",
      "APPLYING FEATURE SELECTOR...\n",
      "657 features removed in feature selection.\n",
      "APPLYING PCA...\n",
      "NUMBER OF PRINCIPAL COMPONENTS: 71\n",
      "TRAIN SHAPE: \t\t(11907, 73)\n",
      "VALIDATION SHAPE: \t(11907, 73)\n",
      "TRAINING...\n",
      "CURRENT LOSS: \t\t0.023828073590868745\n",
      "BEST LOSS: \t\t0.018638930950773056\n",
      "\n",
      "ENGINGEERING FEATURES...\n",
      "APPLYING SCALER...\n",
      "APPLYING FEATURE SELECTOR...\n",
      "634 features removed in feature selection.\n",
      "APPLYING PCA...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W1130 23:17:40.011047 140298787333952 nn_ops.py:4372] Large dropout rate: 0.537026 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NUMBER OF PRINCIPAL COMPONENTS: 273\n",
      "TRAIN SHAPE: \t\t(11907, 275)\n",
      "VALIDATION SHAPE: \t(11907, 275)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1130 23:17:40.046289 140298787333952 nn_ops.py:4372] Large dropout rate: 0.537026 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "W1130 23:17:40.080121 140298787333952 nn_ops.py:4372] Large dropout rate: 0.537026 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "W1130 23:17:40.113681 140298787333952 nn_ops.py:4372] Large dropout rate: 0.537026 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "W1130 23:17:40.225696 140298787333952 nn_ops.py:4372] Large dropout rate: 0.537026 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING...\n",
      "CURRENT LOSS: \t\t0.023078378897471878\n",
      "BEST LOSS: \t\t0.018638930950773056\n",
      "\n",
      "ENGINGEERING FEATURES...\n",
      "APPLYING SCALER...\n",
      "APPLYING FEATURE SELECTOR...\n",
      "272 features removed in feature selection.\n",
      "APPLYING PCA...\n",
      "NUMBER OF PRINCIPAL COMPONENTS: 21\n",
      "TRAIN SHAPE: \t\t(11907, 23)\n",
      "VALIDATION SHAPE: \t(11907, 23)\n",
      "TRAINING...\n",
      "CURRENT LOSS: \t\t0.02244036524942308\n",
      "BEST LOSS: \t\t0.018638930950773056\n",
      "\n",
      "ENGINGEERING FEATURES...\n",
      "APPLYING SCALER...\n",
      "APPLYING FEATURE SELECTOR...\n",
      "521 features removed in feature selection.\n",
      "APPLYING PCA...\n",
      "NUMBER OF PRINCIPAL COMPONENTS: 214\n",
      "TRAIN SHAPE: \t\t(11907, 216)\n",
      "VALIDATION SHAPE: \t(11907, 216)\n",
      "TRAINING...\n",
      "CURRENT LOSS: \t\t0.02127395842604772\n",
      "BEST LOSS: \t\t0.018638930950773056\n",
      "\n",
      "ENGINGEERING FEATURES...\n",
      "APPLYING SCALER...\n",
      "APPLYING FEATURE SELECTOR...\n",
      "162 features removed in feature selection.\n",
      "APPLYING PCA...\n",
      "NUMBER OF PRINCIPAL COMPONENTS: 44\n",
      "TRAIN SHAPE: \t\t(11907, 46)\n",
      "VALIDATION SHAPE: \t(11907, 46)\n",
      "TRAINING...\n",
      "CURRENT LOSS: \t\t0.023384411518014955\n",
      "BEST LOSS: \t\t0.018638930950773056\n",
      "\n",
      "ENGINGEERING FEATURES...\n",
      "APPLYING SCALER...\n",
      "APPLYING FEATURE SELECTOR...\n",
      "606 features removed in feature selection.\n",
      "APPLYING PCA...\n",
      "NUMBER OF PRINCIPAL COMPONENTS: 77\n",
      "TRAIN SHAPE: \t\t(11907, 79)\n",
      "VALIDATION SHAPE: \t(11907, 79)\n",
      "TRAINING...\n",
      "CURRENT LOSS: \t\t0.02339637980645921\n",
      "BEST LOSS: \t\t0.018638930950773056\n",
      "\n",
      "ENGINGEERING FEATURES...\n",
      "APPLYING SCALER...\n",
      "APPLYING FEATURE SELECTOR...\n",
      "317 features removed in feature selection.\n",
      "APPLYING PCA...\n",
      "NUMBER OF PRINCIPAL COMPONENTS: 50\n",
      "TRAIN SHAPE: \t\t(11907, 52)\n",
      "VALIDATION SHAPE: \t(11907, 52)\n",
      "TRAINING...\n",
      "CURRENT LOSS: \t\t0.0224667062309061\n",
      "BEST LOSS: \t\t0.018638930950773056\n",
      "\n",
      "ENGINGEERING FEATURES...\n",
      "APPLYING SCALER...\n",
      "APPLYING FEATURE SELECTOR...\n",
      "294 features removed in feature selection.\n",
      "APPLYING PCA...\n",
      "NUMBER OF PRINCIPAL COMPONENTS: 285\n",
      "TRAIN SHAPE: \t\t(11907, 287)\n",
      "VALIDATION SHAPE: \t(11907, 287)\n",
      "TRAINING...\n",
      "CURRENT LOSS: \t\t0.019347763150745205\n",
      "BEST LOSS: \t\t0.018638930950773056\n",
      "\n",
      "ENGINGEERING FEATURES...\n",
      "APPLYING SCALER...\n",
      "APPLYING FEATURE SELECTOR...\n",
      "124 features removed in feature selection.\n",
      "APPLYING PCA...\n",
      "NUMBER OF PRINCIPAL COMPONENTS: 247\n",
      "TRAIN SHAPE: \t\t(11907, 249)\n",
      "VALIDATION SHAPE: \t(11907, 249)\n",
      "TRAINING...\n",
      "CURRENT LOSS: \t\t0.022902159359109633\n",
      "BEST LOSS: \t\t0.018638930950773056\n",
      "\n",
      "ENGINGEERING FEATURES...\n",
      "APPLYING SCALER...\n",
      "APPLYING FEATURE SELECTOR...\n",
      "118 features removed in feature selection.\n",
      "APPLYING PCA...\n",
      "NUMBER OF PRINCIPAL COMPONENTS: 254\n",
      "TRAIN SHAPE: \t\t(11907, 256)\n",
      "VALIDATION SHAPE: \t(11907, 256)\n",
      "TRAINING...\n",
      "CURRENT LOSS: \t\t0.024781385063749654\n",
      "BEST LOSS: \t\t0.018638930950773056\n",
      "\n",
      "ENGINGEERING FEATURES...\n",
      "APPLYING SCALER...\n",
      "APPLYING FEATURE SELECTOR...\n",
      "706 features removed in feature selection.\n",
      "APPLYING PCA...\n",
      "NUMBER OF PRINCIPAL COMPONENTS: 300\n",
      "TRAIN SHAPE: \t\t(11907, 302)\n",
      "VALIDATION SHAPE: \t(11907, 302)\n",
      "TRAINING...\n",
      "CURRENT LOSS: \t\t0.023146651485802836\n",
      "BEST LOSS: \t\t0.018638930950773056\n",
      "\n",
      "ENGINGEERING FEATURES...\n",
      "APPLYING SCALER...\n",
      "APPLYING FEATURE SELECTOR...\n",
      "192 features removed in feature selection.\n",
      "APPLYING PCA...\n",
      "NUMBER OF PRINCIPAL COMPONENTS: 292\n",
      "TRAIN SHAPE: \t\t(11907, 294)\n",
      "VALIDATION SHAPE: \t(11907, 294)\n",
      "TRAINING...\n",
      "CURRENT LOSS: \t\t0.022902541478853677\n",
      "BEST LOSS: \t\t0.018638930950773056\n",
      "\n",
      "ENGINGEERING FEATURES...\n",
      "APPLYING SCALER...\n",
      "APPLYING FEATURE SELECTOR...\n",
      "268 features removed in feature selection.\n",
      "APPLYING PCA...\n",
      "NUMBER OF PRINCIPAL COMPONENTS: 295\n",
      "TRAIN SHAPE: \t\t(11907, 297)\n",
      "VALIDATION SHAPE: \t(11907, 297)\n",
      "TRAINING...\n",
      "CURRENT LOSS: \t\t0.025125856453263155\n",
      "BEST LOSS: \t\t0.018638930950773056\n",
      "\n",
      "ENGINGEERING FEATURES...\n",
      "APPLYING SCALER...\n",
      "APPLYING FEATURE SELECTOR...\n",
      "681 features removed in feature selection.\n",
      "APPLYING PCA...\n",
      "NUMBER OF PRINCIPAL COMPONENTS: 35\n",
      "TRAIN SHAPE: \t\t(11907, 37)\n",
      "VALIDATION SHAPE: \t(11907, 37)\n",
      "TRAINING...\n",
      "CURRENT LOSS: \t\t0.02981917708949491\n",
      "BEST LOSS: \t\t0.018638930950773056\n",
      "\n",
      "ENGINGEERING FEATURES...\n",
      "APPLYING SCALER...\n",
      "APPLYING FEATURE SELECTOR...\n",
      "142 features removed in feature selection.\n",
      "APPLYING PCA...\n",
      "NUMBER OF PRINCIPAL COMPONENTS: 83\n",
      "TRAIN SHAPE: \t\t(11907, 85)\n",
      "VALIDATION SHAPE: \t(11907, 85)\n",
      "TRAINING...\n",
      "CURRENT LOSS: \t\t0.019734059865588023\n",
      "BEST LOSS: \t\t0.018638930950773056\n",
      "\n",
      "ENGINGEERING FEATURES...\n",
      "APPLYING SCALER...\n",
      "APPLYING FEATURE SELECTOR...\n",
      "715 features removed in feature selection.\n",
      "APPLYING PCA...\n",
      "NUMBER OF PRINCIPAL COMPONENTS: 21\n",
      "TRAIN SHAPE: \t\t(11907, 23)\n",
      "VALIDATION SHAPE: \t(11907, 23)\n",
      "TRAINING...\n",
      "CURRENT LOSS: \t\t0.023549431806688625\n",
      "BEST LOSS: \t\t0.018638930950773056\n",
      "\n",
      "ENGINGEERING FEATURES...\n",
      "APPLYING SCALER...\n",
      "APPLYING FEATURE SELECTOR...\n",
      "697 features removed in feature selection.\n",
      "APPLYING PCA...\n",
      "NUMBER OF PRINCIPAL COMPONENTS: 34\n",
      "TRAIN SHAPE: \t\t(11907, 36)\n",
      "VALIDATION SHAPE: \t(11907, 36)\n",
      "TRAINING...\n",
      "CURRENT LOSS: \t\t0.02372166776024168\n",
      "BEST LOSS: \t\t0.018638930950773056\n",
      "\n",
      "ENGINGEERING FEATURES...\n",
      "APPLYING SCALER...\n",
      "APPLYING FEATURE SELECTOR...\n",
      "236 features removed in feature selection.\n",
      "APPLYING PCA...\n",
      "NUMBER OF PRINCIPAL COMPONENTS: 72\n",
      "TRAIN SHAPE: \t\t(11907, 74)\n",
      "VALIDATION SHAPE: \t(11907, 74)\n",
      "TRAINING...\n",
      "CURRENT LOSS: \t\t0.023620978492848027\n",
      "BEST LOSS: \t\t0.018638930950773056\n",
      "\n",
      "ENGINGEERING FEATURES...\n",
      "APPLYING SCALER...\n",
      "APPLYING FEATURE SELECTOR...\n",
      "167 features removed in feature selection.\n",
      "APPLYING PCA...\n",
      "NUMBER OF PRINCIPAL COMPONENTS: 69\n",
      "TRAIN SHAPE: \t\t(11907, 71)\n",
      "VALIDATION SHAPE: \t(11907, 71)\n",
      "TRAINING...\n",
      "CURRENT LOSS: \t\t0.02290266177619829\n",
      "BEST LOSS: \t\t0.018638930950773056\n",
      "\n",
      "ENGINGEERING FEATURES...\n",
      "APPLYING SCALER...\n",
      "APPLYING FEATURE SELECTOR...\n",
      "713 features removed in feature selection.\n",
      "APPLYING PCA...\n",
      "NUMBER OF PRINCIPAL COMPONENTS: 296\n",
      "TRAIN SHAPE: \t\t(11907, 298)\n",
      "VALIDATION SHAPE: \t(11907, 298)\n",
      "TRAINING...\n",
      "CURRENT LOSS: \t\t0.02342585924115604\n",
      "BEST LOSS: \t\t0.018638930950773056\n",
      "\n",
      "ENGINGEERING FEATURES...\n",
      "APPLYING SCALER...\n",
      "APPLYING FEATURE SELECTOR...\n",
      "217 features removed in feature selection.\n",
      "APPLYING PCA...\n",
      "NUMBER OF PRINCIPAL COMPONENTS: 277\n",
      "TRAIN SHAPE: \t\t(11907, 279)\n",
      "VALIDATION SHAPE: \t(11907, 279)\n",
      "TRAINING...\n",
      "CURRENT LOSS: \t\t0.01946407154159915\n",
      "BEST LOSS: \t\t0.018638930950773056\n",
      "\n",
      "ENGINGEERING FEATURES...\n",
      "APPLYING SCALER...\n",
      "APPLYING FEATURE SELECTOR...\n",
      "228 features removed in feature selection.\n",
      "APPLYING PCA...\n",
      "NUMBER OF PRINCIPAL COMPONENTS: 256\n",
      "TRAIN SHAPE: \t\t(11907, 258)\n",
      "VALIDATION SHAPE: \t(11907, 258)\n",
      "TRAINING...\n",
      "CURRENT LOSS: \t\t0.022915812319524394\n",
      "BEST LOSS: \t\t0.018638930950773056\n",
      "\n",
      "ENGINGEERING FEATURES...\n",
      "APPLYING SCALER...\n",
      "APPLYING FEATURE SELECTOR...\n",
      "529 features removed in feature selection.\n",
      "APPLYING PCA...\n",
      "NUMBER OF PRINCIPAL COMPONENTS: 34\n",
      "TRAIN SHAPE: \t\t(11907, 36)\n",
      "VALIDATION SHAPE: \t(11907, 36)\n",
      "TRAINING...\n",
      "CURRENT LOSS: \t\t0.018800353559737023\n",
      "BEST LOSS: \t\t0.018638930950773056\n",
      "\n",
      "ENGINGEERING FEATURES...\n",
      "APPLYING SCALER...\n",
      "APPLYING FEATURE SELECTOR...\n",
      "237 features removed in feature selection.\n",
      "APPLYING PCA...\n",
      "NUMBER OF PRINCIPAL COMPONENTS: 36\n",
      "TRAIN SHAPE: \t\t(11907, 38)\n",
      "VALIDATION SHAPE: \t(11907, 38)\n",
      "TRAINING...\n",
      "CURRENT LOSS: \t\t0.023441812148030507\n",
      "BEST LOSS: \t\t0.018638930950773056\n",
      "\n",
      "ENGINGEERING FEATURES...\n",
      "APPLYING SCALER...\n",
      "APPLYING FEATURE SELECTOR...\n",
      "636 features removed in feature selection.\n",
      "APPLYING PCA...\n",
      "NUMBER OF PRINCIPAL COMPONENTS: 48\n",
      "TRAIN SHAPE: \t\t(11907, 50)\n",
      "VALIDATION SHAPE: \t(11907, 50)\n",
      "TRAINING...\n",
      "CURRENT LOSS: \t\t0.022902593738657537\n",
      "BEST LOSS: \t\t0.018638930950773056\n",
      "\n",
      "ENGINGEERING FEATURES...\n",
      "APPLYING SCALER...\n",
      "APPLYING FEATURE SELECTOR...\n",
      "646 features removed in feature selection.\n",
      "APPLYING PCA...\n",
      "NUMBER OF PRINCIPAL COMPONENTS: 239\n",
      "TRAIN SHAPE: \t\t(11907, 241)\n",
      "VALIDATION SHAPE: \t(11907, 241)\n",
      "TRAINING...\n",
      "CURRENT LOSS: \t\t0.022542345616309523\n",
      "BEST LOSS: \t\t0.018638930950773056\n",
      "\n",
      "ENGINGEERING FEATURES...\n",
      "APPLYING SCALER...\n",
      "APPLYING FEATURE SELECTOR...\n",
      "207 features removed in feature selection.\n",
      "APPLYING PCA...\n",
      "NUMBER OF PRINCIPAL COMPONENTS: 239\n",
      "TRAIN SHAPE: \t\t(11907, 241)\n",
      "VALIDATION SHAPE: \t(11907, 241)\n",
      "TRAINING...\n",
      "CURRENT LOSS: \t\t0.019811312373643718\n",
      "BEST LOSS: \t\t0.018638930950773056\n",
      "\n",
      "ENGINGEERING FEATURES...\n",
      "APPLYING SCALER...\n",
      "APPLYING FEATURE SELECTOR...\n",
      "704 features removed in feature selection.\n",
      "APPLYING PCA...\n",
      "NUMBER OF PRINCIPAL COMPONENTS: 21\n",
      "TRAIN SHAPE: \t\t(11907, 23)\n",
      "VALIDATION SHAPE: \t(11907, 23)\n",
      "TRAINING...\n",
      "CURRENT LOSS: \t\t0.023338358450729773\n",
      "BEST LOSS: \t\t0.018638930950773056\n",
      "\n",
      "ENGINGEERING FEATURES...\n",
      "APPLYING SCALER...\n",
      "APPLYING FEATURE SELECTOR...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "658 features removed in feature selection.\n",
      "APPLYING PCA...\n",
      "NUMBER OF PRINCIPAL COMPONENTS: 287\n",
      "TRAIN SHAPE: \t\t(11907, 289)\n",
      "VALIDATION SHAPE: \t(11907, 289)\n",
      "TRAINING...\n",
      "CURRENT LOSS: \t\t0.02276133320509867\n",
      "BEST LOSS: \t\t0.018638930950773056\n",
      "\n",
      "ENGINGEERING FEATURES...\n",
      "APPLYING SCALER...\n",
      "APPLYING FEATURE SELECTOR...\n",
      "117 features removed in feature selection.\n",
      "APPLYING PCA...\n",
      "NUMBER OF PRINCIPAL COMPONENTS: 71\n",
      "TRAIN SHAPE: \t\t(11907, 73)\n",
      "VALIDATION SHAPE: \t(11907, 73)\n",
      "TRAINING...\n",
      "CURRENT LOSS: \t\t0.019439229036709197\n",
      "BEST LOSS: \t\t0.018638930950773056\n",
      "\n",
      "ENGINGEERING FEATURES...\n",
      "APPLYING SCALER...\n",
      "APPLYING FEATURE SELECTOR...\n",
      "682 features removed in feature selection.\n",
      "APPLYING PCA...\n",
      "NUMBER OF PRINCIPAL COMPONENTS: 38\n",
      "TRAIN SHAPE: \t\t(11907, 40)\n",
      "VALIDATION SHAPE: \t(11907, 40)\n",
      "TRAINING...\n",
      "CURRENT LOSS: \t\t0.023343518481464404\n",
      "BEST LOSS: \t\t0.018638930950773056\n",
      "\n",
      "ENGINGEERING FEATURES...\n",
      "APPLYING SCALER...\n",
      "APPLYING FEATURE SELECTOR...\n",
      "712 features removed in feature selection.\n",
      "APPLYING PCA...\n",
      "NUMBER OF PRINCIPAL COMPONENTS: 277\n",
      "TRAIN SHAPE: \t\t(11907, 279)\n",
      "VALIDATION SHAPE: \t(11907, 279)\n",
      "TRAINING...\n",
      "CURRENT LOSS: \t\t0.022902564262626532\n",
      "BEST LOSS: \t\t0.018638930950773056\n",
      "\n",
      "ENGINGEERING FEATURES...\n",
      "APPLYING SCALER...\n",
      "APPLYING FEATURE SELECTOR...\n",
      "163 features removed in feature selection.\n",
      "APPLYING PCA...\n",
      "NUMBER OF PRINCIPAL COMPONENTS: 287\n",
      "TRAIN SHAPE: \t\t(11907, 289)\n",
      "VALIDATION SHAPE: \t(11907, 289)\n",
      "TRAINING...\n",
      "CURRENT LOSS: \t\t0.022699548827210828\n",
      "BEST LOSS: \t\t0.018638930950773056\n",
      "\n",
      "ENGINGEERING FEATURES...\n",
      "APPLYING SCALER...\n",
      "APPLYING FEATURE SELECTOR...\n",
      "364 features removed in feature selection.\n",
      "APPLYING PCA...\n",
      "NUMBER OF PRINCIPAL COMPONENTS: 86\n",
      "TRAIN SHAPE: \t\t(11907, 88)\n",
      "VALIDATION SHAPE: \t(11907, 88)\n",
      "TRAINING...\n",
      "CURRENT LOSS: \t\t0.020985559698398864\n",
      "BEST LOSS: \t\t0.018638930950773056\n",
      "\n",
      "ENGINGEERING FEATURES...\n",
      "APPLYING SCALER...\n",
      "APPLYING FEATURE SELECTOR...\n",
      "223 features removed in feature selection.\n",
      "APPLYING PCA...\n",
      "NUMBER OF PRINCIPAL COMPONENTS: 295\n",
      "TRAIN SHAPE: \t\t(11907, 297)\n",
      "VALIDATION SHAPE: \t(11907, 297)\n",
      "TRAINING...\n",
      "CURRENT LOSS: \t\t0.018209048236357682\n",
      "BEST LOSS: \t\t0.018209048236357682\n",
      "\n",
      "ENGINGEERING FEATURES...\n",
      "APPLYING SCALER...\n",
      "APPLYING FEATURE SELECTOR...\n",
      "227 features removed in feature selection.\n",
      "APPLYING PCA...\n",
      "NUMBER OF PRINCIPAL COMPONENTS: 265\n",
      "TRAIN SHAPE: \t\t(11907, 267)\n",
      "VALIDATION SHAPE: \t(11907, 267)\n",
      "TRAINING...\n",
      "CURRENT LOSS: \t\t0.0192250509245517\n",
      "BEST LOSS: \t\t0.018209048236357682\n",
      "\n",
      "ENGINGEERING FEATURES...\n",
      "APPLYING SCALER...\n",
      "APPLYING FEATURE SELECTOR...\n",
      "171 features removed in feature selection.\n",
      "APPLYING PCA...\n",
      "NUMBER OF PRINCIPAL COMPONENTS: 287\n",
      "TRAIN SHAPE: \t\t(11907, 289)\n",
      "VALIDATION SHAPE: \t(11907, 289)\n",
      "TRAINING...\n",
      "CURRENT LOSS: \t\t0.01838178583976103\n",
      "BEST LOSS: \t\t0.018209048236357682\n",
      "\n",
      "ENGINGEERING FEATURES...\n",
      "APPLYING SCALER...\n",
      "APPLYING FEATURE SELECTOR...\n",
      "663 features removed in feature selection.\n",
      "APPLYING PCA...\n",
      "NUMBER OF PRINCIPAL COMPONENTS: 43\n",
      "TRAIN SHAPE: \t\t(11907, 45)\n",
      "VALIDATION SHAPE: \t(11907, 45)\n",
      "TRAINING...\n",
      "CURRENT LOSS: \t\t0.023672120824586355\n",
      "BEST LOSS: \t\t0.018209048236357682\n",
      "\n",
      "ENGINGEERING FEATURES...\n",
      "APPLYING SCALER...\n",
      "APPLYING FEATURE SELECTOR...\n",
      "662 features removed in feature selection.\n",
      "APPLYING PCA...\n",
      "NUMBER OF PRINCIPAL COMPONENTS: 253\n",
      "TRAIN SHAPE: \t\t(11907, 255)\n",
      "VALIDATION SHAPE: \t(11907, 255)\n",
      "TRAINING...\n",
      "CURRENT LOSS: \t\t0.018326155992451933\n",
      "BEST LOSS: \t\t0.018209048236357682\n",
      "\n",
      "ENGINGEERING FEATURES...\n",
      "APPLYING SCALER...\n",
      "APPLYING FEATURE SELECTOR...\n",
      "706 features removed in feature selection.\n",
      "APPLYING PCA...\n",
      "NUMBER OF PRINCIPAL COMPONENTS: 149\n",
      "TRAIN SHAPE: \t\t(11907, 151)\n",
      "VALIDATION SHAPE: \t(11907, 151)\n",
      "TRAINING...\n",
      "CURRENT LOSS: \t\t0.023259419413845954\n",
      "BEST LOSS: \t\t0.018209048236357682\n",
      "\n",
      "ENGINGEERING FEATURES...\n",
      "APPLYING SCALER...\n",
      "APPLYING FEATURE SELECTOR...\n",
      "586 features removed in feature selection.\n",
      "APPLYING PCA...\n",
      "NUMBER OF PRINCIPAL COMPONENTS: 30\n",
      "TRAIN SHAPE: \t\t(11907, 32)\n",
      "VALIDATION SHAPE: \t(11907, 32)\n",
      "TRAINING...\n",
      "CURRENT LOSS: \t\t0.0424750491433649\n",
      "BEST LOSS: \t\t0.018209048236357682\n",
      "\n",
      "ENGINGEERING FEATURES...\n",
      "APPLYING SCALER...\n",
      "APPLYING FEATURE SELECTOR...\n",
      "677 features removed in feature selection.\n",
      "APPLYING PCA...\n",
      "NUMBER OF PRINCIPAL COMPONENTS: 218\n",
      "TRAIN SHAPE: \t\t(11907, 220)\n",
      "VALIDATION SHAPE: \t(11907, 220)\n",
      "TRAINING...\n",
      "CURRENT LOSS: \t\t0.01832900263198899\n",
      "BEST LOSS: \t\t0.018209048236357682\n",
      "\n",
      "ENGINGEERING FEATURES...\n",
      "APPLYING SCALER...\n",
      "APPLYING FEATURE SELECTOR...\n",
      "168 features removed in feature selection.\n",
      "APPLYING PCA...\n",
      "NUMBER OF PRINCIPAL COMPONENTS: 286\n",
      "TRAIN SHAPE: \t\t(11907, 288)\n",
      "VALIDATION SHAPE: \t(11907, 288)\n",
      "TRAINING...\n",
      "CURRENT LOSS: \t\t0.022428045868322997\n",
      "BEST LOSS: \t\t0.018209048236357682\n",
      "\n",
      "ENGINGEERING FEATURES...\n",
      "APPLYING SCALER...\n",
      "APPLYING FEATURE SELECTOR...\n",
      "147 features removed in feature selection.\n",
      "APPLYING PCA...\n",
      "NUMBER OF PRINCIPAL COMPONENTS: 46\n",
      "TRAIN SHAPE: \t\t(11907, 48)\n",
      "VALIDATION SHAPE: \t(11907, 48)\n",
      "TRAINING...\n",
      "CURRENT LOSS: \t\t0.0911280563848876\n",
      "BEST LOSS: \t\t0.018209048236357682\n",
      "\n",
      "ENGINGEERING FEATURES...\n",
      "APPLYING SCALER...\n",
      "APPLYING FEATURE SELECTOR...\n",
      "299 features removed in feature selection.\n",
      "APPLYING PCA...\n",
      "NUMBER OF PRINCIPAL COMPONENTS: 55\n",
      "TRAIN SHAPE: \t\t(11907, 57)\n",
      "VALIDATION SHAPE: \t(11907, 57)\n",
      "TRAINING...\n",
      "CURRENT LOSS: \t\t0.02349441216837491\n",
      "BEST LOSS: \t\t0.018209048236357682\n",
      "\n",
      "ENGINGEERING FEATURES...\n",
      "APPLYING SCALER...\n",
      "APPLYING FEATURE SELECTOR...\n",
      "710 features removed in feature selection.\n",
      "APPLYING PCA...\n",
      "NUMBER OF PRINCIPAL COMPONENTS: 242\n",
      "TRAIN SHAPE: \t\t(11907, 244)\n",
      "VALIDATION SHAPE: \t(11907, 244)\n",
      "TRAINING...\n",
      "CURRENT LOSS: \t\t0.023363298398646225\n",
      "BEST LOSS: \t\t0.018209048236357682\n",
      "\n",
      "\n",
      "SEARCH COMPLETE.\n",
      "MAKING VALIDATION PREDICTIONS...\n",
      "FOLD 0 LOSS: 0.018156298\n",
      "--------------------------------------------------------------------------------------------------\n",
      "FOLD 1\n",
      "--------------------------------------------------------------------------------------------------\n",
      "RUNNING PARAMETER SEARCH...\n",
      "\n",
      "ENGINGEERING FEATURES...\n",
      "APPLYING SCALER...\n",
      "APPLYING FEATURE SELECTOR...\n",
      "517 features removed in feature selection.\n",
      "APPLYING PCA...\n",
      "NUMBER OF PRINCIPAL COMPONENTS: 200\n",
      "TRAIN SHAPE: \t\t(11907, 202)\n",
      "VALIDATION SHAPE: \t(11907, 202)\n",
      "TRAINING...\n",
      "CURRENT LOSS: \t\t0.018761615805405477\n",
      "BEST LOSS: \t\t0.018761615805405477\n",
      "\n",
      "ENGINGEERING FEATURES...\n",
      "APPLYING SCALER...\n",
      "APPLYING FEATURE SELECTOR...\n",
      "459 features removed in feature selection.\n",
      "APPLYING PCA...\n",
      "NUMBER OF PRINCIPAL COMPONENTS: 136\n",
      "TRAIN SHAPE: \t\t(11907, 138)\n",
      "VALIDATION SHAPE: \t(11907, 138)\n",
      "TRAINING...\n",
      "CURRENT LOSS: \t\t0.02611902353895323\n",
      "BEST LOSS: \t\t0.018761615805405477\n",
      "\n",
      "ENGINGEERING FEATURES...\n",
      "APPLYING SCALER...\n",
      "APPLYING FEATURE SELECTOR...\n",
      "559 features removed in feature selection.\n",
      "APPLYING PCA...\n",
      "NUMBER OF PRINCIPAL COMPONENTS: 187\n",
      "TRAIN SHAPE: \t\t(11907, 189)\n",
      "VALIDATION SHAPE: \t(11907, 189)\n",
      "TRAINING...\n",
      "CURRENT LOSS: \t\t0.021688839029001438\n",
      "BEST LOSS: \t\t0.018761615805405477\n",
      "\n",
      "ENGINGEERING FEATURES...\n",
      "APPLYING SCALER...\n",
      "APPLYING FEATURE SELECTOR...\n",
      "350 features removed in feature selection.\n",
      "APPLYING PCA...\n",
      "NUMBER OF PRINCIPAL COMPONENTS: 238\n",
      "TRAIN SHAPE: \t\t(11907, 240)\n",
      "VALIDATION SHAPE: \t(11907, 240)\n",
      "TRAINING...\n",
      "CURRENT LOSS: \t\t0.0189239003570099\n",
      "BEST LOSS: \t\t0.018761615805405477\n",
      "\n",
      "ENGINGEERING FEATURES...\n",
      "APPLYING SCALER...\n",
      "APPLYING FEATURE SELECTOR...\n",
      "657 features removed in feature selection.\n",
      "APPLYING PCA...\n",
      "NUMBER OF PRINCIPAL COMPONENTS: 71\n",
      "TRAIN SHAPE: \t\t(11907, 73)\n",
      "VALIDATION SHAPE: \t(11907, 73)\n",
      "TRAINING...\n",
      "CURRENT LOSS: \t\t0.024432018185882188\n",
      "BEST LOSS: \t\t0.018761615805405477\n",
      "\n",
      "ENGINGEERING FEATURES...\n",
      "APPLYING SCALER...\n",
      "APPLYING FEATURE SELECTOR...\n",
      "634 features removed in feature selection.\n",
      "APPLYING PCA...\n",
      "NUMBER OF PRINCIPAL COMPONENTS: 273\n",
      "TRAIN SHAPE: \t\t(11907, 275)\n",
      "VALIDATION SHAPE: \t(11907, 275)\n",
      "TRAINING...\n",
      "CURRENT LOSS: \t\t0.023434433441635424\n",
      "BEST LOSS: \t\t0.018761615805405477\n",
      "\n",
      "ENGINGEERING FEATURES...\n",
      "APPLYING SCALER...\n",
      "APPLYING FEATURE SELECTOR...\n",
      "272 features removed in feature selection.\n",
      "APPLYING PCA...\n",
      "NUMBER OF PRINCIPAL COMPONENTS: 21\n",
      "TRAIN SHAPE: \t\t(11907, 23)\n",
      "VALIDATION SHAPE: \t(11907, 23)\n",
      "TRAINING...\n",
      "CURRENT LOSS: \t\t0.022717073282585435\n",
      "BEST LOSS: \t\t0.018761615805405477\n",
      "\n",
      "ENGINGEERING FEATURES...\n",
      "APPLYING SCALER...\n",
      "APPLYING FEATURE SELECTOR...\n",
      "521 features removed in feature selection.\n",
      "APPLYING PCA...\n",
      "NUMBER OF PRINCIPAL COMPONENTS: 214\n",
      "TRAIN SHAPE: \t\t(11907, 216)\n",
      "VALIDATION SHAPE: \t(11907, 216)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING...\n",
      "CURRENT LOSS: \t\t0.021507388749464892\n",
      "BEST LOSS: \t\t0.018761615805405477\n",
      "\n",
      "ENGINGEERING FEATURES...\n",
      "APPLYING SCALER...\n",
      "APPLYING FEATURE SELECTOR...\n",
      "162 features removed in feature selection.\n",
      "APPLYING PCA...\n",
      "NUMBER OF PRINCIPAL COMPONENTS: 44\n",
      "TRAIN SHAPE: \t\t(11907, 46)\n",
      "VALIDATION SHAPE: \t(11907, 46)\n",
      "TRAINING...\n",
      "CURRENT LOSS: \t\t0.024495595275885233\n",
      "BEST LOSS: \t\t0.018761615805405477\n",
      "\n",
      "ENGINGEERING FEATURES...\n",
      "APPLYING SCALER...\n",
      "APPLYING FEATURE SELECTOR...\n",
      "606 features removed in feature selection.\n",
      "APPLYING PCA...\n",
      "NUMBER OF PRINCIPAL COMPONENTS: 77\n",
      "TRAIN SHAPE: \t\t(11907, 79)\n",
      "VALIDATION SHAPE: \t(11907, 79)\n",
      "TRAINING...\n",
      "CURRENT LOSS: \t\t0.024082443624019102\n",
      "BEST LOSS: \t\t0.018761615805405477\n",
      "\n",
      "ENGINGEERING FEATURES...\n",
      "APPLYING SCALER...\n",
      "APPLYING FEATURE SELECTOR...\n",
      "317 features removed in feature selection.\n",
      "APPLYING PCA...\n",
      "NUMBER OF PRINCIPAL COMPONENTS: 50\n",
      "TRAIN SHAPE: \t\t(11907, 52)\n",
      "VALIDATION SHAPE: \t(11907, 52)\n",
      "TRAINING...\n",
      "CURRENT LOSS: \t\t0.02220050594839807\n",
      "BEST LOSS: \t\t0.018761615805405477\n",
      "\n",
      "ENGINGEERING FEATURES...\n",
      "APPLYING SCALER...\n",
      "APPLYING FEATURE SELECTOR...\n",
      "294 features removed in feature selection.\n",
      "APPLYING PCA...\n",
      "NUMBER OF PRINCIPAL COMPONENTS: 285\n",
      "TRAIN SHAPE: \t\t(11907, 287)\n",
      "VALIDATION SHAPE: \t(11907, 287)\n",
      "TRAINING...\n",
      "CURRENT LOSS: \t\t0.019429430141413087\n",
      "BEST LOSS: \t\t0.018761615805405477\n",
      "\n",
      "ENGINGEERING FEATURES...\n",
      "APPLYING SCALER...\n",
      "APPLYING FEATURE SELECTOR...\n",
      "124 features removed in feature selection.\n",
      "APPLYING PCA...\n",
      "NUMBER OF PRINCIPAL COMPONENTS: 247\n",
      "TRAIN SHAPE: \t\t(11907, 249)\n",
      "VALIDATION SHAPE: \t(11907, 249)\n",
      "TRAINING...\n",
      "CURRENT LOSS: \t\t0.023370872108721876\n",
      "BEST LOSS: \t\t0.018761615805405477\n",
      "\n",
      "ENGINGEERING FEATURES...\n",
      "APPLYING SCALER...\n",
      "APPLYING FEATURE SELECTOR...\n",
      "118 features removed in feature selection.\n",
      "APPLYING PCA...\n",
      "NUMBER OF PRINCIPAL COMPONENTS: 254\n",
      "TRAIN SHAPE: \t\t(11907, 256)\n",
      "VALIDATION SHAPE: \t(11907, 256)\n",
      "TRAINING...\n",
      "CURRENT LOSS: \t\t0.024754750882398976\n",
      "BEST LOSS: \t\t0.018761615805405477\n",
      "\n",
      "ENGINGEERING FEATURES...\n",
      "APPLYING SCALER...\n",
      "APPLYING FEATURE SELECTOR...\n",
      "706 features removed in feature selection.\n",
      "APPLYING PCA...\n",
      "NUMBER OF PRINCIPAL COMPONENTS: 300\n",
      "TRAIN SHAPE: \t\t(11907, 302)\n",
      "VALIDATION SHAPE: \t(11907, 302)\n",
      "TRAINING...\n",
      "CURRENT LOSS: \t\t0.02393454377973364\n",
      "BEST LOSS: \t\t0.018761615805405477\n",
      "\n",
      "ENGINGEERING FEATURES...\n",
      "APPLYING SCALER...\n",
      "APPLYING FEATURE SELECTOR...\n",
      "192 features removed in feature selection.\n",
      "APPLYING PCA...\n",
      "NUMBER OF PRINCIPAL COMPONENTS: 292\n",
      "TRAIN SHAPE: \t\t(11907, 294)\n",
      "VALIDATION SHAPE: \t(11907, 294)\n",
      "TRAINING...\n",
      "CURRENT LOSS: \t\t0.023371251636592382\n",
      "BEST LOSS: \t\t0.018761615805405477\n",
      "\n",
      "ENGINGEERING FEATURES...\n",
      "APPLYING SCALER...\n",
      "APPLYING FEATURE SELECTOR...\n",
      "268 features removed in feature selection.\n",
      "APPLYING PCA...\n",
      "NUMBER OF PRINCIPAL COMPONENTS: 295\n",
      "TRAIN SHAPE: \t\t(11907, 297)\n",
      "VALIDATION SHAPE: \t(11907, 297)\n",
      "TRAINING...\n",
      "CURRENT LOSS: \t\t0.027350741842443165\n",
      "BEST LOSS: \t\t0.018761615805405477\n",
      "\n",
      "ENGINGEERING FEATURES...\n",
      "APPLYING SCALER...\n",
      "APPLYING FEATURE SELECTOR...\n",
      "681 features removed in feature selection.\n",
      "APPLYING PCA...\n",
      "NUMBER OF PRINCIPAL COMPONENTS: 35\n",
      "TRAIN SHAPE: \t\t(11907, 37)\n",
      "VALIDATION SHAPE: \t(11907, 37)\n",
      "TRAINING...\n",
      "CURRENT LOSS: \t\t0.026189181086008817\n",
      "BEST LOSS: \t\t0.018761615805405477\n",
      "\n",
      "ENGINGEERING FEATURES...\n",
      "APPLYING SCALER...\n",
      "APPLYING FEATURE SELECTOR...\n",
      "142 features removed in feature selection.\n",
      "APPLYING PCA...\n",
      "NUMBER OF PRINCIPAL COMPONENTS: 83\n",
      "TRAIN SHAPE: \t\t(11907, 85)\n",
      "VALIDATION SHAPE: \t(11907, 85)\n",
      "TRAINING...\n",
      "CURRENT LOSS: \t\t0.019712028248491487\n",
      "BEST LOSS: \t\t0.018761615805405477\n",
      "\n",
      "ENGINGEERING FEATURES...\n",
      "APPLYING SCALER...\n",
      "APPLYING FEATURE SELECTOR...\n",
      "715 features removed in feature selection.\n",
      "APPLYING PCA...\n",
      "NUMBER OF PRINCIPAL COMPONENTS: 21\n",
      "TRAIN SHAPE: \t\t(11907, 23)\n",
      "VALIDATION SHAPE: \t(11907, 23)\n",
      "TRAINING...\n",
      "CURRENT LOSS: \t\t0.022429261894675763\n",
      "BEST LOSS: \t\t0.018761615805405477\n",
      "\n",
      "ENGINGEERING FEATURES...\n",
      "APPLYING SCALER...\n",
      "APPLYING FEATURE SELECTOR...\n",
      "697 features removed in feature selection.\n",
      "APPLYING PCA...\n",
      "NUMBER OF PRINCIPAL COMPONENTS: 34\n",
      "TRAIN SHAPE: \t\t(11907, 36)\n",
      "VALIDATION SHAPE: \t(11907, 36)\n",
      "TRAINING...\n",
      "CURRENT LOSS: \t\t0.024449090818102762\n",
      "BEST LOSS: \t\t0.018761615805405477\n",
      "\n",
      "ENGINGEERING FEATURES...\n",
      "APPLYING SCALER...\n",
      "APPLYING FEATURE SELECTOR...\n",
      "236 features removed in feature selection.\n",
      "APPLYING PCA...\n",
      "NUMBER OF PRINCIPAL COMPONENTS: 72\n",
      "TRAIN SHAPE: \t\t(11907, 74)\n",
      "VALIDATION SHAPE: \t(11907, 74)\n",
      "TRAINING...\n",
      "CURRENT LOSS: \t\t0.02350200664753837\n",
      "BEST LOSS: \t\t0.018761615805405477\n",
      "\n",
      "ENGINGEERING FEATURES...\n",
      "APPLYING SCALER...\n",
      "APPLYING FEATURE SELECTOR...\n",
      "167 features removed in feature selection.\n",
      "APPLYING PCA...\n",
      "NUMBER OF PRINCIPAL COMPONENTS: 69\n",
      "TRAIN SHAPE: \t\t(11907, 71)\n",
      "VALIDATION SHAPE: \t(11907, 71)\n",
      "TRAINING...\n",
      "CURRENT LOSS: \t\t0.02337137243948529\n",
      "BEST LOSS: \t\t0.018761615805405477\n",
      "\n",
      "ENGINGEERING FEATURES...\n",
      "APPLYING SCALER...\n",
      "APPLYING FEATURE SELECTOR...\n",
      "713 features removed in feature selection.\n",
      "APPLYING PCA...\n",
      "NUMBER OF PRINCIPAL COMPONENTS: 296\n",
      "TRAIN SHAPE: \t\t(11907, 298)\n",
      "VALIDATION SHAPE: \t(11907, 298)\n",
      "TRAINING...\n",
      "CURRENT LOSS: \t\t0.023796117525118527\n",
      "BEST LOSS: \t\t0.018761615805405477\n",
      "\n",
      "ENGINGEERING FEATURES...\n",
      "APPLYING SCALER...\n",
      "APPLYING FEATURE SELECTOR...\n",
      "217 features removed in feature selection.\n",
      "APPLYING PCA...\n",
      "NUMBER OF PRINCIPAL COMPONENTS: 277\n",
      "TRAIN SHAPE: \t\t(11907, 279)\n",
      "VALIDATION SHAPE: \t(11907, 279)\n",
      "TRAINING...\n",
      "CURRENT LOSS: \t\t0.01936207125811197\n",
      "BEST LOSS: \t\t0.018761615805405477\n",
      "\n",
      "ENGINGEERING FEATURES...\n",
      "APPLYING SCALER...\n",
      "APPLYING FEATURE SELECTOR...\n",
      "228 features removed in feature selection.\n",
      "APPLYING PCA...\n",
      "NUMBER OF PRINCIPAL COMPONENTS: 256\n",
      "TRAIN SHAPE: \t\t(11907, 258)\n",
      "VALIDATION SHAPE: \t(11907, 258)\n",
      "TRAINING...\n",
      "CURRENT LOSS: \t\t0.023445561553858695\n",
      "BEST LOSS: \t\t0.018761615805405477\n",
      "\n",
      "ENGINGEERING FEATURES...\n",
      "APPLYING SCALER...\n",
      "APPLYING FEATURE SELECTOR...\n",
      "529 features removed in feature selection.\n",
      "APPLYING PCA...\n",
      "NUMBER OF PRINCIPAL COMPONENTS: 34\n",
      "TRAIN SHAPE: \t\t(11907, 36)\n",
      "VALIDATION SHAPE: \t(11907, 36)\n",
      "TRAINING...\n",
      "CURRENT LOSS: \t\t0.018613407980221448\n",
      "BEST LOSS: \t\t0.018613407980221448\n",
      "\n",
      "ENGINGEERING FEATURES...\n",
      "APPLYING SCALER...\n",
      "APPLYING FEATURE SELECTOR...\n",
      "237 features removed in feature selection.\n",
      "APPLYING PCA...\n",
      "NUMBER OF PRINCIPAL COMPONENTS: 36\n",
      "TRAIN SHAPE: \t\t(11907, 38)\n",
      "VALIDATION SHAPE: \t(11907, 38)\n",
      "TRAINING...\n",
      "CURRENT LOSS: \t\t0.024079037480479244\n",
      "BEST LOSS: \t\t0.018613407980221448\n",
      "\n",
      "ENGINGEERING FEATURES...\n",
      "APPLYING SCALER...\n",
      "APPLYING FEATURE SELECTOR...\n",
      "636 features removed in feature selection.\n",
      "APPLYING PCA...\n",
      "NUMBER OF PRINCIPAL COMPONENTS: 48\n",
      "TRAIN SHAPE: \t\t(11907, 50)\n",
      "VALIDATION SHAPE: \t(11907, 50)\n",
      "TRAINING...\n",
      "CURRENT LOSS: \t\t0.023371300912535066\n",
      "BEST LOSS: \t\t0.018613407980221448\n",
      "\n",
      "ENGINGEERING FEATURES...\n",
      "APPLYING SCALER...\n",
      "APPLYING FEATURE SELECTOR...\n",
      "646 features removed in feature selection.\n",
      "APPLYING PCA...\n",
      "NUMBER OF PRINCIPAL COMPONENTS: 239\n",
      "TRAIN SHAPE: \t\t(11907, 241)\n",
      "VALIDATION SHAPE: \t(11907, 241)\n",
      "TRAINING...\n",
      "CURRENT LOSS: \t\t0.02329667348528533\n",
      "BEST LOSS: \t\t0.018613407980221448\n",
      "\n",
      "ENGINGEERING FEATURES...\n",
      "APPLYING SCALER...\n",
      "APPLYING FEATURE SELECTOR...\n",
      "207 features removed in feature selection.\n",
      "APPLYING PCA...\n",
      "NUMBER OF PRINCIPAL COMPONENTS: 239\n",
      "TRAIN SHAPE: \t\t(11907, 241)\n",
      "VALIDATION SHAPE: \t(11907, 241)\n",
      "TRAINING...\n",
      "CURRENT LOSS: \t\t0.020335668153925693\n",
      "BEST LOSS: \t\t0.018613407980221448\n",
      "\n",
      "ENGINGEERING FEATURES...\n",
      "APPLYING SCALER...\n",
      "APPLYING FEATURE SELECTOR...\n",
      "704 features removed in feature selection.\n",
      "APPLYING PCA...\n",
      "NUMBER OF PRINCIPAL COMPONENTS: 21\n",
      "TRAIN SHAPE: \t\t(11907, 23)\n",
      "VALIDATION SHAPE: \t(11907, 23)\n",
      "TRAINING...\n",
      "CURRENT LOSS: \t\t0.02388533858127509\n",
      "BEST LOSS: \t\t0.018613407980221448\n",
      "\n",
      "ENGINGEERING FEATURES...\n",
      "APPLYING SCALER...\n",
      "APPLYING FEATURE SELECTOR...\n",
      "658 features removed in feature selection.\n",
      "APPLYING PCA...\n",
      "NUMBER OF PRINCIPAL COMPONENTS: 287\n",
      "TRAIN SHAPE: \t\t(11907, 289)\n",
      "VALIDATION SHAPE: \t(11907, 289)\n",
      "TRAINING...\n",
      "CURRENT LOSS: \t\t0.02362831707995011\n",
      "BEST LOSS: \t\t0.018613407980221448\n",
      "\n",
      "ENGINGEERING FEATURES...\n",
      "APPLYING SCALER...\n",
      "APPLYING FEATURE SELECTOR...\n",
      "117 features removed in feature selection.\n",
      "APPLYING PCA...\n",
      "NUMBER OF PRINCIPAL COMPONENTS: 71\n",
      "TRAIN SHAPE: \t\t(11907, 73)\n",
      "VALIDATION SHAPE: \t(11907, 73)\n",
      "TRAINING...\n",
      "CURRENT LOSS: \t\t0.019771261437745064\n",
      "BEST LOSS: \t\t0.018613407980221448\n",
      "\n",
      "ENGINGEERING FEATURES...\n",
      "APPLYING SCALER...\n",
      "APPLYING FEATURE SELECTOR...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "682 features removed in feature selection.\n",
      "APPLYING PCA...\n",
      "NUMBER OF PRINCIPAL COMPONENTS: 38\n",
      "TRAIN SHAPE: \t\t(11907, 40)\n",
      "VALIDATION SHAPE: \t(11907, 40)\n",
      "TRAINING...\n",
      "CURRENT LOSS: \t\t0.02385471220640356\n",
      "BEST LOSS: \t\t0.018613407980221448\n",
      "\n",
      "ENGINGEERING FEATURES...\n",
      "APPLYING SCALER...\n",
      "APPLYING FEATURE SELECTOR...\n",
      "244 features removed in feature selection.\n",
      "APPLYING PCA...\n",
      "NUMBER OF PRINCIPAL COMPONENTS: 24\n",
      "TRAIN SHAPE: \t\t(11907, 26)\n",
      "VALIDATION SHAPE: \t(11907, 26)\n",
      "TRAINING...\n",
      "CURRENT LOSS: \t\t0.019332706026024653\n",
      "BEST LOSS: \t\t0.018613407980221448\n",
      "\n",
      "ENGINGEERING FEATURES...\n",
      "APPLYING SCALER...\n",
      "APPLYING FEATURE SELECTOR...\n",
      "163 features removed in feature selection.\n",
      "APPLYING PCA...\n",
      "NUMBER OF PRINCIPAL COMPONENTS: 287\n",
      "TRAIN SHAPE: \t\t(11907, 289)\n",
      "VALIDATION SHAPE: \t(11907, 289)\n",
      "TRAINING...\n",
      "CURRENT LOSS: \t\t0.02321679255373583\n",
      "BEST LOSS: \t\t0.018613407980221448\n",
      "\n",
      "ENGINGEERING FEATURES...\n",
      "APPLYING SCALER...\n",
      "APPLYING FEATURE SELECTOR...\n",
      "118 features removed in feature selection.\n",
      "APPLYING PCA...\n",
      "NUMBER OF PRINCIPAL COMPONENTS: 30\n",
      "TRAIN SHAPE: \t\t(11907, 32)\n",
      "VALIDATION SHAPE: \t(11907, 32)\n",
      "TRAINING...\n",
      "CURRENT LOSS: \t\t0.05559274799581198\n",
      "BEST LOSS: \t\t0.018613407980221448\n",
      "\n",
      "ENGINGEERING FEATURES...\n",
      "APPLYING SCALER...\n",
      "APPLYING FEATURE SELECTOR...\n",
      "700 features removed in feature selection.\n",
      "APPLYING PCA...\n",
      "NUMBER OF PRINCIPAL COMPONENTS: 293\n",
      "TRAIN SHAPE: \t\t(11907, 295)\n",
      "VALIDATION SHAPE: \t(11907, 295)\n",
      "TRAINING...\n",
      "CURRENT LOSS: \t\t0.022343440478912256\n",
      "BEST LOSS: \t\t0.018613407980221448\n",
      "\n",
      "ENGINGEERING FEATURES...\n",
      "APPLYING SCALER...\n",
      "APPLYING FEATURE SELECTOR...\n",
      "685 features removed in feature selection.\n",
      "APPLYING PCA...\n",
      "NUMBER OF PRINCIPAL COMPONENTS: 75\n",
      "TRAIN SHAPE: \t\t(11907, 77)\n",
      "VALIDATION SHAPE: \t(11907, 77)\n",
      "TRAINING...\n",
      "CURRENT LOSS: \t\t0.01897623971723105\n",
      "BEST LOSS: \t\t0.018613407980221448\n",
      "\n",
      "ENGINGEERING FEATURES...\n",
      "APPLYING SCALER...\n",
      "APPLYING FEATURE SELECTOR...\n",
      "636 features removed in feature selection.\n",
      "APPLYING PCA...\n",
      "NUMBER OF PRINCIPAL COMPONENTS: 247\n",
      "TRAIN SHAPE: \t\t(11907, 249)\n",
      "VALIDATION SHAPE: \t(11907, 249)\n",
      "TRAINING...\n",
      "CURRENT LOSS: \t\t0.020693415393292662\n",
      "BEST LOSS: \t\t0.018613407980221448\n",
      "\n",
      "ENGINGEERING FEATURES...\n",
      "APPLYING SCALER...\n",
      "APPLYING FEATURE SELECTOR...\n",
      "669 features removed in feature selection.\n",
      "APPLYING PCA...\n",
      "NUMBER OF PRINCIPAL COMPONENTS: 161\n",
      "TRAIN SHAPE: \t\t(11907, 163)\n",
      "VALIDATION SHAPE: \t(11907, 163)\n",
      "TRAINING...\n",
      "CURRENT LOSS: \t\t0.023128522239652984\n",
      "BEST LOSS: \t\t0.018613407980221448\n",
      "\n",
      "ENGINGEERING FEATURES...\n",
      "APPLYING SCALER...\n",
      "APPLYING FEATURE SELECTOR...\n",
      "664 features removed in feature selection.\n",
      "APPLYING PCA...\n",
      "NUMBER OF PRINCIPAL COMPONENTS: 242\n",
      "TRAIN SHAPE: \t\t(11907, 244)\n",
      "VALIDATION SHAPE: \t(11907, 244)\n",
      "TRAINING...\n",
      "CURRENT LOSS: \t\t0.023630034901154173\n",
      "BEST LOSS: \t\t0.018613407980221448\n",
      "\n",
      "ENGINGEERING FEATURES...\n",
      "APPLYING SCALER...\n",
      "APPLYING FEATURE SELECTOR...\n",
      "706 features removed in feature selection.\n",
      "APPLYING PCA...\n",
      "NUMBER OF PRINCIPAL COMPONENTS: 149\n",
      "TRAIN SHAPE: \t\t(11907, 151)\n",
      "VALIDATION SHAPE: \t(11907, 151)\n",
      "TRAINING...\n",
      "CURRENT LOSS: \t\t0.023618015477250478\n",
      "BEST LOSS: \t\t0.018613407980221448\n",
      "\n",
      "ENGINGEERING FEATURES...\n",
      "APPLYING SCALER...\n",
      "APPLYING FEATURE SELECTOR...\n",
      "324 features removed in feature selection.\n",
      "APPLYING PCA...\n",
      "NUMBER OF PRINCIPAL COMPONENTS: 291\n",
      "TRAIN SHAPE: \t\t(11907, 293)\n",
      "VALIDATION SHAPE: \t(11907, 293)\n",
      "TRAINING...\n",
      "CURRENT LOSS: \t\t0.02401818835786877\n",
      "BEST LOSS: \t\t0.018613407980221448\n",
      "\n",
      "ENGINGEERING FEATURES...\n",
      "APPLYING SCALER...\n",
      "APPLYING FEATURE SELECTOR...\n",
      "667 features removed in feature selection.\n",
      "APPLYING PCA...\n",
      "NUMBER OF PRINCIPAL COMPONENTS: 274\n",
      "TRAIN SHAPE: \t\t(11907, 276)\n",
      "VALIDATION SHAPE: \t(11907, 276)\n",
      "TRAINING...\n",
      "CURRENT LOSS: \t\t0.01927736983370329\n",
      "BEST LOSS: \t\t0.018613407980221448\n",
      "\n",
      "ENGINGEERING FEATURES...\n",
      "APPLYING SCALER...\n",
      "APPLYING FEATURE SELECTOR...\n",
      "168 features removed in feature selection.\n",
      "APPLYING PCA...\n",
      "NUMBER OF PRINCIPAL COMPONENTS: 286\n",
      "TRAIN SHAPE: \t\t(11907, 288)\n",
      "VALIDATION SHAPE: \t(11907, 288)\n",
      "TRAINING...\n",
      "CURRENT LOSS: \t\t0.02287416289278003\n",
      "BEST LOSS: \t\t0.018613407980221448\n",
      "\n",
      "ENGINGEERING FEATURES...\n",
      "APPLYING SCALER...\n",
      "APPLYING FEATURE SELECTOR...\n",
      "163 features removed in feature selection.\n",
      "APPLYING PCA...\n",
      "NUMBER OF PRINCIPAL COMPONENTS: 283\n",
      "TRAIN SHAPE: \t\t(11907, 285)\n",
      "VALIDATION SHAPE: \t(11907, 285)\n",
      "TRAINING...\n",
      "CURRENT LOSS: \t\t0.02337065913774221\n",
      "BEST LOSS: \t\t0.018613407980221448\n",
      "\n",
      "ENGINGEERING FEATURES...\n",
      "APPLYING SCALER...\n",
      "APPLYING FEATURE SELECTOR...\n",
      "154 features removed in feature selection.\n",
      "APPLYING PCA...\n",
      "NUMBER OF PRINCIPAL COMPONENTS: 236\n",
      "TRAIN SHAPE: \t\t(11907, 238)\n",
      "VALIDATION SHAPE: \t(11907, 238)\n",
      "TRAINING...\n",
      "CURRENT LOSS: \t\t0.023371119897463478\n",
      "BEST LOSS: \t\t0.018613407980221448\n",
      "\n",
      "ENGINGEERING FEATURES...\n",
      "APPLYING SCALER...\n",
      "APPLYING FEATURE SELECTOR...\n",
      "154 features removed in feature selection.\n",
      "APPLYING PCA...\n",
      "NUMBER OF PRINCIPAL COMPONENTS: 28\n",
      "TRAIN SHAPE: \t\t(11907, 30)\n",
      "VALIDATION SHAPE: \t(11907, 30)\n",
      "TRAINING...\n",
      "CURRENT LOSS: \t\t0.02407904115884365\n",
      "BEST LOSS: \t\t0.018613407980221448\n",
      "\n",
      "\n",
      "SEARCH COMPLETE.\n",
      "MAKING VALIDATION PREDICTIONS...\n",
      "FOLD 1 LOSS: 0.01867706\n",
      "--------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Define CV strategy\n",
    "kf = KFold(n_splits=KFOLDS, random_state=SEED)\n",
    "loss_scores = []\n",
    "best_params = pd.DataFrame(columns=['kfold','selected_features','num_features',\n",
    "                                    'num_components','use_embedding','seed'])\n",
    "\n",
    "for fold, (tdx, vdx) in enumerate(kf.split(X, y)):\n",
    "    print(f'FOLD {fold}')\n",
    "    print('--------------------------------------------------------------------------------------------------')\n",
    "    # Create name to save model by\n",
    "    model_save_name = 'models/' + model_name_save + '/' + model_name_save + '_' + str(fold) + '.h5'\n",
    "    model_save_name_temp = 'models/' + model_name_save + '/' + 'TEMP_'+ model_name_save+ '_' + str(fold) + '.h5'    \n",
    "    \n",
    "    @use_named_args(dimensions=dimensions)\n",
    "    def get_hyperopts(num_features,\n",
    "                      num_components,\n",
    "                      learning_rate, \n",
    "                      num_dense_layers, \n",
    "                      num_input_nodes, \n",
    "                      num_dense_nodes,\n",
    "                      activation, \n",
    "                      batch_size,\n",
    "                      patience,\n",
    "                      optimiser,\n",
    "                      optimiser_decay,\n",
    "                      dropout_layer,\n",
    "                      dropout_val,\n",
    "                      use_embedding):\n",
    "\n",
    "        # Define key parameters - these are affected by parameter search so must be done inside function\n",
    "        NUM_FEATURES     = num_features\n",
    "        NUM_COMPONENTS   = num_components\n",
    "        PCA_METHOD       = PCA(n_components=NUM_COMPONENTS, random_state=SEED)\n",
    "        BATCH_SIZE       = batch_size\n",
    "        PATIENCE         = patience\n",
    "        USE_EMBEDDING    = use_embedding\n",
    "\n",
    "        \n",
    "        # Fetch in-fold data\n",
    "        X_tdx, X_vdx, y_tdx, y_vdx = X.iloc[tdx, :], X.iloc[vdx, :], y.iloc[tdx, :], y.iloc[vdx, :]\n",
    "\n",
    "        # Transform data\n",
    "        X_tdx, X_vdx, num_cols, cat_cols, selected_features = transform_feature_set(X_tdx, X_vdx, y_tdx, y_vdx, \n",
    "                                                                                    verbose=1,\n",
    "                                                                                    num_features=NUM_FEATURES,\n",
    "                                                                                    pca=PCA_METHOD) \n",
    "\n",
    "        # Define activation layers\n",
    "        if activation == 'relu':\n",
    "            ACTIVATION = ReLU()\n",
    "        elif activation == 'leaky_relu':\n",
    "            ACTIVATION = LeakyReLU()\n",
    "        elif activation == 'elu':\n",
    "            ACTIVATION = ELU()\n",
    "        elif activation == 'threshold_relu':\n",
    "            ACTIVATION = ThresholdedReLU()\n",
    "\n",
    "        # Define regularisation layers\n",
    "        if dropout_layer == 'dropout':\n",
    "            REG_LAYER = Dropout(dropout_val)\n",
    "        elif dropout_layer == 'gaussian_dropout':\n",
    "            REG_LAYER = GaussianDropout(dropout_val)\n",
    "        elif dropout_layer == 'alpha_dropout':\n",
    "            REG_LAYER = AlphaDropout(dropout_val)\n",
    "\n",
    "        # Define optimisers #\n",
    "        if optimiser == 'sgd':\n",
    "            OPTIMISER = SGD(lr=learning_rate, decay=optimiser_decay)\n",
    "        elif optimiser == 'adam':\n",
    "            OPTIMISER = RMSprop(lr=learning_rate, decay=optimiser_decay)\n",
    "        elif optimiser == 'rms_prop':\n",
    "            OPTIMISER = Adam(lr=learning_rate, decay=optimiser_decay)\n",
    "        elif optimiser == 'ada_delta':\n",
    "            OPTIMISER = Adadelta(lr=learning_rate, decay=optimiser_decay)\n",
    "        elif optimiser == 'ada_grad':\n",
    "            OPTIMISER = Adagrad(lr=learning_rate, decay=optimiser_decay)\n",
    "        elif optimiser == 'ada_max':\n",
    "            OPTIMISER = Adamax(lr=learning_rate, decay=optimiser_decay)\n",
    "        elif optimiser == 'n_adam':\n",
    "            OPTIMISER = Nadam(lr=learning_rate, decay=optimiser_decay)\n",
    "        elif optimiser == 'ftrl':\n",
    "            OPTIMISER = Ftrl(lr=learning_rate, decay=optimiser_decay)\n",
    "\n",
    "        ## BUILD MODEL BASED ON INPUTTED BAYESIAN HYPERPARAMETERS ##\n",
    "        # Input layer #\n",
    "        if USE_EMBEDDING == 1:\n",
    "            inputs = []\n",
    "            embeddings = []\n",
    "            for col in cat_cols:\n",
    "                # Create categorical embedding for each categorical feature\n",
    "                input_ = Input(shape=(1,))\n",
    "                input_dim = int(X_tdx[col].max() + 1)\n",
    "                embedding = Embedding(input_dim=input_dim, output_dim=10, input_length=1)(input_)\n",
    "                embedding = Reshape(target_shape=(10,))(embedding)\n",
    "                inputs.append(input_)\n",
    "                embeddings.append(embedding)\n",
    "            input_numeric = Input(shape=(len(num_cols),))\n",
    "            embedding_numeric = Dense(num_input_nodes)(input_numeric) \n",
    "            embedding_numeric = ACTIVATION(embedding_numeric) \n",
    "            inputs.append(input_numeric)\n",
    "            embeddings.append(embedding_numeric)\n",
    "            x = Concatenate()(embeddings)\n",
    "        if USE_EMBEDDING == 0:\n",
    "            input_ = Input(shape=(X_tdx.shape[1], ))\n",
    "            x = Dense(num_input_nodes)(input_)\n",
    "        # Hidden layers #\n",
    "        for i in range(num_dense_layers):\n",
    "            layer_name = f'layer_dense_{i+1}'\n",
    "            x = Dense(num_dense_nodes, name=layer_name)(x)\n",
    "            x = ACTIVATION(x)\n",
    "            x = BatchNormalization()(x)\n",
    "            x = REG_LAYER(x) \n",
    "        # Output layer #\n",
    "        output = Dense(y.shape[1], activation='softmax')(x)\n",
    "\n",
    "        if USE_EMBEDDING == 1:\n",
    "            model = Model(inputs, output)\n",
    "        elif USE_EMBEDDING == 0:\n",
    "            model = Model(input_, output)\n",
    "\n",
    "        \n",
    "        # COMPILE MODEL #\n",
    "        model.compile(optimizer=OPTIMISER, \n",
    "                      loss='binary_crossentropy')\n",
    "\n",
    "        # Define learning rate schedule\n",
    "        lr = LearningRateScheduler(lrfn, verbose=0)\n",
    "        \n",
    "        # Define early stopping parameters\n",
    "        es = EarlyStopping(monitor='val_loss', \n",
    "                           mode='min',\n",
    "                           restore_best_weights=True, \n",
    "                           verbose=0, \n",
    "                           patience=PATIENCE)\n",
    "        \n",
    "        # Define model checkpoint parameters\n",
    "        mc = ModelCheckpoint(filepath=model_save_name_temp, \n",
    "                             save_best_only=True, \n",
    "                             save_weights_only=False,\n",
    "                             monitor='val_loss', \n",
    "                             mode='min',\n",
    "                             verbose=0)\n",
    "\n",
    "        if USE_EMBEDDING == 1:\n",
    "            # Separate data to fit into embedding and numerical input layers\n",
    "            X_tdx = [np.absolute(X_tdx[i]) for i in cat_cols] + [X_tdx[num_cols]]\n",
    "            X_vdx = [np.absolute(X_vdx[i]) for i in cat_cols] + [X_vdx[num_cols]]\n",
    "\n",
    "        # FIT MODEL #\n",
    "        print('TRAINING...')\n",
    "        history = model.fit(X_tdx, y_tdx,\n",
    "                            epochs=EPOCHS,\n",
    "                            batch_size=BATCH_SIZE,\n",
    "                            callbacks = [es, lr, mc],\n",
    "                            verbose=0,\n",
    "                            validation_split=0.25\n",
    "                           )\n",
    "        \n",
    "        # Get val_loss for the best model (one saved with ModelCheckpoint)\n",
    "        loss = min(history.history['val_loss'])\n",
    "        print(f'CURRENT LOSS: \\t\\t{loss}')\n",
    "        \n",
    "        # Save best loss and parameters to global memory\n",
    "        global best_loss\n",
    "        global best_params\n",
    "\n",
    "        # If the classification loss of the saved model is improved\n",
    "        if loss < best_loss:\n",
    "            model.save(model_save_name)\n",
    "            best_loss = loss\n",
    "            \n",
    "            # Save transformed validation arrays (so they can be used for prediction)\n",
    "            global X_vdx_best_model, y_vdx_best_model\n",
    "            X_vdx_best_model, y_vdx_best_model = X_vdx, y_vdx\n",
    "            \n",
    "            ### SAVE MODEL PARAMETERS ### \n",
    "            best_params = best_params.loc[best_params.kfold != fold]\n",
    "            best_params = best_params.append({'kfold'            : fold,\n",
    "                                              'selected_features': selected_features,\n",
    "                                              'num_features'     : NUM_FEATURES,\n",
    "                                              'num_components'   : NUM_COMPONENTS,\n",
    "                                              'use_embedding'    : USE_EMBEDDING,\n",
    "                                              'seed'             : SEED}, \n",
    "                                             ignore_index=True)\n",
    "            best_params.to_csv('final_classifier_parameters/' + model_name_save + '.csv', index=False)\n",
    "            \n",
    "        print(f'BEST LOSS: \\t\\t{best_loss}\\n')\n",
    "\n",
    "        del model\n",
    "        k.clear_session()\n",
    "        return(loss)\n",
    "    \n",
    "    ## RUN BAYESIAN HYPERPARAMETER SEARCH ##\n",
    "    print('RUNNING PARAMETER SEARCH...\\n')\n",
    "    time.sleep(2)\n",
    "    best_loss = np.Inf\n",
    "    search_iteration = 1\n",
    "    \n",
    "    gp_result = gp_minimize(func         = get_hyperopts,\n",
    "                            dimensions   = dimensions,\n",
    "                            acq_func     = 'EI', # Expected Improvement.\n",
    "                            n_calls      = 50,\n",
    "                            noise        = 0.01,\n",
    "                            n_jobs       = -1,\n",
    "                            kappa        = 5,\n",
    "                            x0           = default_parameters,\n",
    "                            random_state = SEED\n",
    "                           )\n",
    "    \n",
    "    \n",
    "    print('\\nSEARCH COMPLETE.')\n",
    "    print('MAKING VALIDATION PREDICTIONS...')\n",
    "    \n",
    "    # Load best model\n",
    "    model = load_model(model_save_name)\n",
    "    # Make validation predictions\n",
    "    preds = model.predict(X_vdx_best_model)\n",
    "    \n",
    "    # Calculate OOF loss \n",
    "    oof_loss = metric(np.array(y_vdx_best_model), np.array(preds))\n",
    "\n",
    "    print('FOLD ' + str(fold) + ' LOSS: ' + str(oof_loss))\n",
    "    print('--------------------------------------------------------------------------------------------------')\n",
    "    time.sleep(2)\n",
    "    loss_scores.append(oof_loss)\n",
    "\n",
    "    # Clean up\n",
    "    gc.collect()\n",
    "    os.remove(model_save_name_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------\n",
      "FOLD SCORES\n",
      "--------------\n",
      "0    0.018156\n",
      "1    0.018677\n",
      "dtype: float64\n",
      "\n",
      "--------------\n",
      "FOLD STATS\n",
      "--------------\n",
      "count    2.000000\n",
      "mean     0.018417\n",
      "std      0.000368\n",
      "min      0.018156\n",
      "25%      0.018286\n",
      "50%      0.018417\n",
      "75%      0.018547\n",
      "max      0.018677\n",
      "dtype: float64\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(f'--------------\\nFOLD SCORES\\n--------------\\n{pd.Series(loss_scores)}')\n",
    "print(f'\\n--------------\\nFOLD STATS\\n--------------\\n{pd.Series(loss_scores).describe()}')\n",
    "\n",
    "plt.plot(pd.Series(loss_scores).index, pd.Series(loss_scores), marker='.')\n",
    "plt.title('Loss Fold Results')\n",
    "plt.xlabel('Fold')\n",
    "plt.xticks(np.arange(0, KFOLDS, step=1))\n",
    "plt.ylabel('Loss')\n",
    "plt.ylim([0.015, 0.02])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "## SAVE SCORES ##\n",
    "# Convert to dataframe\n",
    "loss_scores_df = pd.DataFrame(loss_scores).reset_index().rename(columns={'index':'fold', 0:'loss'})\n",
    "# Write to csv\n",
    "loss_scores_df.to_csv('scores/' + model_name_save + '.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
