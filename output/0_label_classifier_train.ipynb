{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mechanisms of Action (MoA) Prediction - 0 Label Classifier\n",
    "## Train\n",
    "\n",
    "In this notebook, instead of training a model to predict the outcomes of the competition, we'll train a classifier to predict records that have no labels. If effective, this kind of model could be used as part of multiple solutions. \n",
    "\n",
    "For example, it could be used as a meta-classifier, where we create a new feature that we feed into the next model (either as a stacked solution, or a binary feature alongside the original data). Also, it could be used as part of the model ensembling stage, where we can invert the prediction values (so a zero-label record is denoted with a 0) and apply any model weighting where necessary. This would bring probabilities down for each record at a level depending on the model weighting. Furthermore, we could even replace the prediction of any record predicted to have no label with a 0 for each label. These possible solutions all increase in size of effect it would have on the final predictions, and it will require some evaluation to decide where to set threshold of how much importance we will give these models.\n",
    "\n",
    "We begin by defining a baseline model, with attributing data transformation functions (so all transformations can be done in-fold, avoiding any data leakage occurring). We then dispose of this baseline model, but keep the basic format, when conducting Bayesian Hyperparameter search to find the best parameters. This step will change the architecture of the model at each fold, so will potentially result in differing model architectures for all 10 folds.\n",
    "\n",
    "\n",
    "## 1.00 Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import gc\n",
    "import random\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Data vis packages\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Data prep\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Modelling packages\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.python.keras import backend as k\n",
    "# Key layers\n",
    "from tensorflow.keras.models import Model, Sequential, load_model\n",
    "from tensorflow.keras.layers import Input, Add, Dense, Flatten\n",
    "# Activation layers\n",
    "from tensorflow.keras.layers import ReLU, LeakyReLU, ELU, ThresholdedReLU\n",
    "# Dropout layers\n",
    "from tensorflow.keras.layers import Dropout, AlphaDropout, GaussianDropout\n",
    "# Normalisation layers\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "# Embedding layers\n",
    "from tensorflow.keras.layers import Embedding, Concatenate, Reshape\n",
    "# Callbacks\n",
    "from tensorflow.keras.callbacks import Callback, EarlyStopping, LearningRateScheduler, ModelCheckpoint\n",
    "# Optimisers\n",
    "from tensorflow.keras.optimizers import SGD, RMSprop, Adam, Adadelta, Adagrad, Adamax, Nadam, Ftrl\n",
    "# Model cross validation and evaluation\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn import metrics\n",
    "\n",
    "# For Bayesian hyperparameter searching\n",
    "from skopt import gbrt_minimize, gp_minimize\n",
    "from skopt.utils import use_named_args\n",
    "from skopt.space import Real, Categorical, Integer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n",
      "REPLICAS: 1\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "\n",
    "strategy = tf.distribute.get_strategy()\n",
    "REPLICAS = strategy.num_replicas_in_sync\n",
    "print(f'REPLICAS: {REPLICAS}')\n",
    "\n",
    "# Data access\n",
    "gpu_options = tf.compat.v1.GPUOptions(allow_growth=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.00 Read in Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_features shape: \t\t(23814, 876)\n",
      "test_features shape: \t\t(3982, 876)\n",
      "train_targets_scored shape: \t(23814, 207)\n",
      "sample_submission shape: \t(3982, 207)\n"
     ]
    }
   ],
   "source": [
    "# Directory and file paths\n",
    "input_dir                 = '../input/lish-moa/'\n",
    "train_features_path       = os.path.join(input_dir, 'train_features.csv')\n",
    "test_features_path        = os.path.join(input_dir, 'test_features.csv')\n",
    "train_targets_scored_path = os.path.join(input_dir, 'train_targets_scored.csv')\n",
    "sample_submission_path    = os.path.join(input_dir, 'sample_submission.csv')\n",
    "\n",
    "# Read in data\n",
    "train_features       = pd.read_csv(train_features_path)\n",
    "test_features        = pd.read_csv(test_features_path)\n",
    "train_targets_scored = pd.read_csv(train_targets_scored_path)\n",
    "sample_submission    = pd.read_csv(sample_submission_path)\n",
    "\n",
    "del train_features_path, test_features_path, train_targets_scored_path, sample_submission_path\n",
    "\n",
    "print(f'train_features shape: \\t\\t{train_features.shape}')\n",
    "print(f'test_features shape: \\t\\t{test_features.shape}')\n",
    "print(f'train_targets_scored shape: \\t{train_targets_scored.shape}')\n",
    "print(f'sample_submission shape: \\t{sample_submission.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model name: nn_0_label_classifier_seed28\n"
     ]
    }
   ],
   "source": [
    "# Define key parameters\n",
    "SEED = 28\n",
    "np.random.seed(SEED)\n",
    "\n",
    "SCALER_METHOD = RobustScaler()\n",
    "\n",
    "FEATURE_SELECTOR = RandomForestClassifier(random_state=SEED)\n",
    "\n",
    "NUM_COMPONENTS = 200\n",
    "PCA_METHOD = PCA(n_components=NUM_COMPONENTS, random_state=SEED)\n",
    "\n",
    "EPOCHS = 100\n",
    "BATCH_SIZE = 64\n",
    "KFOLDS = 10\n",
    "PATIENCE = 10\n",
    "\n",
    "USE_EMBEDDING = True\n",
    "MODEL_TO_USE = 'nn'\n",
    "model_name_save = MODEL_TO_USE + '_0_label_classifier_seed' + str(SEED)\n",
    "\n",
    "# Create weights path if does not exist already\n",
    "if not os.path.exists(f'weights/{model_name_save}'):\n",
    "    os.mkdir(f'weights/{model_name_save}')\n",
    "\n",
    "print(f'Model name: {model_name_save}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.00 Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_target_data(data):\n",
    "    \"\"\"\n",
    "    Transforms the target dataset with multiple labels into \n",
    "    a dataset that has one label (indicating whether there were\n",
    "    0 labels or not)\n",
    "    \"\"\"\n",
    "    # Get number of labels per sig_id\n",
    "    data_transformed = data.drop('sig_id', axis=1).sum(axis=1)\n",
    "    data_transformed = pd.DataFrame(data_transformed).rename(columns={0:'num_labels'})\n",
    "    # Add labels based on whether there are zero labels or not\n",
    "    data_transformed['has_zero_label'] = 0\n",
    "    data_transformed.loc[data_transformed.num_labels == 0, 'has_zero_label'] = 1\n",
    "    # Remove num_labels feature for final target df\n",
    "    data_transformed = data_transformed.drop('num_labels', axis=1)\n",
    "    \n",
    "    return data_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_feature_set(X_train, X_test, y_train, y_test, \n",
    "                          verbose=0, \n",
    "                          scaler=SCALER_METHOD, \n",
    "                          feature_selector=FEATURE_SELECTOR, \n",
    "                          pca=PCA_METHOD, \n",
    "                          seed=SEED):\n",
    "    \"\"\"\n",
    "    Takes in X_train and X_test datasets, and applies feature selection, scaling and pca\n",
    "    depending on arguments. \n",
    "    \n",
    "    Returns X_train and X_test data ready for training/prediction, and returns\n",
    "    list of numerical cols and categorical cols, for the use of creating embeddings.\n",
    "    \"\"\"\n",
    "    \n",
    "    ## DATA PREPARATION ##\n",
    "    \n",
    "    # Drop unique ID feature\n",
    "    X_train = X_train.drop('sig_id', axis=1)\n",
    "    X_test  = X_test.drop('sig_id', axis=1)\n",
    "    # Get indices for train and test dfs - we'll need these later\n",
    "    train_idx = list(X_train.index)\n",
    "    test_idx  = list(X_test.index)\n",
    "    # Separate train data types\n",
    "    X_train_numerical   = X_train.select_dtypes('number')\n",
    "    X_train_categorical = X_train.select_dtypes('object')\n",
    "    X_train_categorical = X_train_categorical.astype('category')\n",
    "    # Separate val data types\n",
    "    X_test_numerical   = X_test.select_dtypes('number')\n",
    "    X_test_categorical = X_test.select_dtypes('object')\n",
    "    X_test_categorical = X_test_categorical.astype('category')\n",
    "    \n",
    "    \n",
    "    ## SCALING ##\n",
    "    \n",
    "    if scaler is not None:\n",
    "        if verbose == 1:\n",
    "            print('APPLYING SCALER...')\n",
    "        # Fit and transform scaler to train and val\n",
    "        scaler.fit(X_train_numerical)\n",
    "        X_train_numerical = scaler.transform(X_train_numerical)\n",
    "        X_test_numerical  = scaler.transform(X_test_numerical)\n",
    "    \n",
    "    \n",
    "    ## FEATURE SELECTION ##\n",
    "    \n",
    "    # Feature selection is only ran on numerical data\n",
    "    if feature_selector is not None:\n",
    "        if verbose == 1:\n",
    "            print('APPLYING FEATURE SELECTOR...')\n",
    "        # Fit tree based classifier to select features\n",
    "        if verbose == 1: \n",
    "            num_cols = X_train_numerical.shape[1]\n",
    "        feature_selector  = SelectFromModel(estimator=feature_selector).fit(X_train_numerical, y_train)\n",
    "        X_train_numerical = feature_selector.transform(X_train_numerical)\n",
    "        X_test_numerical  = feature_selector.transform(X_test_numerical)\n",
    "        if verbose == 1: \n",
    "            print(f'{num_cols - X_train_numerical.shape[1]} features removed in feature selection.')\n",
    "            del num_cols\n",
    "\n",
    "    \n",
    "    ## PCA ##\n",
    "    \n",
    "    if pca is not None:\n",
    "        if verbose == 1:\n",
    "            print('APPLYING PCA...')\n",
    "        # Fit and transform pca to train and val\n",
    "        pca.fit(X_train_numerical)\n",
    "        X_train_numerical = pca.transform(X_train_numerical)\n",
    "        X_test_numerical  = pca.transform(X_test_numerical)\n",
    "        if verbose == 1:\n",
    "            print(f'NUMBER OF PRINCIPAL COMPONENTS: {pca.n_components_}')\n",
    "    # Convert numerical features into pandas dataframe\n",
    "    X_train_numerical = pd.DataFrame(X_train_numerical, index=train_idx).add_prefix('pca_')\n",
    "    X_test_numerical  = pd.DataFrame(X_test_numerical, index=test_idx).add_prefix('pca_')\n",
    "    \n",
    "    \n",
    "    ## CATEGORICAL FEATURES ##\n",
    "    \n",
    "    # Get categorical and numerical column names\n",
    "    num_cols = X_train_numerical.columns\n",
    "    cat_cols = X_train_categorical.columns\n",
    "    # Encode categorical features\n",
    "    X_train_categorical = X_train_categorical.apply(lambda x: x.cat.codes)\n",
    "    X_test_categorical  = X_test_categorical.apply(lambda x: x.cat.codes)\n",
    "\n",
    "    \n",
    "    # Concatenate transformed categorical features with transformed numerical features  \n",
    "    X_train = pd.concat([X_train_categorical, X_train_numerical], axis=1)\n",
    "    X_test = pd.concat([X_test_categorical, X_test_numerical], axis=1)\n",
    "    \n",
    "    if verbose == 1:\n",
    "        print(f'TRAIN SHAPE: \\t{X_train.shape}')\n",
    "        print(f'VALIDATION SHAPE: \\t{X_test.shape}')\n",
    "    \n",
    "    return X_train, X_test, num_cols, cat_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_features\n",
    "y = transform_target_data(train_targets_scored)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.00 Modelling\n",
    "### 4.01 Class Weighting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight for class 0: 0.82\n",
      "Weight for class 1: 1.27\n"
     ]
    }
   ],
   "source": [
    "# Due to the high data imbalance, we add extra weight to the target class\n",
    "neg, pos = np.bincount(np.array(y['has_zero_label']))\n",
    "\n",
    "weight_for_0 = (1 / neg)*(len(np.array(y['has_zero_label']))) / 2.0 \n",
    "weight_for_1 = (1 / pos)*(len(np.array(y['has_zero_label']))) / 2.0\n",
    "\n",
    "class_weight = {0: weight_for_0, 1: weight_for_1}\n",
    "\n",
    "print('Weight for class 0: {:.2f}'.format(weight_for_0))\n",
    "print('Weight for class 1: {:.2f}'.format(weight_for_1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.02 Learning Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def build_lrfn(lr_start          = 0.00001, \n",
    "               lr_max            = 0.0008, \n",
    "               lr_min            = 0.00001, \n",
    "               lr_rampup_epochs  = 20, \n",
    "               lr_sustain_epochs = 0, \n",
    "               lr_exp_decay      = 0.8):\n",
    "    \n",
    "    lr_max = lr_max * strategy.num_replicas_in_sync\n",
    "\n",
    "    def lrfn(epoch):\n",
    "        if epoch < lr_rampup_epochs:\n",
    "            lr = (lr_max - lr_start) / lr_rampup_epochs * epoch + lr_start\n",
    "        elif epoch < lr_rampup_epochs + lr_sustain_epochs:\n",
    "            lr = lr_max\n",
    "        else:\n",
    "            lr = (lr_max - lr_min) * lr_exp_decay**(epoch - lr_rampup_epochs - lr_sustain_epochs) + lr_min\n",
    "        return lr\n",
    "\n",
    "    return lrfn\n",
    "\n",
    "lrfn = build_lrfn()\n",
    "lr = LearningRateScheduler(lrfn, verbose=0)\n",
    "\n",
    "plt.plot([lrfn(epoch) for epoch in range(EPOCHS)])\n",
    "plt.title('Learning Rate Schedule')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Learning Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.03 Define Model\n",
    "\n",
    "The below model was the original architecture, however when we conduct our Bayesian Hyperparameter search, we'll be playing around with the architecture of this baseline model a little. Parameter tuning will affect the model depth as well as the numbers of nodes at each layer, the dropout layers, activation functions and optimisers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(X_data, numerical_cols, categorical_cols, use_embedding=USE_EMBEDDING):\n",
    "    \n",
    "    if use_embedding == True:\n",
    "        inputs = []\n",
    "        embeddings = []\n",
    "\n",
    "        for col in categorical_cols:\n",
    "            # Create categorical embedding\n",
    "            input_ = Input(shape=(1,))\n",
    "            input_dim = int(X_data[col].max() + 1)\n",
    "            embedding = Embedding(input_dim=input_dim, output_dim=10, input_length=1)(input_)\n",
    "            embedding = Reshape(target_shape=(10,))(embedding)\n",
    "            inputs.append(input_)\n",
    "            embeddings.append(embedding)\n",
    "\n",
    "        input_numeric = Input(shape=(len(numerical_cols),))\n",
    "        embedding_numeric = Dense(8192, activation='relu')(input_numeric) \n",
    "        inputs.append(input_numeric)\n",
    "        embeddings.append(embedding_numeric)\n",
    "\n",
    "        x = Concatenate()(embeddings)\n",
    "        \n",
    "    if use_embedding == False:\n",
    "        input_ = Input(shape=(X_data.shape[1], ))\n",
    "        x = Dense(8192, activation='relu')(input_)\n",
    "        \n",
    "    x = Dense(4096, activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.5)(x) \n",
    "    \n",
    "    x = Dense(2048, activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "\n",
    "    x = Dense(1024, activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    \n",
    "    x = Dense(512, activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    \n",
    "    x = Dense(256, activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    \n",
    "    output = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "    if use_embedding == True:\n",
    "        model = Model(inputs, output)\n",
    "    elif use_embedding == False:\n",
    "        model = Model(input_, output)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian Hyperparameter Search \n",
    "\n",
    "Here we will conduct Bayesian hyperparameter search for a number of different parameters in our model - and save the best performing model for each fold. Some of what was coded above will be overwritten, but I wanted to leave the above model in the notebook so we have a record of what our baseline model was before we conducted hyperparameter tuning. \n",
    "\n",
    "The best loss scores we got from the baseline model (before Bayesian hyperparameter search) were centred around ~0.7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Redefine hyperparamater constants\n",
    "EPOCHS           = 100\n",
    "SEED             = 140\n",
    "np.random.seed(SEED)\n",
    "SCALER_METHOD    = RobustScaler()\n",
    "FEATURE_SELECTOR = RandomForestClassifier(random_state=SEED)\n",
    "KFOLDS           = 10\n",
    "MODEL_TO_USE     = 'nn'\n",
    "model_name_save  = MODEL_TO_USE + '_0_label_classifier_seed' + str(SEED)\n",
    "\n",
    "# Create weights path if does not exist already\n",
    "if not os.path.exists(f'weights/{model_name_save}'):\n",
    "    os.mkdir(f'weights/{model_name_save}')\n",
    "\n",
    "# Define hyperparameter search dimensions\n",
    "dim_num_components   = Integer(low=20, high=300,        name='num_components')\n",
    "dim_learning_rate    = Real(low=1e-4,  high=1e-2, prior='log-uniform', name='learning_rate')\n",
    "dim_num_dense_layers = Integer(low=1,  high=6,          name='num_dense_layers')\n",
    "dim_num_input_nodes  = Integer(low=1,  high=4096,       name='num_input_nodes')\n",
    "dim_num_dense_nodes  = Integer(low=1,  high=4096,       name='num_dense_nodes')\n",
    "dim_activation = Categorical(categories=['relu','leaky_relu','elu','threshold_relu'], name='activation')\n",
    "dim_batch_size       = Integer(low=1,  high=64,        name='batch_size')\n",
    "dim_patience         = Integer(low=3,  high=15,         name='patience')\n",
    "dim_optimiser = Categorical(categories=['sgd','adam','rms_prop','ada_delta','ada_grad',\n",
    "                                        'ada_max','n_adam','ftrl'], name='optimiser')\n",
    "dim_optimiser_decay  = Real(low=1e-6,  high=1e-2,       name='optimiser_decay')\n",
    "dim_dropout_layer = Categorical(categories=['dropout','gaussian_dropout','alpha_dropout'],name='dropout_layer')\n",
    "dim_dropout_val      = Real(low=0.1,   high=0.8,        name='dropout_val')\n",
    "dim_use_embedding    = Integer(low=0,  high=1,          name='use_embedding')\n",
    "\n",
    "\n",
    "dimensions = [dim_num_components,\n",
    "              dim_learning_rate,\n",
    "              dim_num_dense_layers,\n",
    "              dim_num_input_nodes,\n",
    "              dim_num_dense_nodes,\n",
    "              dim_activation,\n",
    "              dim_batch_size,\n",
    "              dim_patience,\n",
    "              dim_optimiser,\n",
    "              dim_optimiser_decay,\n",
    "              dim_dropout_layer,\n",
    "              dim_dropout_val,\n",
    "              dim_use_embedding\n",
    "             ]\n",
    "\n",
    "# Set default hyperparameters\n",
    "default_parameters = [\n",
    "    20,        # num_components\n",
    "    1e-3,      # learning_rate\n",
    "    1,         # num_dense_layers\n",
    "    512,       # num_input_nodes\n",
    "    13,        # num_dense_nodes\n",
    "    'relu',    # activation\n",
    "    64,        # batch_size\n",
    "    3,         # patience\n",
    "    'adam',    # optimiser\n",
    "    1e-3,      # optimiser_decay\n",
    "    'dropout', # dropout_layer\n",
    "    0.1,       # dropout_val\n",
    "    1          # use_embedding\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD 0\n",
      "--------------------------------------------------------------------------------------------------\n",
      "RUNNING PARAMETER SEARCH...\n",
      "\n",
      "BEST LOSS: 0.5925396250158224\n",
      "\n",
      "BEST LOSS: 0.5925396250158224\n",
      "\n",
      "BEST LOSS: 0.5925396250158224\n",
      "\n",
      "BEST LOSS: 0.5925396250158224\n",
      "\n",
      "BEST LOSS: 0.5925396250158224\n",
      "\n",
      "BEST LOSS: 0.5925396250158224\n",
      "\n",
      "BEST LOSS: 0.5925396250158224\n",
      "\n",
      "BEST LOSS: 0.5781330671132445\n",
      "\n",
      "BEST LOSS: 0.5781330671132445\n",
      "\n",
      "BEST LOSS: 0.5725953182573452\n",
      "\n",
      "BEST LOSS: 0.5725953182573452\n",
      "\n",
      "BEST LOSS: 0.5725953182573452\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W1027 21:57:07.382137 140517440739136 nn_ops.py:4372] Large dropout rate: 0.79939 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "W1027 21:57:07.416433 140517440739136 nn_ops.py:4372] Large dropout rate: 0.79939 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "W1027 21:57:07.450803 140517440739136 nn_ops.py:4372] Large dropout rate: 0.79939 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "W1027 21:57:07.484610 140517440739136 nn_ops.py:4372] Large dropout rate: 0.79939 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "W1027 21:57:07.518747 140517440739136 nn_ops.py:4372] Large dropout rate: 0.79939 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEST LOSS: 0.5725953182573452\n",
      "\n",
      "BEST LOSS: 0.5725953182573452\n",
      "\n",
      "BEST LOSS: 0.5725953182573452\n",
      "\n",
      "BEST LOSS: 0.5725953182573452\n",
      "\n",
      "BEST LOSS: 0.5725953182573452\n",
      "\n",
      "BEST LOSS: 0.5725953182573452\n",
      "\n",
      "BEST LOSS: 0.5725953182573452\n",
      "\n",
      "BEST LOSS: 0.5725953182573452\n",
      "\n",
      "BEST LOSS: 0.5725953182573452\n",
      "\n",
      "BEST LOSS: 0.5558668484583984\n",
      "\n",
      "BEST LOSS: 0.5558668484583984\n",
      "\n",
      "BEST LOSS: 0.5558668484583984\n",
      "\n",
      "BEST LOSS: 0.5558668484583984\n",
      "\n",
      "BEST LOSS: 0.5558668484583984\n",
      "\n",
      "BEST LOSS: 0.5558668484583984\n",
      "\n",
      "BEST LOSS: 0.5558668484583984\n",
      "\n",
      "BEST LOSS: 0.5558668484583984\n",
      "\n",
      "BEST LOSS: 0.5558668484583984\n",
      "\n",
      "BEST LOSS: 0.5558668484583984\n",
      "\n",
      "BEST LOSS: 0.5558668484583984\n",
      "\n",
      "BEST LOSS: 0.5558668484583984\n",
      "\n",
      "BEST LOSS: 0.5558668484583984\n",
      "\n",
      "BEST LOSS: 0.5558668484583984\n",
      "\n",
      "BEST LOSS: 0.5558668484583984\n",
      "\n",
      "BEST LOSS: 0.5558668484583984\n",
      "\n",
      "BEST LOSS: 0.5558668484583984\n",
      "\n",
      "BEST LOSS: 0.5558668484583984\n",
      "\n",
      "BEST LOSS: 0.5558668484583984\n",
      "\n",
      "BEST LOSS: 0.5558668484583984\n",
      "\n",
      "BEST LOSS: 0.5558668484583984\n",
      "\n",
      "BEST LOSS: 0.5558668484583984\n",
      "\n",
      "BEST LOSS: 0.5558668484583984\n",
      "\n",
      "BEST LOSS: 0.5558668484583984\n",
      "\n",
      "BEST LOSS: 0.5558668484583984\n",
      "\n",
      "BEST LOSS: 0.5558668484583984\n",
      "\n",
      "BEST LOSS: 0.5558668484583984\n",
      "\n",
      "BEST LOSS: 0.5558668484583984\n",
      "\n",
      "BEST LOSS: 0.5558668484583984\n",
      "\n",
      "BEST LOSS: 0.5558668484583984\n",
      "\n",
      "BEST LOSS: 0.5558668484583984\n",
      "\n",
      "BEST LOSS: 0.5558668484583984\n",
      "\n",
      "BEST LOSS: 0.5558668484583984\n",
      "\n",
      "BEST LOSS: 0.5558668484583984\n",
      "\n",
      "BEST LOSS: 0.5558668484583984\n",
      "\n",
      "BEST LOSS: 0.5558668484583984\n",
      "\n",
      "BEST LOSS: 0.5558668484583984\n",
      "\n",
      "BEST LOSS: 0.5558668484583984\n",
      "\n",
      "BEST LOSS: 0.5558668484583984\n",
      "\n",
      "BEST LOSS: 0.5558668484583984\n",
      "\n",
      "BEST LOSS: 0.5558668484583984\n",
      "\n",
      "BEST LOSS: 0.5558668484583984\n",
      "\n",
      "BEST LOSS: 0.5558668484583984\n",
      "\n",
      "BEST LOSS: 0.5558668484583984\n",
      "\n",
      "BEST LOSS: 0.5558668484583984\n",
      "\n",
      "BEST LOSS: 0.5558668484583984\n",
      "\n",
      "BEST LOSS: 0.5558668484583984\n",
      "\n",
      "BEST LOSS: 0.5558668484583984\n",
      "\n",
      "BEST LOSS: 0.5558668484583984\n",
      "\n",
      "BEST LOSS: 0.5558668484583984\n",
      "\n",
      "BEST LOSS: 0.5558668484583984\n",
      "\n",
      "BEST LOSS: 0.5558668484583984\n",
      "\n",
      "BEST LOSS: 0.5558668484583984\n",
      "\n",
      "BEST LOSS: 0.5558668484583984\n",
      "\n",
      "BEST LOSS: 0.5558668484583984\n",
      "\n",
      "BEST LOSS: 0.5558668484583984\n",
      "\n",
      "BEST LOSS: 0.5558668484583984\n",
      "\n",
      "BEST LOSS: 0.5558668484583984\n",
      "\n",
      "BEST LOSS: 0.5558668484583984\n",
      "\n",
      "BEST LOSS: 0.5558668484583984\n",
      "\n",
      "BEST LOSS: 0.5558668484583984\n",
      "\n",
      "BEST LOSS: 0.5558668484583984\n",
      "\n",
      "BEST LOSS: 0.5558668484583984\n",
      "\n",
      "BEST LOSS: 0.5558668484583984\n",
      "\n",
      "BEST LOSS: 0.5558668484583984\n",
      "\n",
      "BEST LOSS: 0.5558668484583984\n",
      "\n",
      "BEST LOSS: 0.5558668484583984\n",
      "\n",
      "BEST LOSS: 0.5558668484583984\n",
      "\n",
      "BEST LOSS: 0.5558668484583984\n",
      "\n",
      "BEST LOSS: 0.5558668484583984\n",
      "\n",
      "BEST LOSS: 0.5558668484583984\n",
      "\n",
      "BEST LOSS: 0.5558668484583984\n",
      "\n",
      "BEST LOSS: 0.5558668484583984\n",
      "\n",
      "BEST LOSS: 0.5558668484583984\n",
      "\n",
      "BEST LOSS: 0.5558668484583984\n",
      "\n",
      "BEST LOSS: 0.5558668484583984\n",
      "\n",
      "BEST LOSS: 0.5558668484583984\n",
      "\n",
      "BEST LOSS: 0.5558668484583984\n",
      "\n",
      "BEST LOSS: 0.5558668484583984\n",
      "\n",
      "BEST LOSS: 0.5558668484583984\n",
      "\n",
      "BEST LOSS: 0.5558668484583984\n",
      "\n",
      "BEST LOSS: 0.5558668484583984\n",
      "\n",
      "BEST LOSS: 0.5558668484583984\n",
      "\n",
      "BEST LOSS: 0.5558668484583984\n",
      "\n",
      "BEST LOSS: 0.5558668484583984\n",
      "\n",
      "BEST LOSS: 0.5558668484583984\n",
      "\n",
      "BEST LOSS: 0.5558668484583984\n",
      "\n",
      "BEST LOSS: 0.5558668484583984\n",
      "\n",
      "BEST LOSS: 0.5558668484583984\n",
      "\n",
      "BEST LOSS: 0.5558668484583984\n",
      "\n",
      "BEST LOSS: 0.5558668484583984\n",
      "\n",
      "BEST LOSS: 0.5558668484583984\n",
      "\n",
      "BEST LOSS: 0.5558668484583984\n",
      "\n",
      "BEST LOSS: 0.5558668484583984\n",
      "\n",
      "BEST LOSS: 0.5558668484583984\n",
      "\n",
      "BEST LOSS: 0.5558668484583984\n",
      "\n",
      "BEST LOSS: 0.5558668484583984\n",
      "\n",
      "BEST LOSS: 0.5558668484583984\n",
      "\n",
      "BEST LOSS: 0.5558668484583984\n",
      "\n",
      "BEST LOSS: 0.5558668484583984\n",
      "\n",
      "BEST LOSS: 0.5558668484583984\n",
      "\n",
      "BEST LOSS: 0.5558668484583984\n",
      "\n",
      "BEST LOSS: 0.5558668484583984\n",
      "\n",
      "BEST LOSS: 0.5558668484583984\n",
      "\n",
      "BEST LOSS: 0.5558668484583984\n",
      "\n",
      "BEST LOSS: 0.5558668484583984\n",
      "\n",
      "BEST LOSS: 0.5558668484583984\n",
      "\n",
      "BEST LOSS: 0.5558668484583984\n",
      "\n",
      "BEST LOSS: 0.5558668484583984\n",
      "\n",
      "BEST LOSS: 0.5558668484583984\n",
      "\n",
      "BEST LOSS: 0.5558668484583984\n",
      "\n",
      "BEST LOSS: 0.5558668484583984\n",
      "\n",
      "BEST LOSS: 0.5558668484583984\n",
      "\n",
      "BEST LOSS: 0.5558668484583984\n",
      "\n",
      "BEST LOSS: 0.5558668484583984\n",
      "\n",
      "BEST LOSS: 0.5558668484583984\n",
      "\n",
      "BEST LOSS: 0.5558668484583984\n",
      "\n",
      "BEST LOSS: 0.5558668484583984\n",
      "\n",
      "BEST LOSS: 0.5558668484583984\n",
      "\n",
      "BEST LOSS: 0.5558668484583984\n",
      "\n",
      "BEST LOSS: 0.5558668484583984\n",
      "\n",
      "BEST LOSS: 0.5558668484583984\n",
      "\n",
      "BEST LOSS: 0.5558668484583984\n",
      "\n",
      "BEST LOSS: 0.5558668484583984\n",
      "\n",
      "BEST LOSS: 0.5558668484583984\n",
      "\n",
      "BEST LOSS: 0.5558668484583984\n",
      "\n",
      "BEST LOSS: 0.5558668484583984\n",
      "\n",
      "BEST LOSS: 0.5558668484583984\n",
      "\n",
      "BEST LOSS: 0.5558668484583984\n",
      "\n",
      "BEST LOSS: 0.5558668484583984\n",
      "\n",
      "BEST LOSS: 0.5558668484583984\n",
      "\n",
      "BEST LOSS: 0.5558668484583984\n",
      "\n",
      "BEST LOSS: 0.5558668484583984\n",
      "\n",
      "BEST LOSS: 0.5558668484583984\n",
      "\n",
      "BEST LOSS: 0.5558668484583984\n",
      "\n",
      "BEST LOSS: 0.5558668484583984\n",
      "\n",
      "BEST LOSS: 0.5558668484583984\n",
      "\n",
      "BEST LOSS: 0.5558668484583984\n",
      "\n",
      "BEST LOSS: 0.5558668484583984\n",
      "\n",
      "BEST LOSS: 0.5558668484583984\n",
      "\n",
      "BEST LOSS: 0.5558668484583984\n",
      "\n",
      "BEST LOSS: 0.5558668484583984\n",
      "\n",
      "BEST LOSS: 0.5558668484583984\n",
      "\n",
      "BEST LOSS: 0.5558668484583984\n",
      "\n",
      "BEST LOSS: 0.5558668484583984\n",
      "\n",
      "BEST LOSS: 0.5558668484583984\n",
      "\n",
      "BEST LOSS: 0.5558668484583984\n",
      "\n",
      "BEST LOSS: 0.5558668484583984\n",
      "\n",
      "BEST LOSS: 0.5558668484583984\n",
      "\n",
      "BEST LOSS: 0.5558668484583984\n",
      "\n",
      "BEST LOSS: 0.5558668484583984\n",
      "\n",
      "BEST LOSS: 0.5558668484583984\n",
      "\n",
      "BEST LOSS: 0.5558668484583984\n",
      "\n",
      "BEST LOSS: 0.5558668484583984\n",
      "\n",
      "BEST LOSS: 0.5558668484583984\n",
      "\n",
      "BEST LOSS: 0.5558668484583984\n",
      "\n",
      "BEST LOSS: 0.5558668484583984\n",
      "\n",
      "BEST LOSS: 0.5558668484583984\n",
      "\n",
      "BEST LOSS: 0.5558668484583984\n",
      "\n",
      "BEST LOSS: 0.5558668484583984\n",
      "\n",
      "BEST LOSS: 0.5558668484583984\n",
      "\n",
      "BEST LOSS: 0.5558668484583984\n",
      "\n",
      "BEST LOSS: 0.5558668484583984\n",
      "\n",
      "BEST LOSS: 0.5558668484583984\n",
      "\n",
      "BEST LOSS: 0.5558668484583984\n",
      "\n",
      "BEST LOSS: 0.5558668484583984\n",
      "\n",
      "BEST LOSS: 0.5558668484583984\n",
      "\n",
      "BEST LOSS: 0.5558668484583984\n",
      "\n",
      "BEST LOSS: 0.5558668484583984\n",
      "\n",
      "BEST LOSS: 0.5558668484583984\n",
      "\n",
      "BEST LOSS: 0.5558668484583984\n",
      "\n",
      "BEST LOSS: 0.5558668484583984\n",
      "\n",
      "BEST LOSS: 0.5558668484583984\n",
      "\n",
      "BEST LOSS: 0.5558668484583984\n",
      "\n",
      "BEST LOSS: 0.5558668484583984\n",
      "\n",
      "BEST LOSS: 0.5558668484583984\n",
      "\n",
      "BEST LOSS: 0.5558668484583984\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define CV strategy\n",
    "skf = StratifiedKFold(n_splits=KFOLDS, random_state=SEED)\n",
    "loss_scores = []\n",
    "\n",
    "for fold, (tdx, vdx) in enumerate(skf.split(X, y)):\n",
    "    print(f'FOLD {fold}')\n",
    "    print('--------------------------------------------------------------------------------------------------')\n",
    "    # Create name to save weights by\n",
    "    model_save_name = 'weights/' + model_name_save + '/' + model_name_save + '_' + str(fold) + '.h5'\n",
    "    model_save_name_temp = 'weights/' + model_name_save + '/' + 'TEMP_'+ model_name_save+ '_' + str(fold) + '.h5'\n",
    "    \n",
    "    @use_named_args(dimensions=dimensions)\n",
    "    def get_hyperopts(num_components,\n",
    "                      learning_rate, \n",
    "                      num_dense_layers, \n",
    "                      num_input_nodes, \n",
    "                      num_dense_nodes,\n",
    "                      activation, \n",
    "                      batch_size,\n",
    "                      patience,\n",
    "                      optimiser,\n",
    "                      optimiser_decay,\n",
    "                      dropout_layer,\n",
    "                      dropout_val,\n",
    "                      use_embedding,\n",
    "                     ):\n",
    "\n",
    "        # Define key parameters - these are affected by parameter search so must be done inside function\n",
    "        NUM_COMPONENTS   = num_components\n",
    "        PCA_METHOD       = PCA(n_components=NUM_COMPONENTS, random_state=SEED)\n",
    "        BATCH_SIZE       = batch_size\n",
    "        PATIENCE         = patience\n",
    "        USE_EMBEDDING    = use_embedding\n",
    "\n",
    "        # Fetch in-fold data\n",
    "        X_tdx, X_vdx, y_tdx, y_vdx = X.iloc[tdx, :], X.iloc[vdx, :], y.iloc[tdx, :], y.iloc[vdx, :]  \n",
    "\n",
    "        # Transform data\n",
    "        X_tdx, X_vdx, num_cols, cat_cols = transform_feature_set(X_tdx, X_vdx, y_tdx, y_vdx,\n",
    "                                                                 pca=PCA_METHOD) \n",
    "\n",
    "        \n",
    "        # Define activation layers\n",
    "        if activation == 'relu':\n",
    "            ACTIVATION = ReLU()\n",
    "        elif activation == 'leaky_relu':\n",
    "            ACTIVATION = LeakyReLU()\n",
    "        elif activation == 'elu':\n",
    "            ACTIVATION = ELU()\n",
    "        elif activation == 'threshold_relu':\n",
    "            ACTIVATION = ThresholdedReLU()\n",
    "\n",
    "        # Define regularisation layers\n",
    "        if dropout_layer == 'dropout':\n",
    "            REG_LAYER = Dropout(dropout_val)\n",
    "        elif dropout_layer == 'gaussian_dropout':\n",
    "            REG_LAYER = GaussianDropout(dropout_val)\n",
    "        elif dropout_layer == 'alpha_dropout':\n",
    "            REG_LAYER = AlphaDropout(dropout_val)\n",
    "\n",
    "        # Define optimisers #\n",
    "        if optimiser == 'sgd':\n",
    "            OPTIMISER = SGD(lr=learning_rate, decay=optimiser_decay)\n",
    "        elif optimiser == 'adam':\n",
    "            OPTIMISER = RMSprop(lr=learning_rate, decay=optimiser_decay)\n",
    "        elif optimiser == 'rms_prop':\n",
    "            OPTIMISER = Adam(lr=learning_rate, decay=optimiser_decay)\n",
    "        elif optimiser == 'ada_delta':\n",
    "            OPTIMISER = Adadelta(lr=learning_rate, decay=optimiser_decay)\n",
    "        elif optimiser == 'ada_grad':\n",
    "            OPTIMISER = Adagrad(lr=learning_rate, decay=optimiser_decay)\n",
    "        elif optimiser == 'ada_max':\n",
    "            OPTIMISER = Adamax(lr=learning_rate, decay=optimiser_decay)\n",
    "        elif optimiser == 'n_adam':\n",
    "            OPTIMISER = Nadam(lr=learning_rate, decay=optimiser_decay)\n",
    "        elif optimiser == 'ftrl':\n",
    "            OPTIMISER = Ftrl(lr=learning_rate, decay=optimiser_decay)\n",
    "\n",
    "        ## BUILD MODEL BASED ON INPUTTED BAYESIAN HYPERPARAMETERS ##\n",
    "        # Input layer #\n",
    "        if USE_EMBEDDING == 1:\n",
    "            inputs = []\n",
    "            embeddings = []\n",
    "            for col in cat_cols:\n",
    "                # Create categorical embedding for each categorical feature\n",
    "                input_ = Input(shape=(1,))\n",
    "                input_dim = int(X_tdx[col].max() + 1)\n",
    "                embedding = Embedding(input_dim=input_dim, output_dim=10, input_length=1)(input_)\n",
    "                embedding = Reshape(target_shape=(10,))(embedding)\n",
    "                inputs.append(input_)\n",
    "                embeddings.append(embedding)\n",
    "            input_numeric = Input(shape=(len(num_cols),))\n",
    "            embedding_numeric = Dense(num_input_nodes)(input_numeric) \n",
    "            embedding_numeric = ACTIVATION(embedding_numeric) \n",
    "            inputs.append(input_numeric)\n",
    "            embeddings.append(embedding_numeric)\n",
    "            x = Concatenate()(embeddings)\n",
    "        if USE_EMBEDDING == 0:\n",
    "            input_ = Input(shape=(X_tdx.shape[1], ))\n",
    "            x = Dense(num_input_nodes)(input_)\n",
    "        # Hidden layers #\n",
    "        for i in range(num_dense_layers):\n",
    "            layer_name = f'layer_dense_{i+1}'\n",
    "            x = Dense(num_dense_nodes, name=layer_name)(x)\n",
    "            x = ACTIVATION(x)\n",
    "            x = BatchNormalization()(x)\n",
    "            x = REG_LAYER(x) \n",
    "        # Output layer #\n",
    "        output = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "        if USE_EMBEDDING == 1:\n",
    "            model = Model(inputs, output)\n",
    "        elif USE_EMBEDDING == 0:\n",
    "            model = Model(input_, output)\n",
    "\n",
    "        \n",
    "        # COMPILE MODEL #\n",
    "        model.compile(optimizer=OPTIMISER, \n",
    "                      loss='binary_crossentropy')\n",
    "\n",
    "        # Define learning rate schedule\n",
    "        lr = LearningRateScheduler(lrfn, verbose=0)\n",
    "        \n",
    "        # Define early stopping parameters\n",
    "        es = EarlyStopping(monitor='val_loss', \n",
    "                           mode='min',\n",
    "                           restore_best_weights=True, \n",
    "                           verbose=0, \n",
    "                           patience=PATIENCE)\n",
    "        \n",
    "        # Define model checkpoint parameters\n",
    "        mc = ModelCheckpoint(filepath=model_save_name_temp, \n",
    "                             save_best_only=True, \n",
    "                             save_weights_only=False,\n",
    "                             monitor='val_loss', \n",
    "                             mode='min',\n",
    "                             verbose=0)\n",
    "\n",
    "        if USE_EMBEDDING == 1:\n",
    "            # Separate data to fit into embedding and numerical input layers\n",
    "            X_tdx = [np.absolute(X_tdx[i]) for i in cat_cols] + [X_tdx[num_cols]]\n",
    "            X_vdx = [np.absolute(X_vdx[i]) for i in cat_cols] + [X_vdx[num_cols]]\n",
    "\n",
    "        # FIT MODEL #\n",
    "        history = model.fit(X_tdx, y_tdx,\n",
    "                            epochs=EPOCHS,\n",
    "                            batch_size=BATCH_SIZE,\n",
    "                            callbacks = [es, lr, mc],\n",
    "                            class_weight=list(class_weight.values()),\n",
    "                            verbose=0,\n",
    "                            validation_split=0.15\n",
    "                           )\n",
    "        \n",
    "        # Get val_loss for the best model (one saved with ModelCheckpoint)\n",
    "        loss = min(history.history['val_loss'])\n",
    "        \n",
    "        # Save best loss to global memory\n",
    "        global best_loss\n",
    "\n",
    "        # If the classification loss of the saved model is improved\n",
    "        if loss < best_loss:\n",
    "            model.save(model_save_name)\n",
    "            best_loss = loss\n",
    "            \n",
    "            # Save transformed validation arrays (so they can be used for prediction)\n",
    "            global X_vdx_best_model, y_vdx_best_model\n",
    "            X_vdx_best_model, y_vdx_best_model = X_vdx, y_vdx\n",
    "            \n",
    "        print(f'BEST LOSS: {best_loss}\\n')\n",
    "        \n",
    "        del model\n",
    "        k.clear_session()\n",
    "        return(loss)\n",
    "    \n",
    "    ## RUN BAYESIAN HYPERPARAMETER SEARCH ##\n",
    "    print('RUNNING PARAMETER SEARCH...\\n')\n",
    "    time.sleep(2)\n",
    "    best_loss = np.Inf\n",
    "    gp_result = gp_minimize(func         = get_hyperopts,\n",
    "                            dimensions   = dimensions,\n",
    "                            acq_func     = 'EI', # Expected Improvement.\n",
    "                            n_calls      = 200,\n",
    "                            noise        = 0.01,\n",
    "                            n_jobs       = -1,\n",
    "                            kappa        = 5,\n",
    "                            x0           = default_parameters,\n",
    "                            random_state = SEED\n",
    "                           )\n",
    "    \n",
    "       \n",
    "    print('SEARCH COMPLETE.')\n",
    "    print('MAKING VALIDATION PREDICTIONS...\\n')\n",
    "    \n",
    "    # Load best model\n",
    "    model = load_model(model_save_name)\n",
    "    # Make validation predictions\n",
    "    preds = model.predict(X_vdx_best_model)\n",
    "\n",
    "    # Calculate OOF loss \n",
    "    oof_loss = metrics.log_loss(y_vdx_best_model, preds)\n",
    "\n",
    "    print('FOLD ' + str(fold) + ' LOSS: ' + str(oof_loss))\n",
    "    print('--------------------------------------------------------------------------------------------------')\n",
    "    time.sleep(2)\n",
    "    loss_scores.append(oof_loss)\n",
    "\n",
    "    # Clean up\n",
    "    gc.collect()\n",
    "    os.remove(model_save_name_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------\n",
      "FOLD SCORES\n",
      "--------------\n",
      "0    0.574458\n",
      "1    0.542139\n",
      "2    0.548955\n",
      "3    0.548018\n",
      "4    0.550960\n",
      "5    0.558142\n",
      "6    0.553232\n",
      "7    0.550390\n",
      "8    0.559744\n",
      "9    0.554526\n",
      "dtype: float64\n",
      "\n",
      "--------------\n",
      "FOLD STATS\n",
      "--------------\n",
      "count    10.000000\n",
      "mean      0.554056\n",
      "std       0.008779\n",
      "min       0.542139\n",
      "25%       0.549314\n",
      "50%       0.552096\n",
      "75%       0.557238\n",
      "max       0.574458\n",
      "dtype: float64\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(f'--------------\\nFOLD SCORES\\n--------------\\n{pd.Series(loss_scores)}')\n",
    "print(f'\\n--------------\\nFOLD STATS\\n--------------\\n{pd.Series(loss_scores).describe()}')\n",
    "\n",
    "plt.plot(pd.Series(loss_scores).index, pd.Series(loss_scores), marker='.')\n",
    "plt.title('Loss Fold Results')\n",
    "plt.xlabel('Fold')\n",
    "plt.xticks(np.arange(0, KFOLDS, step=1))\n",
    "plt.ylabel('Loss')\n",
    "plt.ylim([0.5, 0.6])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
